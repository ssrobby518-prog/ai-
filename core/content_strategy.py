"""Shared content-intelligence layer for executive output generators.

Centralizes:
- Curated/index page detection
- Non-event / index page filtering for CEO deck
- Grammar-safe sanitization (never creates broken sentences)
- 6-column decision card construction (no homework, no template sentences)
- CEO article blocks with real content
- Context-aware term explainer with curated dictionary
- Responsibility mapping

Used by: ppt_generator.py, doc_generator.py
"""

from __future__ import annotations

import re
from collections import Counter
from urllib.parse import urlparse

from schemas.education_models import EduNewsCard
from utils.text_quality import trim_trailing_fragment as _trim_trailing

# ---------------------------------------------------------------------------
# Banned words — sanitize all output text
# ---------------------------------------------------------------------------
BANNED_WORDS = [
    "ai捕捉", "AI Intel", "Z1", "Z2", "Z3", "Z4", "Z5",
    "pipeline", "ETL", "verify_run", "ingestion", "ai_core",
]

# System-operation terms that must never appear in CEO output
_SYSTEM_WORDS_RE = re.compile(
    r"系統健康|系統運作|資料可信度|延遲|P95|雜訊清除|健康狀態|"
    r"pipeline|ingestion|資料完整率|traffic_light|success_rate|"
    r"latency_p95|noise_filtered",
    re.IGNORECASE,
)

# Patterns that indicate empty-talk / homework / template sentences
_HOMEWORK_PATTERNS = [
    r"本週內[：:]",
    r"→\s*產出[：:]",
    r"摘要筆記",
    r"搜尋[「『]",
    r"去查|去找|去搜",
    r"建議追蹤",
    r"值得持續觀察",
    r"反映.*重要.*趨勢",
    r"此事件反映",
    r"值得.*關注.*後續",
    r"兩週內[：:].*評估",
    r"產出[：:].*評估表",
    r"影響評估表",
    r"了當前的重要產業趨勢",
    r"與.*相關的技術或概念",
    r"領域在\s*\S+\s*方面存在.*缺口",
    r"可探索提供相關工具或解決方案",
    r"初步評估筆",
    r"重要產業趨勢",
    # Template sentences generated by the analysis pipeline
    r"領域的現有參與.*評估",
    r"將驅動相關方重新評估",
    r"這是.*領域的.*動態",
    r"對相關產業或使用者產生",
    r"相關方可能需要新的",
    r"需要評估相容性影響",
    r"As part of its mission",
    r"[」』].*?最新報導",
    r"[」』].*?最新趨勢",
    r"[」』].*?進展",
    r"對自身工作或投資的潛在影響",
    r"了解更多.*資訊",
    r"進一步了解",
    r"持續關注.*動向",
    r"的趨勢.*觀察",
    r"風險/機會清單",
]
_HOMEWORK_RE = re.compile("|".join(_HOMEWORK_PATTERNS), re.IGNORECASE)

# Pattern to detect "filler echo" — raw what_happened text echoed into analysis
# e.g. 基於「As much of Silicon Valley chases mega-rounds and...
_ECHO_RE = re.compile(
    r"基於[「『].*?[」』]|"
    r"事件的潛在影響[：:]|"
    r"\[假說\]\s*若[「『]",
    re.IGNORECASE,
)

# Fragment-start patterns that should be removed from output text.
_FRAGMENT_START_RE = re.compile(
    r"^Last\s+\w+\s+was\b|^This\s+\w+\s+was\b|^This\s+month\s+was\b",
    re.IGNORECASE,
)

# Keywords suggesting a curated/index/archive page rather than a single event
_INDEX_PAGE_KEYWORDS = [
    "curated list", "curated", "rss feed", "rss", "entries for the year",
    "collecting", "archive", "overview", "stats", "year in review",
    "changelog", "release notes list", "index of", "table of contents",
    "as part of its mission", "newsletter",
]

# Patterns that indicate non-event content (title/summary level)
_NON_EVENT_TITLE_PREFIXES = [
    r"^as part of its mission",
    r"^overview",
    r"^archive",
    r"^curated",
    r"^table of contents",
    r"^newsletter",
    r"^a list of",
    r"^collection of",
    # Personal blog / appreciation posts
    r"^for this year",
    r"^i love ",
    # Scraper artifacts (login pages, UI text)
    r"you must be signed",
    r"sign(ed)? in to",
    r"notification settings",
    r"^show hn:",
]
_NON_EVENT_TITLE_RE = re.compile(
    "|".join(_NON_EVENT_TITLE_PREFIXES), re.IGNORECASE
)

# Event action verbs (Chinese + English) — at least one must appear for event
_EVENT_VERBS_RE = re.compile(
    r"推出|收購|禁用|發布|漏洞|裁員|上調|下調|立法|訴訟|"
    r"投資|併購|開源|下架|召回|罰款|合併|關閉|暫停|擴張|"
    r"launch|acquir|ban|releas|vulnerabilit|layoff|raise|lower|"
    r"legislat|sued|invest|merg|open.?source|remov|recall|fine|"
    r"shut|suspend|expand|block|hide|filter|maintain|operat|"
    r"announc|introduc|report|discover|breach|hack|leak|"
    r"partner|fund|grant|approve|reject|delay|cancel|"
    r"happen|develop|reveal|confirm|warn|impact|affect|change",
    re.IGNORECASE,
)

# Responsibility mapping for "要問誰" column
RESPONSIBILITY_MAP = {
    "綜合": "策略長/PM",
    "tech": "策略長/PM",
    "科技/技術": "研發/CTO",
    "人工智慧": "研發/CTO",
    "資安": "資安長",
    "政策/監管": "法務",
    "法規": "法務",
    "金融/財經": "財務長/CFO",
    "創業/投融資": "策略長/PM",
    "氣候/能源": "營運/COO",
    "併購/企業": "策略長/CEO",
    "消費電子": "產品/PM",
    "遊戲/娛樂": "產品/PM",
    "雲": "研發/CTO",
    "AI": "研發/CTO",
    "工程": "研發/CTO",
    "產品": "產品/PM",
    "市場": "產品/市場",
}


# ---------------------------------------------------------------------------
# Sanitization — grammar-safe, never creates broken sentences
# ---------------------------------------------------------------------------


def sanitize(text: str) -> str:
    """Remove banned words, then strip homework/template sentences.

    Fragment-replacement approach: replaces matched phrases instead
    of deleting entire sentences, then repairs broken Chinese grammar.
    """
    if not text:
        return ""
    result = text
    for bw in BANNED_WORDS:
        result = result.replace(bw, "")
    # Remove system operation terms
    result = _SYSTEM_WORDS_RE.sub("", result)
    # Replace homework fragments
    result = _HOMEWORK_RE.sub("", result).strip()
    # Remove echo-filler patterns
    result = _ECHO_RE.sub("", result).strip()
    # Remove fragment-start patterns (e.g., "Last July was...")
    result = _FRAGMENT_START_RE.sub("", result).strip()
    # Chinese grammar repair: fix dangling connectors at sentence start
    result = re.sub(r"^[了而並且因此所以但是然而]+", "", result)
    result = re.sub(r"(?<=[。！？\n])[了而並且因此所以但是然而]+", "", result)
    # Fix broken punctuation
    result = re.sub(r"。。+", "。", result)
    result = re.sub(r"[「」『』]\s*[「」『』]", "", result)
    # Remove orphaned closing quotes (」not preceded by matching 「)
    result = re.sub(r"(?<![「『])[」』]", "", result)
    result = re.sub(r"^\s*[，。；：]+", "", result)
    result = re.sub(r"\s{2,}", " ", result).strip()
    # If result is empty or too short after cleanup, return empty
    if len(result) < 3:
        return ""
    # Detect broken fragments left after pattern removal
    # e.g. "，的機會 記" or "的趨勢，" orphans
    if re.search(r"，的[^，。]{0,6}[記表]|^[，的。]{1,4}$", result):
        return ""
    # If mostly punctuation / whitespace, discard
    alpha_count = sum(1 for c in result if c.isalnum())
    if alpha_count < max(3, len(result) * 0.3):
        return ""
    # Trim trailing fragments (dangling connectors/particles)
    result = _trim_trailing(result)
    return result


def _smart_truncate(text: str, limit: int) -> str:
    """Truncate at sentence/word boundary. Never mid-word or dangling Chinese."""
    if len(text) <= limit:
        return text
    cut = text[:limit]
    # Try sentence boundary first (Chinese punctuation, then English period)
    for sep in ["。", "！", "？", "；", ". ", "，"]:
        pos = cut.rfind(sep)
        if pos > limit * 0.5:
            return cut[:pos + len(sep)].rstrip()
    # Try word boundary (space)
    pos = cut.rfind(" ")
    if pos > limit * 0.5:
        return cut[:pos].rstrip() + "…"
    return cut.rstrip() + "…"


def responsible_party(category: str) -> str:
    """Map category to responsible party."""
    cat = (category or "").strip()
    if cat in RESPONSIBILITY_MAP:
        return RESPONSIBILITY_MAP[cat]
    cat_lower = cat.lower()
    for key, val in RESPONSIBILITY_MAP.items():
        if key.lower() in cat_lower or cat_lower in key.lower():
            return val
    return "策略長/PM"


# ---------------------------------------------------------------------------
# Non-event / index page detection
# ---------------------------------------------------------------------------


def is_index_page(card: EduNewsCard) -> bool:
    """Detect if a card represents a curated/index/archive page, not a single event."""
    combined = f"{card.title_plain} {card.what_happened}".lower()
    hits = sum(1 for kw in _INDEX_PAGE_KEYWORDS if kw in combined)
    return hits >= 1


def is_non_event_or_index(card: EduNewsCard) -> bool:
    """Strong filter: detect index pages AND content lacking event substance.

    Returns True (= should be excluded from CEO deck) if:
    - Title starts with known non-event prefixes, OR
    - Content is an index/archive page, OR
    - Content lacks 2+ of the 3 event elements: subject, action, time
    """
    title = (card.title_plain or "").strip()
    summary = (card.what_happened or "").strip()
    one_liner = getattr(card, "one_liner", "") or ""
    combined = f"{title} {summary} {one_liner}"

    # Check title prefixes
    if _NON_EVENT_TITLE_RE.search(title):
        return True

    # Check index page keywords
    if is_index_page(card):
        return True

    # Check event substance: subject + action + time
    has_action = bool(_EVENT_VERBS_RE.search(combined))
    has_time = bool(re.search(
        r"20\d{2}|本週|昨日|今日|近日|日前|上週|本月|今年|yesterday|today|"
        r"this week|last week|recently|Monday|Tuesday|Wednesday|Thursday|"
        r"Friday|Q[1-4]|January|February|March|April|May|June|July|"
        r"August|September|October|November|December",
        combined, re.IGNORECASE,
    ))
    # Subject: any recognizable proper noun (capitalized word >=3 chars)
    has_subject = bool(re.search(r"[A-Z][a-z]{2,}", combined))

    elements = sum([has_action, has_time, has_subject])
    # Allow missing 1 element, but not 2+
    if elements <= 1:
        return True

    return False


def _clean_text(text: str, max_len: int) -> str:
    """Sanitize + truncate."""
    return _smart_truncate(sanitize(text), max_len) if text else ""


# ---------------------------------------------------------------------------
# 6-column decision card builder
# ---------------------------------------------------------------------------


def build_decision_card(card: EduNewsCard) -> dict[str, list[str] | str]:
    """Build a structured 6-column decision card from an EduNewsCard.

    Returns dict with keys: event, facts, effects, risks, actions, owner.
    Each value is either a str or list[str].
    All text is sanitized and free of homework/template sentences.
    """
    is_index = is_index_page(card)
    owner = responsible_party(card.category)

    # 1) 事件一句話 (≤22 chars)
    if is_index:
        event = "來源疑似彙整索引頁，非單一事件"
    else:
        raw_event = _clean_text(card.what_happened or "", 22)
        event = raw_event if raw_event and len(raw_event) > 4 else "事件摘要資料不足"

    # 2) 已知事實 (3 points) — ONLY from fact_check/evidence, never from what_happened
    facts: list[str] = []
    if is_index:
        if card.title_plain:
            facts.append(f"來源標題：{_clean_text(card.title_plain, 50)}")
        facts.append("缺口：此頁為彙整/索引，無法提取單一事件的事實")
    else:
        for f in (card.fact_check_confirmed or [])[:3]:
            cleaned = _clean_text(f, 55)
            if cleaned and len(cleaned) > 5:
                facts.append(cleaned)
        if not facts:
            for e in (card.evidence_lines or [])[:3]:
                cleaned = _clean_text(e, 55)
                if cleaned and len(cleaned) > 5:
                    facts.append(cleaned)

    gap_templates = [
        "缺口：缺可驗證來源或原始出處",
        "缺口：缺事件時間或主體",
        "缺口：缺第三方佐證",
    ]
    gi = 0
    while len(facts) < 3 and gi < len(gap_templates):
        if gap_templates[gi] not in facts:
            facts.append(gap_templates[gi])
        gi += 1

    # 3) 可能影響 (2-3 points) — from derivable_effects ONLY, no what_happened echo
    effects: list[str] = []
    if is_index:
        effects = [
            "若此來源多為索引頁，會稀釋每日情報的決策價值",
            "資訊來源品質下降可能導致漏抓真正的重要事件",
        ]
    else:
        for eff in (card.derivable_effects or [])[:3]:
            cleaned = _clean_text(eff, 50)
            if cleaned and len(cleaned) > 5 and not _HOMEWORK_RE.search(cleaned):
                effects.append(cleaned)
        if not effects:
            # Try why_important as a single fallback, but DON'T echo what_happened
            if card.why_important:
                cleaned = _clean_text(card.why_important, 50)
                if cleaned and len(cleaned) > 10 and not _HOMEWORK_RE.search(cleaned):
                    effects.append(cleaned)
        if not effects:
            effects.append("缺口：影響面尚待分析（原始資料不足）")

    # 4) 主要風險 (2 points) — from speculative_effects ONLY
    risks: list[str] = []
    if is_index:
        risks = [
            "若持續納入索引頁，可能遮蔽真正需要決策的事件",
            "資料品質下降→決策基礎受損",
        ]
    else:
        for r in (card.speculative_effects or [])[:2]:
            cleaned = _clean_text(r, 50)
            if cleaned and len(cleaned) > 5 and not _HOMEWORK_RE.search(cleaned):
                risks.append(cleaned)
        if not risks:
            risks.append("缺口：風險評估需更多背景資料")

    # 5) 建議決策/動作 (1-2 points) — NEVER homework, NEVER echo what_happened
    actions: list[str] = []
    if is_index:
        actions = [
            "先整理來源可信度與關鍵數字，建立可追溯檢核清單。",
            "指派負責人以最小範圍驗證訊號，下一輪再決定是否升級行動。",
        ]
    else:
        for a in (card.action_items or [])[:2]:
            cleaned = _clean_text(a, 55)
            if (cleaned and len(cleaned) > 5
                    and not _HOMEWORK_RE.search(cleaned)
                    and not _ECHO_RE.search(cleaned)):
                actions.append(cleaned)
        if not actions:
            actions.append("追蹤本日高熱度訊號，先確認來源與數字證據，再決定是否 MOVE。")

    return {
        "event": event[:22],
        "facts": facts[:3],
        "effects": effects[:3],
        "risks": risks[:2],
        "actions": actions[:2],
        "owner": owner,
    }


# ---------------------------------------------------------------------------
# Key term extraction — strict: only technical/proper nouns, never common words
# ---------------------------------------------------------------------------

_STOP_WORDS = {
    # Common English — aggressively broad to prevent "changed", "landscape", etc.
    "the", "and", "for", "are", "but", "not", "you", "all", "can", "had",
    "her", "was", "one", "our", "out", "has", "his", "how", "its", "may",
    "new", "now", "old", "see", "way", "who", "did", "get", "let", "say",
    "she", "too", "use", "with", "this", "that", "from", "have", "been",
    "will", "more", "when", "some", "than", "them", "what", "your", "each",
    "make", "like", "into", "over", "such", "take", "year", "also", "back",
    "could", "would", "about", "after", "other", "which", "their", "there",
    "first", "these", "those", "being", "where", "every", "should", "because",
    "http", "https", "www", "com", "org", "html", "json", "xml", "url",
    "via", "per", "etc", "just", "very", "much", "most", "only", "then",
    "here", "well", "still", "even", "does", "done", "going", "want",
    "said", "says", "many", "been", "were", "they", "them", "both",
    "same", "while", "during", "before", "since", "between", "under",
    "within", "through", "already", "several", "another", "however",
    "including", "according", "although", "using", "based", "part",
    "report", "reports", "reported", "company", "companies", "people",
    "data", "time", "made", "last", "next", "down", "help", "show",
    "shows", "showed", "look", "need", "needs", "work", "works",
    "plan", "plans", "move", "call", "called", "keep", "start",
    "started", "come", "came", "think", "given", "give", "gave",
    "found", "find", "known", "know", "long", "high", "lead",
    "early", "late", "left", "right", "real", "open", "test",
    "tests", "tested", "added", "used", "set", "run", "big",
    "file", "files", "page", "pages", "link", "links", "site",
    "image", "images", "text", "click", "view", "read", "list",
    "item", "items", "type", "name", "code", "line", "lines",
    "source", "content", "title", "post", "blog", "web",
    "major", "latest", "recent", "global", "full", "total",
    "million", "billion", "percent", "number", "version",
    # Additional generic words that are NOT technical terms
    "mission", "preserve", "maintained", "traces", "hide",
    "videos", "operating", "operates", "capture", "snapshots",
    "increasingly", "becoming", "unarchivable", "part",
    # Common English words that got extracted as "terms" incorrectly
    "changed", "landscape", "exec", "doubling", "running", "chases",
    "deals", "buzzy", "throwback", "venture", "capital", "like",
    "world", "important", "something", "happened", "tech",
    "news", "good", "best", "better", "worse", "small", "large",
    "also", "really", "actually", "basically", "probably",
    "today", "yesterday", "tomorrow", "week", "month",
    "says", "told", "asked", "means", "think", "working",
    "trying", "getting", "making", "taking", "looking",
    "going", "coming", "putting", "turning", "building",
    "growing", "moving", "changing", "becoming", "running",
    "saying", "doing", "having", "seeing", "giving",
    "different", "specific", "particular", "general", "possible",
    "certain", "likely", "clear", "strong", "among", "across",
    "around", "against", "along", "beyond", "toward",
    "nearly", "almost", "above", "below", "enough", "rather",
    "instead", "whether", "though", "behind", "might",
    "model", "system", "process", "service", "platform",
    "network", "market", "product", "business", "industry",
    "level", "point", "issue", "state", "order", "place",
    "power", "group", "thing", "world", "money", "family",
    "story", "fact", "month", "night", "school", "three",
    "human", "local", "small", "large", "young", "public",
    "early", "often", "those", "whole", "whose", "bring",
    "shall", "being", "among", "allow", "begin", "would",
    # Common English words that appear capitalized at sentence start
    "free", "software", "folders", "department", "homeland",
    "fork", "settings", "notification", "signed", "myself",
    "leader", "day", "love", "thank", "documentation",
    "pressure", "identify", "owners", "accounts", "hundreds",
    "border", "information", "increasing", "project",
    "entries", "collecting", "maintainers", "maintained",
    "written", "ported", "trust", "anymore", "header",
    "library", "review", "disc", "archival", "capability",
    "bookmark", "manager", "radar", "live", "threat",
    "cyber", "intelligence", "grid", "client", "sent",
    "reportedly", "hundreds", "requests", "like",
    "compiler", "amsterdam", "deep", "single", "origin",
    "arch", "desc", "threat", "tool", "tools", "user", "users",
}

# Only extract words that look like actual terms (capitalized, or contain digits/hyphens)
_TERM_RE = re.compile(r"[A-Za-z][A-Za-z0-9-]{2,}")


# ---------------------------------------------------------------------------
# Curated term dictionary (CEO-readable, white-language)
# ---------------------------------------------------------------------------
TERM_DICTIONARY: dict[str, dict[str, str]] = {
    # Internet / Web
    "internet archive": {
        "what": "一個非營利組織，專門保存網頁、書籍、影片的歷史副本",
        "biz": "若公司網站內容被保存，可能暴露舊版頁面或已修正的資訊",
    },
    "ublock origin": {
        "what": "一款免費的瀏覽器廣告攔截外掛，可過濾網頁廣告和追蹤器",
        "biz": "若用戶大量使用，會直接減少公司線上廣告的觸及率和收入",
    },
    "filter list": {
        "what": "一份規則清單，告訴攔截工具哪些網頁元素要隱藏或封鎖",
        "biz": "新的過濾規則可能影響公司產品頁面的顯示或廣告投放效果",
    },
    "youtube shorts": {
        "what": "YouTube 的短影片功能，類似 TikTok 的直式短影片",
        "biz": "短影片是目前社群行銷主力管道，若被過濾會影響觸及率",
    },
    "web crawler": {
        "what": "自動瀏覽網頁並下載內容的程式，搜尋引擎就是靠它收集資料",
        "biz": "爬蟲政策影響公司網站的 SEO 排名和資料被第三方使用的方式",
    },
    "webpage snapshot": {
        "what": "某個時間點的網頁完整備份，像是幫網頁拍照存檔",
        "biz": "舊版網頁快照可能被用於法律舉證或競爭對手情報分析",
    },
    "open source": {
        "what": "程式碼公開、任何人都能檢視和修改的軟體開發模式",
        "biz": "採用開源可降低授權成本，但需評估維護責任和安全風險",
    },
    "api": {
        "what": "不同軟體之間溝通的標準介面，像是餐廳的點餐窗口",
        "biz": "API 品質直接影響產品整合速度和合作夥伴的接入體驗",
    },
    "saas": {
        "what": "透過網路訂閱使用的軟體服務，不需自己安裝維護",
        "biz": "SaaS 模式影響公司的 IT 支出結構和資料控制權",
    },
    "ai": {
        "what": "人工智慧，讓電腦模擬人類思考和決策的技術",
        "biz": "AI 工具可提升效率但需評估準確性、成本和合規風險",
    },
    "llm": {
        "what": "大型語言模型，能理解和生成人類語言的 AI 系統",
        "biz": "可用於客服、內容生成、資料分析，但有幻覺和隱私風險",
    },
    "blockchain": {
        "what": "一種分散式帳本技術，資料一旦寫入就很難竄改",
        "biz": "可能影響供應鏈追溯、數位資產管理和跨境支付流程",
    },
    "cloud": {
        "what": "透過網路使用遠端伺服器的運算和儲存資源",
        "biz": "雲端成本和供應商鎖定是 IT 策略的核心決策點",
    },
    "cybersecurity": {
        "what": "保護電腦系統和資料不被未經授權存取或攻擊的措施",
        "biz": "資安事件可導致營運中斷、罰款和商譽損失",
    },
    "zero-day": {
        "what": "軟體中尚未被修補的安全漏洞，攻擊者可能已在利用",
        "biz": "零日漏洞代表最高風險等級，需立即評估受影響系統",
    },
    "ransomware": {
        "what": "會加密你的檔案並要求付贖金才解鎖的惡意程式",
        "biz": "勒索軟體可癱瘓整個營運，備份和應變計畫是關鍵防線",
    },
    "gdpr": {
        "what": "歐盟的個人資料保護法規，對資料收集和使用有嚴格規範",
        "biz": "違規罰款最高達全球營收 4%，影響所有有歐洲用戶的業務",
    },
    "iot": {
        "what": "物聯網，讓日常設備（感測器、家電等）連上網路互相溝通",
        "biz": "IoT 設備增加攻擊面，但也帶來自動化和數據收集機會",
    },
    "edge computing": {
        "what": "把運算放在靠近資料來源的地方處理，而非全送到雲端",
        "biz": "可降低延遲和頻寬成本，適合即時應用場景",
    },
    "kubernetes": {
        "what": "自動管理大量容器化應用程式的開源平台",
        "biz": "降低維運人力但學習門檻高，是雲端架構的核心技術選擇",
    },
    "docker": {
        "what": "把應用程式和所需環境打包成標準化容器的工具",
        "biz": "加速部署和環境一致性，是現代軟體交付的基礎設施",
    },
    "microservices": {
        "what": "把大系統拆成多個獨立小服務，各自開發部署",
        "biz": "提升開發速度但增加系統複雜度，需權衡團隊規模",
    },
    "devops": {
        "what": "開發和維運團隊緊密合作、自動化交付的工作方式",
        "biz": "DevOps 成熟度直接影響產品上線速度和系統穩定性",
    },
    "fintech": {
        "what": "運用科技改善金融服務的產業，如行動支付、線上借貸",
        "biz": "FinTech 競爭者可能侵蝕傳統金融業務的市場份額",
    },
    "quantum computing": {
        "what": "利用量子力學原理進行運算的新型電腦，部分問題可指數加速",
        "biz": "長期可能破解現有加密，短期影響有限但需開始規劃",
    },
    "5g": {
        "what": "第五代行動通訊技術，速度更快、延遲更低、連接更多設備",
        "biz": "5G 基礎建設影響遠端辦公、智慧工廠和新產品開發",
    },
    "ar": {
        "what": "擴增實境，在現實世界上疊加數位資訊的技術",
        "biz": "AR 可用於培訓、產品展示和遠端維修，降低實體成本",
    },
    "vr": {
        "what": "虛擬實境，用頭戴裝置沈浸在完全數位化的環境中",
        "biz": "VR 應用於培訓和協作，但硬體成本和使用者接受度是門檻",
    },
    "nft": {
        "what": "非同質化代幣，用區塊鏈證明數位物品的獨特性和所有權",
        "biz": "NFT 熱潮已降溫，但底層技術仍可用於數位資產認證",
    },
    "web3": {
        "what": "基於區塊鏈的去中心化網路願景，用戶擁有自己的數據",
        "biz": "Web3 概念尚在早期，投資需謹慎評估實際商業價值",
    },
    "semiconductor": {
        "what": "半導體，製造晶片的核心材料，驅動所有電子設備",
        "biz": "晶片供應影響產品交期和成本，是供應鏈的戰略性物資",
    },
    "x86-64": {
        "what": "個人電腦和伺服器最常用的處理器架構標準",
        "biz": "x86 生態系決定軟體相容性，架構轉移影響 IT 採購決策",
    },
    "arm": {
        "what": "一種低功耗處理器架構，廣泛用於手機和新型筆電",
        "biz": "ARM 架構在伺服器和筆電的崛起影響軟體開發和採購策略",
    },
    "gpu": {
        "what": "圖形處理器，擅長大量並行運算，是 AI 訓練的核心硬體",
        "biz": "GPU 供需和價格直接影響 AI 專案的成本和可行性",
    },
    "cpu": {
        "what": "中央處理器，電腦的運算核心，執行所有指令",
        "biz": "CPU 效能和供應影響伺服器成本和產品效能上限",
    },
    "nvidia": {
        "what": "全球最大的 GPU 晶片設計公司，AI 訓練硬體的主導者",
        "biz": "NVIDIA 的產品供應和定價直接影響 AI 專案可行性",
    },
    "openai": {
        "what": "開發 ChatGPT 和 GPT 系列的 AI 公司",
        "biz": "OpenAI 的技術和定價策略影響企業 AI 導入的成本效益",
    },
    "microsoft": {
        "what": "全球最大軟體公司之一，擁有 Windows、Office、Azure 雲端",
        "biz": "Microsoft 生態系是多數企業 IT 基礎設施的核心",
    },
    "google": {
        "what": "全球最大搜尋引擎和廣告平台，也是主要雲端供應商",
        "biz": "Google 的政策變動影響 SEO、廣告投放和雲端成本",
    },
    "aws": {
        "what": "Amazon Web Services，全球最大的雲端運算平台",
        "biz": "AWS 是多數企業雲端基礎設施首選，定價影響 IT 預算",
    },
    "sec": {
        "what": "美國證券交易委員會，監管股票市場和上市公司",
        "biz": "SEC 的監管動作影響上市/融資策略和合規成本",
    },
    "ipo": {
        "what": "首次公開發行股票，公司從私有轉為公開上市",
        "biz": "IPO 影響募資策略、估值預期和競爭對手動向",
    },
    "antitrust": {
        "what": "反壟斷法規，防止企業過度壟斷市場的法律",
        "biz": "反壟斷調查可能阻止併購案或迫使企業拆分業務",
    },
    "bandwidth": {
        "what": "網路一次能傳輸多少資料的上限，像是水管的粗細",
        "biz": "頻寬不足會影響雲端服務品質和遠端辦公體驗",
    },
    "latency": {
        "what": "從發出請求到收到回應的等待時間",
        "biz": "高延遲影響用戶體驗和即時交易系統的可靠性",
    },
    "encryption": {
        "what": "把資料轉換成只有授權者才能讀取的密碼形式",
        "biz": "加密是資料保護的基礎，法規常要求傳輸和儲存都要加密",
    },
    "vpn": {
        "what": "虛擬私人網路，在公共網路上建立加密的私人通道",
        "biz": "VPN 是遠端辦公的安全基礎設施，但也可能被用來繞過管制",
    },
    "cdn": {
        "what": "內容分發網路，把網站內容複製到全球各地的伺服器加速存取",
        "biz": "CDN 影響網站速度和全球用戶體驗，是數位業務的基礎設施",
    },
    "machine learning": {
        "what": "讓電腦從資料中自動學習規律和做預測的技術",
        "biz": "ML 可應用於推薦系統、風控、預測分析，但需要高品質資料",
    },
    "deep learning": {
        "what": "機器學習的進階版，用多層神經網路處理複雜模式",
        "biz": "深度學習驅動圖像辨識、語音助理等產品，但訓練成本高",
    },
    "autonomous driving": {
        "what": "自動駕駛，讓車輛不需人類操控就能行駛的技術",
        "biz": "自駕技術影響物流成本、保險模式和法規合規需求",
    },
    "sustainability": {
        "what": "在滿足當前需求的同時，不損害未來世代滿足需求的能力",
        "biz": "ESG 和永續報告已成為投資人和監管機構的硬性要求",
    },
    # VC / Finance
    "venture capital": {
        "what": "創投，投資早期新創公司並換取股權的資金",
        "biz": "創投趨勢反映哪些領域被看好，影響競爭格局和人才流向",
    },
    "mega-round": {
        "what": "超大型融資輪，通常指單輪募資超過 1 億美元",
        "biz": "巨額融資意味著該領域競爭加劇，新進者門檻升高",
    },
    "silicon valley": {
        "what": "美國加州的科技產業聚集區，全球科技創新中心",
        "biz": "矽谷趨勢通常領先全球 6-12 個月，值得提前佈局",
    },
}


def extract_key_terms(card: EduNewsCard) -> list[str]:
    """Extract unique English technical terms from card fields.

    Strict rules:
    - Skip all words in _STOP_WORDS (very broad)
    - Only keep words that are likely technical: capitalized, contain digits/hyphens,
      or match a known term in TERM_DICTIONARY
    - Max 5 terms
    """
    sources = [
        card.title_plain or "",
        card.what_happened or "",
        card.technical_interpretation or "",
    ]
    for line in (card.evidence_lines or []):
        sources.append(line)
    for line in (card.fact_check_confirmed or []):
        sources.append(line)
    for line in (card.derivable_effects or []):
        sources.append(line)

    combined = " ".join(sources)
    raw_terms = _TERM_RE.findall(combined)

    seen: set[str] = set()
    unique: list[str] = []
    for t in raw_terms:
        low = t.lower()
        if low in _STOP_WORDS or low in seen or len(t) < 3:
            continue
        # Only keep if it looks like a real term:
        # 1) In our curated dictionary
        # 2) Contains a digit or hyphen (e.g. "x86-64", "5G")
        # 3) Is a proper noun (starts with uppercase, length >= 4)
        # 4) Matches a known TERM_METAPHOR
        is_in_dict = _lookup_term(t) is not None
        has_special = bool(re.search(r"[\d-]", t))
        is_proper = t[0].isupper() and len(t) >= 4
        if not (is_in_dict or has_special or is_proper):
            continue
        seen.add(low)
        unique.append(t)

    return unique[:5]


# Keep old name as alias for backward compatibility in tests
_extract_english_terms = extract_key_terms


# ---------------------------------------------------------------------------
# Context-aware term explainer with curated dictionary
# ---------------------------------------------------------------------------

from schemas.education_models import TERM_METAPHORS as _CURATED_TERMS


def _lookup_term(term: str) -> dict[str, str] | None:
    """Look up a term in the curated dictionary (case-insensitive).

    Matching rules (prevents false positives like "ar" matching "department"):
    - Exact match: always accepted.
    - Multi-word keys ("deep learning"): only match if the query term
      exactly matches one of the key's component words.
    - Single-word keys: substring match only if both sides >= 4 chars
      AND the shorter side is >= 70% of the longer side.
    """
    low = term.lower()
    # Exact match
    if low in TERM_DICTIONARY:
        return TERM_DICTIONARY[low]
    for key, val in TERM_DICTIONARY.items():
        if " " in key:
            # Multi-word key: query must match one of the words exactly
            if low in key.split():
                return val
        else:
            # Single-word key: tight substring match
            shorter = min(len(low), len(key))
            longer = max(len(low), len(key))
            if shorter >= 4 and shorter >= longer * 0.7:
                if low in key or key in low:
                    return val
    return None


def build_term_explainer(card: EduNewsCard) -> list[dict[str, str]]:
    """Build context-aware term explanations for CEO audience.

    Returns list of dicts: [{"term": ..., "explain": ...}, ...]
    Uses curated dictionary first, then TERM_METAPHORS, then gap indicator.
    Never produces "與...相關的技術或概念" template.
    """
    terms = extract_key_terms(card)
    if not terms:
        return []

    results: list[dict[str, str]] = []
    for term in terms[:3]:  # max 3 per spec
        low = term.lower()

        # 1) Check our curated CEO dictionary first
        curated_dict = _lookup_term(term)
        if curated_dict:
            what_line = curated_dict["what"]
            biz_line = curated_dict["biz"]
            results.append({
                "term": term,
                "explain": what_line,
                "biz": f"CEO 在意：{biz_line}",
            })
            continue

        # 2) Check education TERM_METAPHORS (exact or near-exact only)
        metaphor = None
        for key, val in _CURATED_TERMS.items():
            key_low = key.lower()
            if low == key_low:
                metaphor = val
                break
            # Substring match only if >= 4 chars
            if len(low) >= 4 and low in key_low:
                metaphor = val
                break
            if len(key_low) >= 4 and key_low in low:
                metaphor = val
                break
        if metaphor:
            results.append({"term": term, "explain": metaphor, "biz": ""})
            continue

        # 3) Gap indicator — never produce empty-talk template
        results.append({
            "term": term,
            "explain": "目前資料缺口：無法在來源中確認此名詞的具體指涉",
            "biz": "",
        })

    return results


def build_term_explainer_lines(card: EduNewsCard) -> list[str]:
    """Flat-line version of build_term_explainer for Notion-style output.

    Format per term:
      名詞：一句話是什麼
      CEO 在意：一句話（成本/風險/競爭/合規）
    """
    items = build_term_explainer(card)
    lines: list[str] = []
    for item in items:
        lines.append(f"{item['term']}：{item['explain']}")
        if item.get("biz"):
            lines.append(f"  {item['biz']}")
        lines.append("")
    return lines


# Keep old function name as alias so existing imports don't break
def build_term_explainer_qa(card: EduNewsCard) -> list[str]:
    """Backward-compatible alias — returns flat lines."""
    return build_term_explainer_lines(card)


# ---------------------------------------------------------------------------
# Executive Summary — daily business narrative (3–5 sentences)
# ---------------------------------------------------------------------------

SUMMARY_TONE_LIBRARY: dict[str, dict[str, list[str]]] = {
    "neutral": {
        "risk_words": ["可能帶來風險", "需要持續觀察"],
        "action_words": ["建議持續關注", "建議評估影響"],
    },
    "conservative": {
        "risk_words": ["需審慎評估", "潛在不確定性升高"],
        "action_words": ["建議暫緩", "建議保守應對"],
    },
    "aggressive": {
        "risk_words": ["競爭壓力上升", "市場窗口正在形成"],
        "action_words": ["應積極布局", "建議加速投入"],
    },
    "risk": {
        "risk_words": ["風險正在累積", "需立即關注"],
        "action_words": ["建議啟動應對", "需要快速決策"],
    },
}


def build_executive_summary(
    news_cards: list[EduNewsCard],
    tone: str = "neutral",
) -> list[str]:
    """Synthesize multiple news cards into a 3–5 sentence business narrative.

    This is NOT a bullet list — it reads like a daily CEO briefing paragraph.
    Sources: each card's why_it_matters, possible_impact, risks.

    Args:
        news_cards: list of EduNewsCard to summarize.
        tone: one of "neutral", "conservative", "aggressive", "risk".

    Returns list of 3–5 complete Chinese sentences (no bullets).
    """
    tone_dict = SUMMARY_TONE_LIBRARY.get(tone, SUMMARY_TONE_LIBRARY["neutral"])
    risk_word = tone_dict["risk_words"][0]
    action_word = tone_dict["action_words"][0]

    event_cards = [
        c for c in news_cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    if not event_cards:
        return [
            "今日掃描的資訊來源中，沒有需要管理層立即關注的重大事件。",
            "多數內容為產業索引頁或非單一事件報導，已自動排除。",
            f"{action_word}目前監控頻率，明日再檢視是否有新動態浮現。",
        ]

    # Collect raw material from all event cards
    categories: list[str] = []
    why_fragments: list[str] = []
    impact_fragments: list[str] = []
    risk_fragments: list[str] = []

    for card in event_cards:
        article = build_ceo_article_blocks(card)

        cat = (card.category or "").strip()
        if cat and cat not in categories:
            categories.append(cat)

        for w in article.get("why_it_matters", []):
            cleaned = _clean_text(w, 80)
            if cleaned and len(cleaned) > 8 and not cleaned.startswith("缺口"):
                why_fragments.append(cleaned)

        for imp in article.get("possible_impact", []):
            cleaned = _clean_text(imp, 80)
            if cleaned and len(cleaned) > 8 and not cleaned.startswith("缺口"):
                impact_fragments.append(cleaned)

        for r in article.get("risks", []):
            cleaned = _clean_text(r, 80)
            if cleaned and len(cleaned) > 8 and not cleaned.startswith("缺口"):
                risk_fragments.append(cleaned)

    # Build narrative sentences
    n_events = len(event_cards)
    sentences: list[str] = []

    # Sentence 1: market overview (what happened today)
    cat_label = "、".join(categories[:3]) if categories else "科技"
    sentences.append(
        f"今日共有 {n_events} 則值得關注的市場動態，"
        f"主要集中在{cat_label}領域。"
    )

    # Sentence 2: why it matters (trend / competitive signal)
    if why_fragments:
        best_why = _smart_truncate(why_fragments[0], 100)
        sentences.append(f"目前趨勢顯示：{best_why}。")
    elif impact_fragments:
        best_imp = _smart_truncate(impact_fragments[0], 100)
        sentences.append(f"對公司而言，{best_imp}。")

    # Sentence 3: broader impact
    if impact_fragments and len(sentences) < 4:
        used = set(sentences)
        for frag in impact_fragments:
            candidate = f"從競爭與成本面來看，{_smart_truncate(frag, 100)}。"
            if candidate not in used:
                sentences.append(candidate)
                break

    # Sentence 4: risk posture (tone-aware)
    if risk_fragments:
        best_risk = _smart_truncate(risk_fragments[0], 100)
        sentences.append(f"風險方面，{risk_word}：{best_risk}。")
    else:
        sentences.append(f"目前沒有需要立即決策的高風險事項，但{risk_word}。")

    # Sentence 5: recommendation (tone-aware)
    if n_events >= 3:
        sentences.append(
            f"{action_word}，管理層本週內安排簡短討論，確認是否需要調整策略方向。"
        )
    else:
        sentences.append(
            f"{action_word}相關發展，如有變化將在下次報告中更新。"
        )

    # Ensure 3–5 sentences
    return sentences[:5]


# ---------------------------------------------------------------------------
# CEO article blocks (replaces Q/A format with article-style content)
# ---------------------------------------------------------------------------


def build_ceo_article_blocks(card: EduNewsCard) -> dict[str, str | list[str]]:
    """Build article-style content blocks for CEO-readable output.

    Returns dict with keys:
        headline_cn: headline (≤40 chars, word-boundary truncated)
        one_liner: One-sentence summary of the event (≤80 chars)
        known_facts: list of 3 known facts
        why_it_matters: list of 2 business-language reasons
        possible_impact: list of 2-3 possible impacts
        risks: list of 2 risks
        what_to_do: list of 1-2 concrete actions
        quote: Key evidence quote from the source (≤120 chars)
        sources: list of source URLs
        owner: responsible party
    """
    dc = build_decision_card(card)

    # headline — word-boundary truncated
    raw_title = sanitize(card.title_plain or "事件摘要")
    headline = _smart_truncate(raw_title, 40)

    # one_liner — what happened, sentence-boundary truncated
    if card.what_happened:
        raw_one = sanitize(card.what_happened)
        one_liner = _smart_truncate(raw_one, 80) if raw_one else dc["event"]
    else:
        one_liner = dc["event"]

    # known_facts — from decision card
    known_facts = dc["facts"][:3]

    # why_it_matters — from why_important ONLY (not effects, to avoid duplication)
    why_parts: list[str] = []
    if card.why_important:
        cleaned = _clean_text(card.why_important, 120)
        if cleaned and len(cleaned) > 10:
            why_parts.append(cleaned)
    if not why_parts:
        # Fallback: use first non-gap effect
        for eff in dc["effects"]:
            if not eff.startswith("缺口"):
                why_parts.append(eff)
                break
    if not why_parts:
        why_parts.append("缺口：影響面待進一步分析（原始資料不足）")

    # possible_impact — from derivable_effects ONLY, no overlap with why_it_matters
    impacts: list[str] = []
    used_texts = set(why_parts)  # avoid duplicating why_it_matters content
    for eff in dc["effects"]:
        if not eff.startswith("缺口") and eff not in used_texts:
            impacts.append(eff)
    if not impacts:
        impacts.append("缺口：影響面尚待分析")

    # risks — separate from impacts, from speculative_effects only
    risks = dc["risks"][:2]

    # what_to_do — concrete action with owner; quality-check each item
    actions: list[str] = []
    for a in dc["actions"]:
        # Skip broken fragments (< 6 meaningful chars)
        if len(a) >= 6:
            actions.append(f"{a}（負責人：{dc['owner']}）")
    if not actions:
        actions.append(f"決策者需要確認：此事件是否影響現有業務（負責人：{dc['owner']}）")

    # quote — best evidence line, sentence-boundary truncated
    quote = ""
    for line in (card.evidence_lines or []):
        cleaned = sanitize(line)
        if cleaned and len(cleaned) > 15:
            quote = _smart_truncate(cleaned, 120)
            break
    if not quote:
        for line in (card.fact_check_confirmed or []):
            cleaned = sanitize(line)
            if cleaned and len(cleaned) > 10:
                quote = _smart_truncate(cleaned, 120)
                break

    # sources
    sources: list[str] = []
    if card.source_url and card.source_url.startswith("http"):
        sources.append(card.source_url)

    return {
        "headline_cn": headline,
        "one_liner": one_liner,
        "known_facts": known_facts,
        "why_it_matters": why_parts[:3],
        "possible_impact": impacts[:3],
        "risks": risks,
        "what_to_do": actions[:2],
        "quote": quote,
        "sources": sources,
        "owner": dc["owner"],
    }


def build_executive_qa(card: EduNewsCard, dc: dict) -> list[str]:
    """Build 總經理決策 QA lines, referencing actual card data (not templates)."""
    short_title = _smart_truncate(sanitize(card.title_plain or ""), 20)
    fact_ref = dc["facts"][0] if dc["facts"] else "資料不足"
    effect_ref = dc["effects"][0] if dc["effects"] else "待評估"
    risk_ref = dc["risks"][0] if dc["risks"] else "低"
    action_ref = dc["actions"][0] if dc["actions"] else "待確認"
    owner = dc["owner"]

    lines = [
        f"Q1：「{short_title}」影響收入/成本/合規/交付節奏？",
        f"→ {effect_ref}。風險為「{risk_ref}」。",
        "",
        f"Q2：今天要拍板嗎？延後 2 週代價？",
        f"→ {action_ref}。建議由{owner}於本週內回覆評估結論。",
        "",
        f"Q3：最小試探動作（<=1週 <=1 owner）？",
        f"→ 指派{owner}用 1 個工作天完成初步影響評估並回報。",
    ]
    return lines


# ---------------------------------------------------------------------------
# CEO Brief — new functions for CEO Decision Brief upgrade
# ---------------------------------------------------------------------------

# Regex to extract numbers with optional units from text
_NUMBER_RE = re.compile(
    r"(\d[\d,.]*)\s*(%|億|萬|百萬|千萬|million|billion|percent|"
    r"美元|元|USD|users|用戶|人|家|間|台|款|項|個|筆|件)",
    re.IGNORECASE,
)


def build_data_card(card: EduNewsCard) -> list[dict[str, str]]:
    """Extract numeric metrics from card text via regex.

    Returns list of 1–3 dicts: [{"label": ..., "value": ...}, ...]
    Sources: what_happened, fact_check_confirmed, evidence_lines, derivable_effects.
    """
    sources = [card.what_happened or ""]
    sources.extend(card.fact_check_confirmed or [])
    sources.extend(card.evidence_lines or [])
    sources.extend(card.derivable_effects or [])
    combined = " ".join(sources)

    metrics: list[dict[str, str]] = []
    seen_values: set[str] = set()
    for match in _NUMBER_RE.finditer(combined):
        value = match.group(1)
        unit = match.group(2)
        if value in seen_values:
            continue
        seen_values.add(value)
        # Extract surrounding context as label (up to 15 chars before match)
        start = max(0, match.start() - 15)
        prefix = combined[start:match.start()].strip()
        # Clean prefix to a short label
        label = re.sub(r"^.*[，。；：,;:\s]", "", prefix)
        if not label:
            label = "數據"
        metrics.append({"label": label, "value": f"{value}{unit}"})
        if len(metrics) >= 3:
            break

    # Ensure at least 1 metric
    if not metrics:
        score = card.final_score
        metrics.append({"label": "重要性評分", "value": f"{score:.1f}/10"})

    return metrics


def build_chart_spec(card: EduNewsCard) -> dict:
    """Build a simple chart specification from card data.

    Returns dict: {"type": "bar"|"line"|"pie", "labels": [...], "values": [...]}
    """
    data_card = build_data_card(card)

    labels = [m["label"] for m in data_card]
    # Extract numeric part from value strings
    values: list[float] = []
    for m in data_card:
        nums = re.findall(r"[\d,.]+", m["value"])
        if nums:
            try:
                values.append(float(nums[0].replace(",", "")))
            except ValueError:
                values.append(0.0)
        else:
            values.append(0.0)

    # Choose chart type based on data characteristics
    if len(labels) == 1:
        chart_type = "bar"
    elif len(labels) == 2:
        chart_type = "line"
    else:
        chart_type = "pie"

    return {
        "type": chart_type,
        "labels": labels,
        "values": values,
    }


def build_video_source(card: EduNewsCard) -> list[dict[str, str]]:
    """Convert video_suggestions to YouTube search URLs.

    Returns list of dicts: [{"title": ..., "url": ...}, ...]
    """
    results: list[dict[str, str]] = []
    suggestions = card.video_suggestions or []
    for sug in suggestions[:2]:
        cleaned = sanitize(sug)
        if not cleaned:
            continue
        # Strip common prefixes
        query = re.sub(r"^(YouTube\s*(search|搜尋)[：:]\s*)", "", cleaned, flags=re.IGNORECASE)
        query = query.strip()
        if not query:
            continue
        # URL-encode the query
        import urllib.parse
        encoded = urllib.parse.quote_plus(query)
        url = f"https://www.youtube.com/results?search_query={encoded}"
        results.append({"title": query, "url": url})

    # Fallback: generate from title
    if not results and card.title_plain:
        title = sanitize(card.title_plain)
        if title:
            import urllib.parse
            encoded = urllib.parse.quote_plus(title)
            url = f"https://www.youtube.com/results?search_query={encoded}"
            results.append({"title": title, "url": url})

    return results


# CEO metaphor connectors — at least one must appear
_METAPHOR_CONNECTORS = ["就像", "等於是", "可以想像成", "好比", "類似於"]


def build_ceo_metaphor(card: EduNewsCard) -> str:
    """Build a CEO-readable metaphor (2–3 sentences) with mandatory connector.

    Sources: card.metaphor, what_happened, why_important.
    Must contain at least one of: 就像、等於是、可以想像成、好比、類似於
    """
    raw = (card.metaphor or "").strip()
    raw_sanitized = sanitize(raw) if raw else ""

    # Check if raw metaphor already has a connector
    if raw_sanitized and any(c in raw_sanitized for c in _METAPHOR_CONNECTORS):
        return _smart_truncate(raw_sanitized, 120)

    # Build from scratch using card content
    what = _clean_text(card.what_happened or "", 40)
    why = _clean_text(card.why_important or "", 40)

    if raw_sanitized:
        # Add connector to existing metaphor
        result = f"這件事就像{raw_sanitized}。"
        if why:
            result += f"對公司而言，等於是{why}。"
    elif what and why:
        result = f"可以想像成：{what}。等於是{why}。"
    elif what:
        result = f"可以想像成：{what}。"
    else:
        result = "就像產業地圖正在重新繪製，需要確認自己的位置是否改變。"

    return _smart_truncate(result, 150)


def build_ceo_brief_blocks(card: EduNewsCard) -> dict:
    """Core assembly function: build all CEO Brief blocks for one card.

    Returns dict with keys for Slide 1 (WHAT HAPPENED) and Slide 2 (WHY IT MATTERS):

    Slide 1 keys:
        title: str (≤14 chars)
        ai_trend_liner: str
        image_query: str
        event_liner: str
        data_card: list[dict]
        chart_spec: dict
        ceo_metaphor: str

    Slide 2 keys:
        q1_meaning: str (商業意義)
        q2_impact: str (對公司影響)
        q3_actions: list[str] (≤3 actions)
        video_source: list[dict]
        sources: list[str]
    """
    article = build_ceo_article_blocks(card)
    dc = build_decision_card(card)

    # --- Slide 1: WHAT HAPPENED ---
    raw_title = sanitize(card.title_plain or "事件摘要")
    title = _smart_truncate(raw_title, 14)

    # AI trend liner from category + why_important
    cat = card.category or "科技"
    why = _clean_text(card.why_important or "", 60)
    ai_trend_liner = f"【{cat}趨勢】{why}" if why else f"【{cat}趨勢】市場正在關注此領域的最新動態"

    # Image query for visual search
    image_query = f"{sanitize(card.title_plain or '')} {cat} AI technology"

    # Event liner
    event_liner = article["one_liner"]

    # Data card + chart spec
    data_card = build_data_card(card)
    chart_spec = build_chart_spec(card)

    # CEO metaphor
    ceo_metaphor = build_ceo_metaphor(card)

    # --- Slide 2: WHY IT MATTERS (Q&A) ---
    # Q1: 商業意義
    why_parts = article.get("why_it_matters", [])
    q1 = why_parts[0] if why_parts else "此事件的商業意義尚待進一步分析"

    # Q2: 對公司影響 (product/cost/opportunity/risk)
    impacts = article.get("possible_impact", [])
    risks = article.get("risks", [])
    impact_str = impacts[0] if impacts else "影響面待評估"
    risk_str = risks[0] if risks else "風險待評估"
    q2 = f"{impact_str}。風險面：{risk_str}"

    # Q3: 現在要做什麼 (≤3 actions)
    actions: list[str] = []
    for a in dc["actions"][:2]:
        cleaned = _clean_text(a, 50)
        if cleaned:
            actions.append(cleaned)
    if not actions:
        actions.append("確認此事件是否影響現有業務或專案排程")
    # Add owner action
    owner = dc["owner"]
    actions.append(f"指派{owner}於本週內回覆評估結論")
    actions = actions[:3]

    # Video source
    video_source = build_video_source(card)

    # Sources
    sources: list[str] = []
    if card.source_url and card.source_url.startswith("http"):
        sources.append(card.source_url)

    return {
        # Slide 1
        "title": title,
        "ai_trend_liner": ai_trend_liner,
        "image_query": image_query,
        "event_liner": event_liner,
        "data_card": data_card,
        "chart_spec": chart_spec,
        "ceo_metaphor": ceo_metaphor,
        # Slide 2
        "q1_meaning": q1,
        "q2_impact": q2,
        "q3_actions": actions,
        "video_source": video_source,
        "sources": sources,
    }


def build_structured_executive_summary(
    news_cards: list[EduNewsCard],
    tone: str = "neutral",
) -> dict[str, list[str]]:
    """Build a structured 5-section executive summary for CEO Decision Brief.

    Returns dict with 5 keys, each containing a list of bullet strings:
        ai_trends: AI 技術趨勢方向
        tech_landing: 正在落地的技術
        market_competition: 市場與競爭動態
        opportunities_risks: 機會與風險
        recommended_actions: 建議行動
    """
    tone_dict = SUMMARY_TONE_LIBRARY.get(tone, SUMMARY_TONE_LIBRARY["neutral"])
    risk_word = tone_dict["risk_words"][0]
    action_word = tone_dict["action_words"][0]

    event_cards = [
        c for c in news_cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    if not event_cards:
        return {
            "ai_trends": ["今日未發現需要關注的 AI 技術新趨勢。"],
            "tech_landing": ["今日無新技術落地動態。"],
            "market_competition": ["市場面暫無重大變化。"],
            "opportunities_risks": [f"目前沒有立即的風險事項，但{risk_word}。"],
            "recommended_actions": [f"{action_word}目前監控頻率。"],
        }

    ai_trends: list[str] = []
    tech_landing: list[str] = []
    market_competition: list[str] = []
    opportunities_risks: list[str] = []
    recommended_actions: list[str] = []

    for card in event_cards:
        article = build_ceo_article_blocks(card)
        cat = (card.category or "").lower()
        short_title = _smart_truncate(sanitize(card.title_plain or ""), 25)

        # Classify into sections based on category + content
        why = article.get("why_it_matters", [])
        why_text = why[0] if why else ""
        impacts = article.get("possible_impact", [])
        impact_text = impacts[0] if impacts else ""
        risks_list = article.get("risks", [])
        risk_text = risks_list[0] if risks_list else ""
        actions = article.get("what_to_do", [])
        action_text = actions[0] if actions else ""

        # AI trends: AI-related categories
        if any(kw in cat for kw in ["ai", "人工", "機器", "模型"]):
            if why_text and not why_text.startswith("缺口"):
                ai_trends.append(f"{short_title}：{_smart_truncate(why_text, 60)}")
            else:
                ai_trends.append(f"{short_title}：值得關注的 AI 領域動態")

        # Tech landing: tech/engineering categories
        if any(kw in cat for kw in ["tech", "科技", "工程", "雲", "資安"]):
            if impact_text and not impact_text.startswith("缺口"):
                tech_landing.append(f"{short_title}：{_smart_truncate(impact_text, 60)}")

        # Market competition: market/finance categories
        if any(kw in cat for kw in ["市場", "金融", "投融", "併購", "創業"]):
            if impact_text and not impact_text.startswith("缺口"):
                market_competition.append(f"{short_title}：{_smart_truncate(impact_text, 60)}")

        # Risks from all cards
        if risk_text and not risk_text.startswith("缺口"):
            opportunities_risks.append(f"{short_title}：{_smart_truncate(risk_text, 60)}")

        # Actions from all cards
        if action_text and not action_text.startswith("缺口"):
            recommended_actions.append(_smart_truncate(action_text, 70))

    # Ensure each section has at least one item
    if not ai_trends:
        # Fall back: use first event card
        c = event_cards[0]
        ai_trends.append(
            f"{_smart_truncate(sanitize(c.title_plain or ''), 25)}："
            f"可能影響 AI 產業格局"
        )
    if not tech_landing:
        tech_landing.append("本日事件中尚未出現明確的技術落地案例。")
    if not market_competition:
        market_competition.append("市場競爭面暫無重大訊號。")
    if not opportunities_risks:
        opportunities_risks.append(f"目前{risk_word}，需持續追蹤。")
    if not recommended_actions:
        recommended_actions.append(f"{action_word}相關發展，如有變化將即時更新。")

    # Cap each section at 3 items
    return {
        "ai_trends": ai_trends[:3],
        "tech_landing": tech_landing[:3],
        "market_competition": market_competition[:3],
        "opportunities_risks": opportunities_risks[:3],
        "recommended_actions": recommended_actions[:3],
    }


# ---------------------------------------------------------------------------
# v5 — Corp Watch company tiers
# ---------------------------------------------------------------------------

CORP_TIER_A = [
    "OpenAI", "Google", "Microsoft", "AWS", "Meta", "NVIDIA", "Apple",
]
CORP_TIER_B = [
    "阿里", "字節", "騰訊", "百度", "華為", "Alibaba", "ByteDance",
    "Tencent", "Baidu", "Huawei",
]
_ALL_CORPS = CORP_TIER_A + CORP_TIER_B

# Signal type keywords (derived from card content)
_SIGNAL_KEYWORDS = {
    "TOOL_ADOPTION": [
        "推出", "發布", "launch", "release", "開源", "open-source",
        "SDK", "API", "工具", "tool", "platform", "平台",
    ],
    "USER_PAIN": [
        "漏洞", "vulnerability", "資安", "安全", "breach", "hack",
        "隱私", "privacy", "風險", "risk", "裁員", "layoff",
        "關閉", "shutdown", "下架", "removed",
    ],
    "WORKFLOW_CHANGE": [
        "收購", "acqui", "併購", "merg", "投資", "invest", "合作",
        "partner", "整合", "integrat", "遷移", "migrat", "轉型",
        "transform", "取代", "replac",
    ],
}


# ---------------------------------------------------------------------------
# v5 — Market Heat Index
# ---------------------------------------------------------------------------


def compute_market_heat(cards: list[EduNewsCard]) -> dict:
    """Compute a 0-100 market heat index from card scores and volume.

    Returns dict:
        score: int (0-100)
        level: str ("LOW" / "MEDIUM" / "HIGH" / "VERY_HIGH")
        trend_word: str (Chinese description)
    """
    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]
    if not event_cards:
        return {"score": 0, "level": "LOW", "trend_word": "市場平靜"}

    # Base score: average final_score mapped to 0-100
    avg_score = sum(c.final_score for c in event_cards) / len(event_cards)
    base = min(avg_score * 10, 80)  # max 80 from avg score

    # Volume bonus: more events = hotter market (up to +20)
    volume_bonus = min(len(event_cards) * 4, 20)

    raw = int(base + volume_bonus)
    score = max(0, min(100, raw))

    if score >= 75:
        return {"score": score, "level": "VERY_HIGH", "trend_word": "市場極度活躍"}
    elif score >= 50:
        return {"score": score, "level": "HIGH", "trend_word": "市場活躍"}
    elif score >= 25:
        return {"score": score, "level": "MEDIUM", "trend_word": "市場溫和波動"}
    else:
        return {"score": score, "level": "LOW", "trend_word": "市場平靜"}


# ---------------------------------------------------------------------------
# v5 — Event Impact Score (1-5)
# ---------------------------------------------------------------------------


def score_event_impact(card: EduNewsCard) -> dict:
    """Score an event 1-5 based on final_score + content richness.

    Returns dict:
        impact: int (1-5)
        label: str (e.g. "HIGH")
        color_tag: str ("red" / "orange" / "yellow" / "gray")
    """
    base = card.final_score or 0

    # Content richness bonus
    richness = 0
    if card.fact_check_confirmed:
        richness += len(card.fact_check_confirmed)
    if card.derivable_effects:
        richness += len(card.derivable_effects)
    if card.action_items:
        richness += len(card.action_items)

    # Map to 1-5
    raw = (base / 2.0) + min(richness * 0.3, 1.5)
    impact = max(1, min(5, round(raw)))

    labels = {5: "CRITICAL", 4: "HIGH", 3: "MEDIUM", 2: "LOW", 1: "MINIMAL"}
    colors = {5: "red", 4: "orange", 3: "yellow", 2: "gray", 1: "gray"}
    return {
        "impact": impact,
        "label": labels[impact],
        "color_tag": colors[impact],
    }


# ---------------------------------------------------------------------------
# v5 — CEO Action Engine (WATCH / TEST / MOVE)
# ---------------------------------------------------------------------------

_ACTION_MOVE_KEYWORDS = re.compile(
    r"立即|緊急|urgent|immediately|ASAP|now|馬上|必須|critical|高風險",
    re.IGNORECASE,
)
_ACTION_TEST_KEYWORDS = re.compile(
    r"評估|測試|試用|pilot|POC|prototype|experiment|assess|explore|研究",
    re.IGNORECASE,
)


def build_ceo_actions(cards: list[EduNewsCard]) -> list[dict]:
    """Classify events into WATCH / TEST / MOVE actions for CEO.

    Returns list of dicts:
        action_type: "MOVE" / "TEST" / "WATCH"
        title: str (event title, ≤25 chars)
        detail: str (action description)
        owner: str
        color_tag: str ("red" / "yellow" / "gray")
    """
    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]
    results: list[dict] = []

    for card in event_cards[:8]:
        dc = build_decision_card(card)
        impact = score_event_impact(card)
        combined = f"{card.what_happened or ''} {card.why_important or ''}"
        for a in (card.action_items or []):
            combined += f" {a}"

        # Classify action type
        if impact["impact"] >= 4 or _ACTION_MOVE_KEYWORDS.search(combined):
            action_type = "MOVE"
            color_tag = "red"
        elif impact["impact"] >= 3 or _ACTION_TEST_KEYWORDS.search(combined):
            action_type = "TEST"
            color_tag = "yellow"
        else:
            action_type = "WATCH"
            color_tag = "gray"

        action_detail = dc["actions"][0] if dc["actions"] else "持續監控此事件發展"
        action_detail = _clean_text(action_detail, 60)
        # Guard (D): drop fragment/placeholder detail bullets
        from utils.semantic_quality import is_placeholder_or_fragment as _is_frag_act
        if not action_detail or _is_frag_act(action_detail):
            action_detail = "持續監控此事件發展（T+7）"
        results.append({
            "action_type": action_type,
            "title": _smart_truncate(sanitize(card.title_plain or ""), 25),
            "detail": action_detail,
            "owner": dc["owner"],
            "color_tag": color_tag,
        })

    # Sort: MOVE first, then TEST, then WATCH
    order = {"MOVE": 0, "TEST": 1, "WATCH": 2}
    results.sort(key=lambda x: order.get(x["action_type"], 3))
    return results


# ---------------------------------------------------------------------------
# v5 — Signal Summary (derive signal types from card content)
# ---------------------------------------------------------------------------


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Derive market signals from card content, classify by signal type.

    Returns list of dicts:
        signal_type: "TOOL_ADOPTION" / "USER_PAIN" / "WORKFLOW_CHANGE"
        title: str
        source_count: int (how many cards match)
        heat: str ("hot" / "warm" / "cool")
    """
    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]
    type_buckets: dict[str, list[EduNewsCard]] = {
        "TOOL_ADOPTION": [],
        "USER_PAIN": [],
        "WORKFLOW_CHANGE": [],
    }

    for card in event_cards:
        combined = (
            f"{card.title_plain or ''} {card.what_happened or ''} "
            f"{card.why_important or ''}"
        ).lower()

        best_type = "TOOL_ADOPTION"  # default
        best_hits = 0
        for sig_type, keywords in _SIGNAL_KEYWORDS.items():
            hits = sum(1 for kw in keywords if kw.lower() in combined)
            if hits > best_hits:
                best_hits = hits
                best_type = sig_type
        type_buckets[best_type].append(card)

    results: list[dict] = []
    type_labels = {
        "TOOL_ADOPTION": "工具/產品發布",
        "USER_PAIN": "風險/痛點訊號",
        "WORKFLOW_CHANGE": "工作流程變革",
    }

    for sig_type, bucket_cards in type_buckets.items():
        if not bucket_cards:
            continue
        avg_score = sum(c.final_score for c in bucket_cards) / len(bucket_cards)
        if avg_score >= 7:
            heat = "hot"
        elif avg_score >= 5:
            heat = "warm"
        else:
            heat = "cool"

        heat_score = 90 if heat == "hot" else 65 if heat == "warm" else 40
        signal_text = _smart_truncate(sanitize(bucket_cards[0].title_plain or ""), 30)
        # Guard: if title is hollow/placeholder, use the human-readable type label
        from utils.semantic_quality import is_placeholder_or_fragment as _sig_frag
        if not signal_text or _sig_frag(signal_text):
            signal_text = type_labels[sig_type]
        platform_count = len(bucket_cards)
        results.append({
            "signal_type": sig_type,
            "label": type_labels[sig_type],
            "title": signal_text,
            "source_count": platform_count,
            "heat": heat,
            "signal_text": signal_text,
            "platform_count": platform_count,
            "heat_score": heat_score,
        })

    # Sort by source_count descending
    results.sort(key=lambda x: x["source_count"], reverse=True)

    # v5.1: no-event fallback must return Top 3
    if not results:
        fallback_signals = [
            ("NO_EVENT", "Event Coverage", "No event candidates from scanned content", 0, 35),
            ("SOURCE_HEALTH", "Source Health", "Source ingestion completed with fallback coverage", 0, 45),
            ("WATCHLIST", "Watchlist", "Monitor upstream feeds for next actionable event", 0, 40),
        ]
        for sig_type, label, signal_text, platform_count, heat_score in fallback_signals:
            results.append(
                {
                    "signal_type": sig_type,
                    "label": label,
                    "title": signal_text,
                    "source_count": platform_count,
                    "heat": "cool",
                    "signal_text": signal_text,
                    "platform_count": platform_count,
                    "heat_score": heat_score,
                }
            )
        return results

    # Always return at least one signal
    if not results:
        results.append({
            "signal_type": "TOOL_ADOPTION",
            "label": "工具/產品發布",
            "title": "今日無明顯訊號",
            "source_count": 0,
            "heat": "cool",
        })

    return results


# ---------------------------------------------------------------------------
# v5 — Corp Watch (big tech monitoring)
# ---------------------------------------------------------------------------


def build_corp_watch_summary(cards: list[EduNewsCard]) -> dict:
    """Scan cards for mentions of major tech companies (Tier A + Tier B).

    Returns dict:
        tier_a: list[dict] — [{name, event_title, impact_label, action}]
        tier_b: list[dict] — [{name, event_title, impact_label, action}]
        total_mentions: int
    """
    tier_a_results: list[dict] = []
    tier_b_results: list[dict] = []

    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    seen_corps: set[str] = set()

    for card in event_cards:
        combined = f"{card.title_plain or ''} {card.what_happened or ''}"

        for corp in CORP_TIER_A:
            if corp.lower() in combined.lower() and corp not in seen_corps:
                seen_corps.add(corp)
                impact = score_event_impact(card)
                dc = build_decision_card(card)
                tier_a_results.append({
                    "name": corp,
                    "event_title": _smart_truncate(
                        sanitize(card.title_plain or ""), 30
                    ),
                    "impact_label": impact["label"],
                    "action": dc["actions"][0] if dc["actions"] else "持續監控",
                })

        for corp in CORP_TIER_B:
            if corp.lower() in combined.lower() and corp not in seen_corps:
                seen_corps.add(corp)
                impact = score_event_impact(card)
                dc = build_decision_card(card)
                tier_b_results.append({
                    "name": corp,
                    "event_title": _smart_truncate(
                        sanitize(card.title_plain or ""), 30
                    ),
                    "impact_label": impact["label"],
                    "action": dc["actions"][0] if dc["actions"] else "持續監控",
                })

    source_names = {
        str(getattr(c, "source_name", "") or "").strip()
        for c in cards
        if str(getattr(c, "source_name", "") or "").strip()
    }
    fail_counter: Counter[str] = Counter()
    for c in cards:
        if bool(getattr(c, "is_valid_news", False)):
            continue
        invalid_cause = str(getattr(c, "invalid_cause", "") or "")
        invalid_reason = str(getattr(c, "invalid_reason", "") or "")
        reason = sanitize((invalid_cause or invalid_reason or "unknown").strip())
        if reason:
            fail_counter[reason] += 1

    top_fail_reasons = [
        {"reason": reason, "count": count}
        for reason, count in fail_counter.most_common(3)
    ]
    total_mentions = len(tier_a_results) + len(tier_b_results)

    return {
        "tier_a": tier_a_results[:7],
        "tier_b": tier_b_results[:5],
        "total_mentions": total_mentions,
        "updates": total_mentions,
        "sources_total": len(source_names) if source_names else len(cards),
        "success_count": sum(1 for c in cards if bool(getattr(c, "is_valid_news", False))),
        "fail_count": sum(1 for c in cards if not bool(getattr(c, "is_valid_news", False))),
        "top_fail_reasons": top_fail_reasons,
    }


# ---------------------------------------------------------------------------
# v5.2 overrides (minimal-intrusion patch layer)
# ---------------------------------------------------------------------------

# Corp watch target list update (v5.2)
CORP_TIER_A = [
    "OpenAI",
    "Google",
    "Microsoft",
    "Amazon",
    "Meta",
    "Apple",
    "NVIDIA",
]
CORP_TIER_B = [
    "Alibaba",
    "Tencent",
    "ByteDance",
    "Baidu",
    "Huawei",
]
_ALL_CORPS = CORP_TIER_A + CORP_TIER_B

_FALLBACK_SIGNAL_POOL = [
    "TOOL_ADOPTION",
    "USER_PAIN",
    "WORKFLOW_CHANGE",
    "COST_PRESSURE",
    "COMPETITION_SIGNAL",
]

_SIGNAL_LABELS = {
    "TOOL_ADOPTION": "Tool Adoption",
    "USER_PAIN": "User Pain",
    "WORKFLOW_CHANGE": "Workflow Change",
    "COST_PRESSURE": "Cost Pressure",
    "COMPETITION_SIGNAL": "Competition Signal",
}


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Derive market signals from card content.

    Always returns Top 3 entries. When real signals are insufficient,
    backfills from a deterministic fallback pool.
    """
    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    type_buckets: dict[str, list[EduNewsCard]] = {
        "TOOL_ADOPTION": [],
        "USER_PAIN": [],
        "WORKFLOW_CHANGE": [],
    }

    for card in event_cards:
        combined = (
            f"{card.title_plain or ''} {card.what_happened or ''} "
            f"{card.why_important or ''}"
        ).lower()

        best_type = "TOOL_ADOPTION"
        best_hits = 0
        for sig_type, keywords in _SIGNAL_KEYWORDS.items():
            hits = sum(1 for kw in keywords if kw.lower() in combined)
            if hits > best_hits:
                best_hits = hits
                best_type = sig_type
        type_buckets[best_type].append(card)

    results: list[dict] = []

    for sig_type, bucket_cards in type_buckets.items():
        if not bucket_cards:
            continue

        avg_score = sum(c.final_score for c in bucket_cards) / len(bucket_cards)
        heat_score = int(max(0, min(100, round(avg_score * 12))))
        if heat_score >= 75:
            heat = "hot"
        elif heat_score >= 50:
            heat = "warm"
        else:
            heat = "cool"

        signal_text = _smart_truncate(sanitize(bucket_cards[0].title_plain or ""), 30)
        # Guard: if title is hollow/placeholder, use the human-readable signal label
        from utils.semantic_quality import is_placeholder_or_fragment as _sig_frag2
        if not signal_text or _sig_frag2(signal_text):
            signal_text = _SIGNAL_LABELS.get(sig_type, sig_type)
        source_names = {
            str(getattr(c, "source_name", "") or "").strip().lower()
            for c in bucket_cards
            if str(getattr(c, "source_name", "") or "").strip()
        }
        platform_count = len(source_names) if source_names else len(bucket_cards)

        first_snippet = bucket_cards[0].what_happened or bucket_cards[0].title_plain or ""
        example_snippet = _smart_truncate(sanitize(first_snippet), 120)

        results.append({
            "signal_name": sig_type,
            "signal_type": sig_type,
            "label": _SIGNAL_LABELS.get(sig_type, sig_type),
            "title": signal_text,
            "source_count": platform_count,
            "heat": heat,
            "signal_text": signal_text,
            "platform_count": platform_count,
            "heat_score": heat_score,
            "example_snippet": example_snippet,
        })

    results.sort(
        key=lambda x: (
            int(x.get("platform_count", 0)),
            int(x.get("heat_score", 0)),
        ),
        reverse=True,
    )

    used_names = {str(r.get("signal_name", "")) for r in results}
    fallback_scores = {
        "TOOL_ADOPTION": 60,
        "USER_PAIN": 58,
        "WORKFLOW_CHANGE": 55,
        "COST_PRESSURE": 50,
        "COMPETITION_SIGNAL": 48,
    }

    for fallback_name in _FALLBACK_SIGNAL_POOL:
        if len(results) >= 3:
            break
        if fallback_name in used_names:
            continue

        fallback_score = int(fallback_scores.get(fallback_name, 50))
        fallback_heat = "warm" if fallback_score >= 50 else "cool"
        fallback_text = _smart_truncate(
            sanitize(f"Monitoring signal: {fallback_name}"),
            30,
        )
        fallback_snippet = _smart_truncate(
            sanitize(f"Monitoring {fallback_name} due to limited event evidence."),
            120,
        )
        results.append({
            "signal_name": fallback_name,
            "signal_type": fallback_name,
            "label": _SIGNAL_LABELS.get(fallback_name, fallback_name),
            "title": fallback_text,
            "source_count": 0,
            "heat": fallback_heat,
            "signal_text": fallback_text,
            "platform_count": 0,
            "heat_score": fallback_score,
            "example_snippet": fallback_snippet,
        })

    return results[:3]


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Scan cards for mentions of major tech companies (Tier A + Tier B)."""
    tier_a_results: list[dict] = []
    tier_b_results: list[dict] = []

    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    seen_corps: set[str] = set()
    impact_values: list[int] = []

    for card in event_cards:
        combined = f"{card.title_plain or ''} {card.what_happened or ''}"

        for corp in CORP_TIER_A:
            if corp.lower() in combined.lower() and corp not in seen_corps:
                seen_corps.add(corp)
                impact = score_event_impact(card)
                impact_values.append(int(impact["impact"]))
                dc = build_decision_card(card)
                tier_a_results.append({
                    "name": corp,
                    "event_title": _smart_truncate(
                        sanitize(card.title_plain or ""), 30
                    ),
                    "impact_label": impact["label"],
                    "action": dc["actions"][0] if dc["actions"] else "Monitor follow-up",
                })

        for corp in CORP_TIER_B:
            if corp.lower() in combined.lower() and corp not in seen_corps:
                seen_corps.add(corp)
                impact = score_event_impact(card)
                impact_values.append(int(impact["impact"]))
                dc = build_decision_card(card)
                tier_b_results.append({
                    "name": corp,
                    "event_title": _smart_truncate(
                        sanitize(card.title_plain or ""), 30
                    ),
                    "impact_label": impact["label"],
                    "action": dc["actions"][0] if dc["actions"] else "Monitor follow-up",
                })

    source_names = {
        str(getattr(c, "source_name", "") or "").strip()
        for c in cards
        if str(getattr(c, "source_name", "") or "").strip()
    }

    fail_counter: Counter[str] = Counter()
    for c in cards:
        if bool(getattr(c, "is_valid_news", False)):
            continue
        invalid_cause = str(getattr(c, "invalid_cause", "") or "")
        invalid_reason = str(getattr(c, "invalid_reason", "") or "")
        reason = sanitize((invalid_cause or invalid_reason or "unknown").strip())
        if reason:
            fail_counter[reason] += 1

    top_fail_reasons = [
        {"reason": reason, "count": count}
        for reason, count in fail_counter.most_common(3)
    ]

    metrics_dict = metrics or {}
    total_mentions = len(tier_a_results) + len(tier_b_results)
    mentions_count = total_mentions

    if impact_values:
        impact_score_avg = round(sum(impact_values) / len(impact_values), 2)
    else:
        impact_score_avg = 0.0

    if mentions_count == 0:
        trend_direction = "STABLE"
    elif impact_score_avg >= 3.8 or mentions_count >= 3:
        trend_direction = "UP"
    elif impact_score_avg <= 2.0:
        trend_direction = "DOWN"
    else:
        trend_direction = "STABLE"

    sources_total = int(
        metrics_dict.get(
            "sources_total",
            len(source_names) if source_names else len(cards),
        )
    )
    success_count = int(
        metrics_dict.get(
            "sources_success",
            sum(1 for c in cards if bool(getattr(c, "is_valid_news", False))),
        )
    )
    fail_count = int(
        metrics_dict.get(
            "sources_failed",
            sum(1 for c in cards if not bool(getattr(c, "is_valid_news", False))),
        )
    )

    status_message = (
        "No major updates detected — monitoring continues."
        if mentions_count == 0
        else f"{mentions_count} tracked companies with notable updates."
    )

    return {
        "tier_a": tier_a_results[:7],
        "tier_b": tier_b_results[:5],
        "total_mentions": total_mentions,
        "mentions_count": mentions_count,
        "impact_score_avg": impact_score_avg,
        "trend_direction": trend_direction,
        "status_message": status_message,
        "updates": total_mentions,
        "sources_total": sources_total,
        "success_count": success_count,
        "fail_count": fail_count,
        "top_fail_reasons": top_fail_reasons,
    }


# ---------------------------------------------------------------------------
# v5.2.2 final overrides (must be last in file)
# ---------------------------------------------------------------------------

_v522_prev_build_corp_watch_summary = build_corp_watch_summary


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Final override: no placeholder text, always return Top 3 informative signals."""
    event_cards = [c for c in cards if c.is_valid_news and not is_non_event_or_index(c)]
    valid_cards = [c for c in cards if c.is_valid_news]
    base_cards = event_cards if event_cards else valid_cards
    passed_total_count = len(valid_cards)

    bucket_names = (
        ["TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE"]
        if event_cards
        else list(_FALLBACK_SIGNAL_POOL)
    )
    buckets: dict[str, list[EduNewsCard]] = {name: [] for name in bucket_names}
    allowed_types = set(buckets.keys()) | {"TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE"}

    for card in base_cards:
        combined = (
            f"{card.title_plain or ''} {card.what_happened or ''} "
            f"{card.why_important or ''}"
        ).lower()
        sig_type = _v521_resolve_signal_type(combined)
        if sig_type not in allowed_types:
            sig_type = "TOOL_ADOPTION"
        buckets.setdefault(sig_type, []).append(card)

    entries: list[dict] = []
    for sig_name, bucket_cards in buckets.items():
        if not bucket_cards:
            continue
        entries.append(
            _v522_signal_entry(
                sig_name,
                bucket_cards,
                passed_total_count=passed_total_count,
            )
        )

    entries.sort(
        key=lambda x: (
            int(x.get("platform_count", 0)),
            int(x.get("heat_score", 0)),
        ),
        reverse=True,
    )

    used_names = {str(e.get("signal_name", "")) for e in entries}
    fallback_idx = 0
    while len(entries) < 3:
        fallback_name = _FALLBACK_SIGNAL_POOL[fallback_idx % len(_FALLBACK_SIGNAL_POOL)]
        fallback_idx += 1
        if fallback_name in used_names:
            continue

        template_card = base_cards[(fallback_idx - 1) % len(base_cards)] if base_cards else None
        card_list = [template_card] if template_card is not None else []
        entry = _v522_signal_entry(
            fallback_name,
            card_list,
            passed_total_count=passed_total_count,
        )
        if template_card is None:
            entry["example_snippet"] = _smart_truncate(
                f"Source scan completed with no gate-passed snippets; continue monitoring {fallback_name.lower()} coverage.",
                120,
            )
            entry["source_name"] = "none"
        entries.append(entry)
        used_names.add(fallback_name)

    for entry in entries[:3]:
        entry["signal_text"] = _v522_strip_placeholders(str(entry.get("signal_text", ""))) or str(entry.get("label", "Signal"))
        entry["title"] = _v522_strip_placeholders(str(entry.get("title", ""))) or entry["signal_text"]
        snippet = _v522_strip_placeholders(str(entry.get("example_snippet", "")))
        if len(snippet) < 30:
            snippet = _smart_truncate(
                f"{entry['signal_text']} observed on {entry.get('source_name', 'unknown')} with platform_count={entry.get('platform_count', 1)}.",
                120,
            )
        entry["example_snippet"] = snippet
        entry["platform_count"] = max(int(entry.get("platform_count", 1)), 1)
        entry["source_count"] = entry["platform_count"]
        entry["heat_score"] = max(int(entry.get("heat_score", 30)), 30)
    return entries[:3]


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Final override: keep corp watch informative when no updates are detected."""
    result = _v522_prev_build_corp_watch_summary(cards, metrics=metrics)

    source_rows: dict[str, dict[str, int | str]] = {}
    for card in cards:
        src = _v522_source_name(card)
        row = source_rows.setdefault(
            src,
            {"source_name": src, "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0},
        )
        row["items_seen"] = int(row["items_seen"]) + 1
        if bool(getattr(card, "is_valid_news", False)):
            row["gate_pass"] = int(row["gate_pass"]) + 1
        if bool(getattr(card, "is_soft_pass", False)) or str(getattr(card, "gate_level", "")).lower() == "soft":
            row["gate_soft_pass"] = int(row["gate_soft_pass"]) + 1

    top_sources = sorted(
        source_rows.values(),
        key=lambda x: int(x["items_seen"]),
        reverse=True,
    )[:3]
    if not top_sources:
        top_sources = [{"source_name": "none", "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0}]

    top_fail_reasons = result.get("top_fail_reasons") or []
    if not top_fail_reasons:
        top_fail_reasons = [{"reason": "none", "count": 0}]

    mentions_count = int(result.get("mentions_count", result.get("total_mentions", 0)))
    if mentions_count == 0:
        result["status_message"] = "No major updates detected — monitoring continues."

    result["top_fail_reasons"] = top_fail_reasons
    result["top_sources"] = top_sources
    return result


# ---------------------------------------------------------------------------
# v5.2.2 overrides (append-only quality hotfix layer)
# ---------------------------------------------------------------------------

_v521_build_structured_executive_summary = build_structured_executive_summary
_v521_build_corp_watch_summary = build_corp_watch_summary

_PLACEHOLDER_GUARD_TERMS = (
    "desktop smoke signal",
    "fallback monitoring signal",
    "signals_insufficient=true",
    "last july was",
)


def _v522_strip_placeholders(text: str) -> str:
    cleaned = sanitize(text or "")
    if not cleaned:
        return ""
    for term in _PLACEHOLDER_GUARD_TERMS:
        cleaned = re.sub(re.escape(term), "", cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r"\b(?:was|is|are)\s*\.\.\.", "", cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r"\s{2,}", " ", cleaned).strip(" ,.;:")
    return cleaned.strip()


def _v522_source_name(card: EduNewsCard) -> str:
    source_name = str(getattr(card, "source_name", "") or "").strip()
    if source_name:
        return source_name
    source_url = str(getattr(card, "source_url", "") or "").strip()
    if not source_url:
        return "unknown"
    try:
        from urllib.parse import urlparse

        domain = urlparse(source_url).netloc.strip().lower()
        return domain or "unknown"
    except Exception:
        return "unknown"


def _v522_signal_snippet(card: EduNewsCard | None, fallback_label: str) -> str:
    if card is None:
        return f"Monitoring {fallback_label} from source scan statistics and recent validated coverage."

    fields = [
        str(getattr(card, "what_happened", "") or "").strip(),
        str(getattr(card, "why_important", "") or "").strip(),
        str(getattr(card, "title_plain", "") or "").strip(),
    ]
    merged = " ".join(p for p in fields if p)
    merged = _v522_strip_placeholders(merged)
    if len(merged) < 30:
        merged = _v522_strip_placeholders(
            f"{fields[2]} | {fields[0]} | 來源：{_v522_source_name(card)}"
        )
    merged = merged or f"{fallback_label} signal from {_v522_source_name(card)}."
    return _smart_truncate(merged, 120)


def _v522_signal_entry(
    signal_name: str,
    bucket_cards: list[EduNewsCard],
    *,
    passed_total_count: int,
) -> dict:
    card = bucket_cards[0] if bucket_cards else None
    label = _SIGNAL_LABELS.get(signal_name, signal_name)
    unique_sources = {_v522_source_name(c) for c in bucket_cards if _v522_source_name(c) != "unknown"}
    platform_count = max(len(unique_sources), len(bucket_cards), 1)

    avg_score = (
        sum(float(getattr(c, "final_score", 0.0) or 0.0) for c in bucket_cards) / len(bucket_cards)
        if bucket_cards else 3.0
    )
    heat_score = int(max(30, min(100, round(avg_score * 12))))
    if heat_score >= 75:
        heat = "hot"
    elif heat_score >= 50:
        heat = "warm"
    else:
        heat = "cool"

    signal_text_raw = (
        (str(getattr(card, "title_plain", "") or "").strip() if card else "")
        or (str(getattr(card, "what_happened", "") or "").strip() if card else "")
        or label
    )
    signal_text = _smart_truncate(_v522_strip_placeholders(signal_text_raw) or label, 30)

    return {
        "signal_name": signal_name,
        "signal_type": signal_name,
        "label": label,
        "title": signal_text,
        "source_count": platform_count,
        "heat": heat,
        "signal_text": signal_text,
        "platform_count": platform_count,
        "heat_score": heat_score,
        "example_snippet": _v522_signal_snippet(card, label),
        "source_name": _v522_source_name(card) if card else "none",
        "signals_insufficient": passed_total_count < 3,
        "passed_total_count": passed_total_count,
    }


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """v5.2.2 signal summary: never emit placeholder text, keep Top 3 informative."""
    event_cards = [c for c in cards if c.is_valid_news and not is_non_event_or_index(c)]
    valid_cards = [c for c in cards if c.is_valid_news]

    base_cards = event_cards if event_cards else valid_cards
    passed_total_count = len(valid_cards)
    bucket_names = (
        ["TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE"]
        if event_cards
        else list(_FALLBACK_SIGNAL_POOL)
    )
    buckets: dict[str, list[EduNewsCard]] = {name: [] for name in bucket_names}
    allowed_types = set(buckets.keys()) | {"TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE"}

    for card in base_cards:
        combined = (
            f"{card.title_plain or ''} {card.what_happened or ''} "
            f"{card.why_important or ''}"
        ).lower()
        sig_type = _v521_resolve_signal_type(combined)
        if sig_type not in allowed_types:
            sig_type = "TOOL_ADOPTION"
        buckets.setdefault(sig_type, []).append(card)

    entries: list[dict] = []
    for sig_name, bucket_cards in buckets.items():
        if not bucket_cards:
            continue
        entries.append(
            _v522_signal_entry(
                sig_name,
                bucket_cards,
                passed_total_count=passed_total_count,
            )
        )

    entries.sort(
        key=lambda x: (
            int(x.get("platform_count", 0)),
            int(x.get("heat_score", 0)),
        ),
        reverse=True,
    )

    used_names = {str(e.get("signal_name", "")) for e in entries}
    fallback_idx = 0
    fallback_fill_pool = list(_FALLBACK_SIGNAL_POOL)
    while len(entries) < 3:
        fallback_name = fallback_fill_pool[fallback_idx % len(fallback_fill_pool)]
        fallback_idx += 1
        if fallback_name in used_names:
            continue

        template_card = base_cards[(fallback_idx - 1) % len(base_cards)] if base_cards else None
        card_list = [template_card] if template_card is not None else []
        entry = _v522_signal_entry(
            fallback_name,
            card_list,
            passed_total_count=passed_total_count,
        )
        # No source cards at all: keep signal informative without placeholder vocabulary.
        if template_card is None:
            entry["example_snippet"] = _smart_truncate(
                f"Source scan completed with no gate-passed snippets; continue monitoring {fallback_name.lower()} coverage.",
                120,
            )
            entry["source_name"] = "none"
        entries.append(entry)
        used_names.add(fallback_name)

    # Final guard: strip placeholder/fallback fragments from visible text.
    for entry in entries[:3]:
        entry["signal_text"] = _v522_strip_placeholders(str(entry.get("signal_text", ""))) or str(entry.get("label", "Signal"))
        entry["title"] = _v522_strip_placeholders(str(entry.get("title", ""))) or entry["signal_text"]
        snippet = _v522_strip_placeholders(str(entry.get("example_snippet", "")))
        if len(snippet) < 30:
            snippet = _smart_truncate(
                f"{entry['signal_text']} observed on {entry.get('source_name', 'unknown')} with platform_count={entry.get('platform_count', 1)}.",
                120,
            )
        entry["example_snippet"] = snippet
        entry["platform_count"] = max(int(entry.get("platform_count", 1)), 1)
        entry["source_count"] = entry["platform_count"]
        entry["heat_score"] = max(int(entry.get("heat_score", 30)), 30)
    return entries[:3]


def _v522_metrics_int(metrics: dict | None, key: str, default: int) -> int:
    if not metrics:
        return int(default)
    try:
        return int(metrics.get(key, default))
    except Exception:
        return int(default)


def build_structured_executive_summary(
    news_cards: list[EduNewsCard],
    tone: str = "neutral",
    metrics: dict | None = None,
) -> dict[str, list[str]]:
    """v5.2.2 structured summary: no-event branch must still be information-dense."""
    event_cards = [c for c in news_cards if c.is_valid_news and not is_non_event_or_index(c)]
    if event_cards:
        return _v521_build_structured_executive_summary(news_cards, tone)

    fetched_total = _v522_metrics_int(metrics, "fetched_total", len(news_cards))
    deduped_total = _v522_metrics_int(metrics, "deduped_total", len(news_cards))
    gate_pass_total = _v522_metrics_int(
        metrics,
        "gate_pass_total",
        sum(1 for c in news_cards if bool(getattr(c, "is_valid_news", False))),
    )
    gate_reject_total = _v522_metrics_int(metrics, "gate_reject_total", max(fetched_total - gate_pass_total, 0))
    after_filter_total = _v522_metrics_int(metrics, "after_filter_total", gate_pass_total)
    sources_total = _v522_metrics_int(
        metrics,
        "sources_total",
        len({_v522_source_name(c) for c in news_cards if _v522_source_name(c) != "unknown"}),
    )
    signals_total = _v522_metrics_int(metrics, "signals_detected", 0)

    top_sources_counter: Counter[str] = Counter()
    keyword_counter: Counter[str] = Counter()
    for card in news_cards:
        src = _v522_source_name(card)
        if src and src != "unknown":
            top_sources_counter[src] += 1
        title = _v522_strip_placeholders(str(getattr(card, "title_plain", "") or ""))
        for token in re.findall(r"[A-Za-z][A-Za-z0-9_-]{3,}", title):
            token_lower = token.lower()
            if token_lower not in {"today", "daily", "news", "update"}:
                keyword_counter[token.upper()] += 1

    top_sources = ", ".join(f"{name}({count})" for name, count in top_sources_counter.most_common(3)) or "none"
    top_keywords = ", ".join(k for k, _ in keyword_counter.most_common(3)) or "none"

    return {
        "ai_trends": [
            (
                f"Scan coverage: fetched_total={fetched_total}, deduped_total={deduped_total}, "
                f"gate_pass_total={gate_pass_total}, gate_reject_total={gate_reject_total}."
            )
        ],
        "tech_landing": [
            (
                f"Source health: sources_total={sources_total}, after_filter_total={after_filter_total}, "
                f"signals_total={signals_total}."
            )
        ],
        "market_competition": [f"Top sources by volume: {top_sources}."],
        "opportunities_risks": [f"Observed keywords from scanned titles: {top_keywords}."],
        "recommended_actions": [
            "Maintain monitoring cadence and prioritize enrichment for high-volume sources with low gate pass rates."
        ],
    }


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """v5.2.2 corp watch: keep scan stats informative even with zero updates."""
    result = _v521_build_corp_watch_summary(cards, metrics=metrics)

    source_rows: dict[str, dict[str, int | str]] = {}
    for card in cards:
        src = _v522_source_name(card)
        row = source_rows.setdefault(
            src,
            {"source_name": src, "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0},
        )
        row["items_seen"] = int(row["items_seen"]) + 1
        if bool(getattr(card, "is_valid_news", False)):
            row["gate_pass"] = int(row["gate_pass"]) + 1
        if bool(getattr(card, "is_soft_pass", False)) or str(getattr(card, "gate_level", "")).lower() == "soft":
            row["gate_soft_pass"] = int(row["gate_soft_pass"]) + 1

    top_sources = sorted(
        source_rows.values(),
        key=lambda x: int(x["items_seen"]),
        reverse=True,
    )[:3]
    if not top_sources:
        top_sources = [{"source_name": "none", "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0}]

    top_fail_reasons = result.get("top_fail_reasons") or []
    if not top_fail_reasons:
        top_fail_reasons = [{"reason": "none", "count": 0}]

    mentions_count = int(result.get("mentions_count", result.get("total_mentions", 0)))
    if mentions_count == 0:
        result["status_message"] = "No major updates detected — monitoring continues."
    result["top_fail_reasons"] = top_fail_reasons
    result["top_sources"] = top_sources
    return result

# ---------------------------------------------------------------------------
# v5.2.1 overrides (append-only hotfix layer)
# ---------------------------------------------------------------------------

# Keep the monitored corp list explicit and complete.
CORP_TIER_A = [
    "OpenAI",
    "Google",
    "Microsoft",
    "Amazon",
    "Meta",
    "Apple",
    "NVIDIA",
]
CORP_TIER_B = [
    "Alibaba",
    "Tencent",
    "ByteDance",
    "Baidu",
    "Huawei",
]
_ALL_CORPS = CORP_TIER_A + CORP_TIER_B

_FALLBACK_SIGNAL_POOL = [
    "TOOL_ADOPTION",
    "USER_PAIN",
    "WORKFLOW_CHANGE",
    "COST_PRESSURE",
    "COMPETITION_SIGNAL",
]

_SIGNAL_LABELS = {
    "TOOL_ADOPTION": "Tool Adoption",
    "USER_PAIN": "User Pain",
    "WORKFLOW_CHANGE": "Workflow Change",
    "COST_PRESSURE": "Cost Pressure",
    "COMPETITION_SIGNAL": "Competition Signal",
}

_V521_EXTRA_SIGNAL_KEYWORDS = {
    "COST_PRESSURE": [
        "cost", "pricing", "margin", "budget", "token price", "spend", "expense",
    ],
    "COMPETITION_SIGNAL": [
        "rival", "competition", "race", "market share", "challenger", "benchmark",
    ],
}


def _v521_strip_signal_placeholders(text: str) -> str:
    cleaned = text or ""
    cleaned = re.sub(r"(?i)fallback monitoring signal", "", cleaned)
    cleaned = re.sub(r"(?i)last\s+\w+\s+was\.{3,}", "", cleaned)
    return cleaned.strip()


def _v521_resolve_signal_type(combined: str) -> str:
    best_type = "TOOL_ADOPTION"
    best_hits = 0

    for sig_type, keywords in _SIGNAL_KEYWORDS.items():
        hits = sum(1 for kw in keywords if kw.lower() in combined)
        if hits > best_hits:
            best_hits = hits
            best_type = sig_type

    for sig_type, keywords in _V521_EXTRA_SIGNAL_KEYWORDS.items():
        hits = sum(1 for kw in keywords if kw.lower() in combined)
        if hits > best_hits:
            best_hits = hits
            best_type = sig_type

    return best_type


def _v521_heat_from_avg_score(avg_score: float) -> tuple[str, int]:
    heat_score = int(max(30, min(100, round(avg_score * 12))))
    if heat_score >= 75:
        return "hot", heat_score
    if heat_score >= 50:
        return "warm", heat_score
    return "cool", heat_score


def _v521_build_signal_entry(signal_name: str, bucket_cards: list[EduNewsCard]) -> dict:
    source_names = {
        str(getattr(c, "source_name", "") or "").strip().lower()
        for c in bucket_cards
        if str(getattr(c, "source_name", "") or "").strip()
    }
    platform_count = len(source_names) if source_names else len(bucket_cards)
    platform_count = max(platform_count, 1)

    avg_score = (
        sum(float(getattr(c, "final_score", 0.0) or 0.0) for c in bucket_cards) / len(bucket_cards)
        if bucket_cards else 3.0
    )
    heat, heat_score = _v521_heat_from_avg_score(avg_score)

    first = bucket_cards[0] if bucket_cards else None
    signal_text_raw = (
        (first.title_plain if first else "")
        or (first.what_happened if first else "")
        or _SIGNAL_LABELS.get(signal_name, signal_name)
    )
    signal_text = _smart_truncate(
        sanitize(_v521_strip_signal_placeholders(signal_text_raw)),
        30,
    )
    if not signal_text:
        signal_text = _SIGNAL_LABELS.get(signal_name, signal_name)

    snippet_raw = (
        (first.what_happened if first else "")
        or (first.why_important if first else "")
        or signal_text
    )
    example_snippet = _smart_truncate(
        sanitize(_v521_strip_signal_placeholders(snippet_raw)),
        120,
    )
    if not example_snippet:
        example_snippet = _smart_truncate(
            f"Tracking {_SIGNAL_LABELS.get(signal_name, signal_name)} from validated sources.",
            120,
        )

    return {
        "signal_name": signal_name,
        "signal_type": signal_name,
        "label": _SIGNAL_LABELS.get(signal_name, signal_name),
        "title": signal_text,
        "source_count": platform_count,
        "heat": heat,
        "signal_text": signal_text,
        "platform_count": platform_count,
        "heat_score": heat_score,
        "example_snippet": example_snippet,
    }


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Derive market signals from card content.

    Event path keeps prior behavior; no-event path always returns Top 3
    concrete signals with non-zero platform_count and usable snippets.
    """
    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    if event_cards:
        type_buckets: dict[str, list[EduNewsCard]] = {
            "TOOL_ADOPTION": [],
            "USER_PAIN": [],
            "WORKFLOW_CHANGE": [],
        }
        for card in event_cards:
            combined = (
                f"{card.title_plain or ''} {card.what_happened or ''} "
                f"{card.why_important or ''}"
            ).lower()
            best_type = "TOOL_ADOPTION"
            best_hits = 0
            for sig_type in ("TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE"):
                keywords = _SIGNAL_KEYWORDS.get(sig_type, [])
                hits = sum(1 for kw in keywords if kw.lower() in combined)
                if hits > best_hits:
                    best_hits = hits
                    best_type = sig_type
            type_buckets[best_type].append(card)

        results = [
            _v521_build_signal_entry(sig_type, bucket_cards)
            for sig_type, bucket_cards in type_buckets.items()
            if bucket_cards
        ]
        results.sort(
            key=lambda x: (
                int(x.get("platform_count", 0)),
                int(x.get("heat_score", 0)),
            ),
            reverse=True,
        )
        return results

    valid_cards = [c for c in cards if c.is_valid_news]
    passed_total_count = len(valid_cards)
    try:
        from config import settings as _settings

        min_keep_signals = int(getattr(_settings, "CONTENT_GATE_MIN_KEEP_SIGNALS", 9) or 9)
    except Exception:
        min_keep_signals = 9

    signal_source_cards = valid_cards[: max(1, min_keep_signals)]

    buckets: dict[str, list[EduNewsCard]] = {
        name: [] for name in _FALLBACK_SIGNAL_POOL
    }
    for card in signal_source_cards:
        combined = (
            f"{card.title_plain or ''} {card.what_happened or ''} "
            f"{card.why_important or ''}"
        ).lower()
        sig_type = _v521_resolve_signal_type(combined)
        if sig_type not in buckets:
            sig_type = "TOOL_ADOPTION"
        buckets[sig_type].append(card)

    results: list[dict] = []
    for sig_name, bucket_cards in buckets.items():
        if not bucket_cards:
            continue
        entry = _v521_build_signal_entry(sig_name, bucket_cards)
        entry["heat_score"] = max(int(entry.get("heat_score", 30)), 30)
        entry["platform_count"] = max(int(entry.get("platform_count", 1)), 1)
        entry["source_count"] = entry["platform_count"]
        entry["signals_insufficient"] = passed_total_count < 3
        entry["passed_total_count"] = passed_total_count
        results.append(entry)

    results.sort(
        key=lambda x: (
            int(x.get("platform_count", 0)),
            int(x.get("heat_score", 0)),
        ),
        reverse=True,
    )

    used_names = {str(r.get("signal_name", "")) for r in results}
    template_cards = signal_source_cards if signal_source_cards else valid_cards

    for idx, fallback_name in enumerate(_FALLBACK_SIGNAL_POOL):
        if len(results) >= 3:
            break
        if fallback_name in used_names:
            continue

        if template_cards:
            entry = _v521_build_signal_entry(
                fallback_name,
                [template_cards[idx % len(template_cards)]],
            )
            entry["heat_score"] = max(int(entry.get("heat_score", 30)), 30)
            entry["platform_count"] = max(int(entry.get("platform_count", 1)), 1)
            entry["source_count"] = entry["platform_count"]
            entry["signals_insufficient"] = passed_total_count < 3
            entry["passed_total_count"] = passed_total_count
            results.append(entry)
            continue

        fallback_score = max(30, 45 - idx * 3)
        fallback_heat = "warm" if fallback_score >= 50 else "cool"
        fallback_text = _SIGNAL_LABELS.get(fallback_name, fallback_name)
        results.append(
            {
                "signal_name": fallback_name,
                "signal_type": fallback_name,
                "label": fallback_text,
                "title": fallback_text,
                "source_count": 1,
                "heat": fallback_heat,
                "signal_text": fallback_text,
                "platform_count": 1,
                "heat_score": fallback_score,
                "example_snippet": _smart_truncate(
                    f"Monitoring {fallback_text} from source coverage baselines.",
                    120,
                ),
                "signals_insufficient": True,
                "passed_total_count": passed_total_count,
            }
        )

    final = results[:3]
    if len(final) < 3:
        for sig in final:
            sig["signals_insufficient"] = True
            sig["passed_total_count"] = passed_total_count
    return final


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Scan cards for mentions of major tech companies (Tier A + Tier B)."""
    tier_a_results: list[dict] = []
    tier_b_results: list[dict] = []

    event_cards = [
        c for c in cards
        if c.is_valid_news and not is_non_event_or_index(c)
    ]

    seen_corps: set[str] = set()
    impact_values: list[int] = []

    for card in event_cards:
        combined = f"{card.title_plain or ''} {card.what_happened or ''}"

        for corp in CORP_TIER_A:
            if corp.lower() in combined.lower() and corp not in seen_corps:
                seen_corps.add(corp)
                impact = score_event_impact(card)
                impact_values.append(int(impact["impact"]))
                dc = build_decision_card(card)
                tier_a_results.append(
                    {
                        "name": corp,
                        "event_title": _smart_truncate(
                            sanitize(card.title_plain or ""),
                            30,
                        ),
                        "impact_label": impact["label"],
                        "action": dc["actions"][0] if dc["actions"] else "Monitor follow-up",
                    }
                )

        for corp in CORP_TIER_B:
            if corp.lower() in combined.lower() and corp not in seen_corps:
                seen_corps.add(corp)
                impact = score_event_impact(card)
                impact_values.append(int(impact["impact"]))
                dc = build_decision_card(card)
                tier_b_results.append(
                    {
                        "name": corp,
                        "event_title": _smart_truncate(
                            sanitize(card.title_plain or ""),
                            30,
                        ),
                        "impact_label": impact["label"],
                        "action": dc["actions"][0] if dc["actions"] else "Monitor follow-up",
                    }
                )

    source_names = {
        str(getattr(c, "source_name", "") or "").strip()
        for c in cards
        if str(getattr(c, "source_name", "") or "").strip()
    }

    fail_counter: Counter[str] = Counter()
    for c in cards:
        if bool(getattr(c, "is_valid_news", False)):
            continue
        invalid_cause = str(getattr(c, "invalid_cause", "") or "")
        invalid_reason = str(getattr(c, "invalid_reason", "") or "")
        reason = sanitize((invalid_cause or invalid_reason or "unknown").strip())
        if reason:
            fail_counter[reason] += 1

    top_fail_reasons = [
        {"reason": reason, "count": count}
        for reason, count in fail_counter.most_common(3)
    ]
    if not top_fail_reasons:
        top_fail_reasons = [{"reason": "none", "count": 0}]

    metrics_dict = metrics or {}
    total_mentions = len(tier_a_results) + len(tier_b_results)
    mentions_count = total_mentions

    if impact_values:
        impact_score_avg = round(sum(impact_values) / len(impact_values), 2)
    else:
        impact_score_avg = 0.0

    if mentions_count == 0:
        trend_direction = "STABLE"
    elif impact_score_avg >= 3.8 or mentions_count >= 3:
        trend_direction = "UP"
    elif impact_score_avg <= 2.0:
        trend_direction = "DOWN"
    else:
        trend_direction = "STABLE"

    sources_total = int(
        metrics_dict.get(
            "sources_total",
            len(source_names) if source_names else len(cards),
        )
    )
    success_count = int(
        metrics_dict.get(
            "sources_success",
            sum(1 for c in cards if bool(getattr(c, "is_valid_news", False))),
        )
    )
    fail_count = int(
        metrics_dict.get(
            "sources_failed",
            sum(1 for c in cards if not bool(getattr(c, "is_valid_news", False))),
        )
    )

    status_message = (
        "No major updates detected — monitoring continues."
        if mentions_count == 0
        else f"{mentions_count} tracked companies with notable updates."
    )

    return {
        "tier_a": tier_a_results[:7],
        "tier_b": tier_b_results[:5],
        "total_mentions": total_mentions,
        "mentions_count": mentions_count,
        "impact_score_avg": impact_score_avg,
        "trend_direction": trend_direction,
        "status_message": status_message,
        "updates": total_mentions,
        "sources_total": sources_total,
        "success_count": success_count,
        "fail_count": fail_count,
        "top_fail_reasons": top_fail_reasons,
    }

# ---------------------------------------------------------------------------
# v5.2.2 terminal wrappers (absolute last definitions)
# ---------------------------------------------------------------------------

_v522_terminal_signal_summary = build_signal_summary
_v522_terminal_corp_summary = build_corp_watch_summary


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Terminal wrapper: enforce non-placeholder Top3 with source metadata."""
    rows = list(_v522_terminal_signal_summary(cards) or [])

    # Ensure 3 rows minimum and strip known placeholders from visible text.
    fallback_names = ["TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE", "COST_PRESSURE", "COMPETITION_SIGNAL"]
    idx = 0
    while len(rows) < 3:
        name = fallback_names[idx % len(fallback_names)]
        idx += 1
        rows.append(
            {
                "signal_name": name,
                "signal_type": name,
                "label": _SIGNAL_LABELS.get(name, name),
                "title": _SIGNAL_LABELS.get(name, name),
                "source_count": 1,
                "heat": "cool",
                "signal_text": _SIGNAL_LABELS.get(name, name),
                "platform_count": 1,
                "heat_score": 30,
                "example_snippet": f"Monitoring {_SIGNAL_LABELS.get(name, name)} from validated source coverage.",
                "source_name": "none",
            }
        )

    for row in rows[:3]:
        signal_text = _v522_strip_placeholders(str(row.get("signal_text", row.get("title", ""))))
        title = _v522_strip_placeholders(str(row.get("title", signal_text)))
        snippet = _v522_strip_placeholders(str(row.get("example_snippet", "")))

        source_name = str(row.get("source_name", "")).strip()
        if not source_name:
            source_name = "unknown"

        if len(snippet) < 30:
            snippet = _smart_truncate(
                f"{signal_text or title or 'Signal'} observed on {source_name} with platform_count={int(row.get('platform_count', 1) or 1)}.",
                120,
            )

        row["signal_text"] = signal_text or title or str(row.get("label", "Signal"))
        row["title"] = title or row["signal_text"]
        row["example_snippet"] = snippet
        row["source_name"] = source_name
        row["platform_count"] = max(int(row.get("platform_count", row.get("source_count", 1)) or 1), 1)
        row["source_count"] = row["platform_count"]
        row["heat_score"] = max(int(row.get("heat_score", 30) or 30), 30)

    return rows[:3]


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Terminal wrapper: always include top_sources and non-empty fail reasons."""
    result = dict(_v522_terminal_corp_summary(cards, metrics=metrics) or {})

    source_rows: dict[str, dict[str, int | str]] = {}
    for card in cards:
        src = _v522_source_name(card)
        row = source_rows.setdefault(
            src,
            {"source_name": src, "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0},
        )
        row["items_seen"] = int(row["items_seen"]) + 1
        if bool(getattr(card, "is_valid_news", False)):
            row["gate_pass"] = int(row["gate_pass"]) + 1
        if bool(getattr(card, "is_soft_pass", False)) or str(getattr(card, "gate_level", "")).lower() == "soft":
            row["gate_soft_pass"] = int(row["gate_soft_pass"]) + 1

    top_sources = sorted(source_rows.values(), key=lambda x: int(x["items_seen"]), reverse=True)[:3]
    if not top_sources:
        top_sources = [{"source_name": "none", "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0}]

    top_fail_reasons = result.get("top_fail_reasons") or []
    if not top_fail_reasons:
        top_fail_reasons = [{"reason": "none", "count": 0}]

    mentions_count = int(result.get("mentions_count", result.get("total_mentions", 0) or 0))
    if mentions_count == 0:
        result["status_message"] = "No major updates detected — monitoring continues."

    result["top_sources"] = top_sources
    result["top_fail_reasons"] = top_fail_reasons
    return result


# ---------------------------------------------------------------------------
# v5.2.3 terminal wrappers (info-density hard gate + non-empty fallbacks)
# ---------------------------------------------------------------------------

_v523_prev_build_signal_summary = build_signal_summary
_v523_prev_build_corp_watch_summary = build_corp_watch_summary
_v523_prev_build_structured_summary = build_structured_executive_summary
_v523_prev_build_ceo_brief_blocks = build_ceo_brief_blocks


def _v523_card_source(card: EduNewsCard) -> str:
    source = str(getattr(card, "source_name", "") or "").strip()
    if source:
        return source
    url = str(getattr(card, "source_url", "") or "").strip()
    if not url:
        return "platform"
    try:
        from urllib.parse import urlparse

        domain = urlparse(url).netloc.strip().lower()
        return domain or "platform"
    except Exception:
        return "platform"


def _v523_platform_stats(cards: list[EduNewsCard], passed_ids: set[str]) -> list[dict[str, int | str]]:
    rows: dict[str, dict[str, int | str]] = {}
    for card in cards:
        src = _v523_card_source(card)
        row = rows.setdefault(
            src,
            {"source_name": src, "items_seen": 0, "heat_sum": 0, "gate_pass": 0},
        )
        row["items_seen"] = int(row["items_seen"]) + 1
        score = float(getattr(card, "final_score", 0.0) or 0.0)
        row["heat_sum"] = int(row["heat_sum"]) + int(max(score * 10, 0))
        if str(getattr(card, "item_id", "")) in passed_ids:
            row["gate_pass"] = int(row["gate_pass"]) + 1
    return sorted(rows.values(), key=lambda x: (int(x["gate_pass"]), int(x["heat_sum"])), reverse=True)


def _v523_extract_evidence_tokens(
    text: str,
    *,
    source_name: str = "",
    heat_score: int | None = None,
) -> list[str]:
    payload = str(text or "")
    tokens: list[str] = []

    # Numbers/versions/dates and simple units.
    for m in re.findall(
        r"\b(?:20\d{2}|v?\d+(?:\.\d+){0,2}(?:%|x|ms|gb|mb|k|m|b|tokens?)?)\b",
        payload,
        flags=re.IGNORECASE,
    ):
        if m:
            tokens.append(m)

    # Verifiable entities (companies/models/tools).
    for m in re.findall(
        r"\b(?:OpenAI|Google|Microsoft|Amazon|Meta|Apple|NVIDIA|Alibaba|Tencent|ByteDance|Baidu|Huawei|"
        r"GPT-?\d+(?:\.\d+)?|Claude|Gemini|Llama|Qwen|DeepSeek|Copilot)\b",
        payload,
        flags=re.IGNORECASE,
    ):
        if m:
            tokens.append(m)

    if source_name and source_name.lower() not in {"unknown", "none", "platform"}:
        tokens.append(source_name)
    if heat_score is not None:
        tokens.append(str(int(heat_score)))

    dedup: list[str] = []
    seen: set[str] = set()
    for tok in tokens:
        key = tok.strip().lower()
        if not key or key in seen:
            continue
        seen.add(key)
        dedup.append(tok.strip())
    return dedup[:6]


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Final signal summary with density tiering and verifiable evidence tokens."""
    from core.info_density import apply_density_gate, apply_density_tiering, evaluate_text_density

    base_rows = list(_v523_prev_build_signal_summary(cards) or [])
    valid_cards = [c for c in cards if bool(getattr(c, "is_valid_news", False))]
    tier_a_cards, tier_b_cards, _tier_c_cards, _tier_map = apply_density_tiering(valid_cards, "event")
    signal_pool = tier_a_cards + tier_b_cards

    # Keep legacy signal stats for metrics continuity.
    signal_passed, _signal_rejected, signal_stats, _ = apply_density_gate(valid_cards, "signal")
    passed_ids = {str(getattr(c, "item_id", "")) for c in signal_passed}
    platform_stats = _v523_platform_stats(valid_cards, passed_ids)

    def _pick_card(index: int) -> EduNewsCard | None:
        if signal_pool:
            return signal_pool[index % len(signal_pool)]
        if valid_cards:
            return valid_cards[index % len(valid_cards)]
        return None

    rows: list[dict] = []
    for idx, row in enumerate(base_rows):
        current = dict(row)
        card = _pick_card(idx)
        source_name = str(current.get("source_name") or "").strip() or (_v523_card_source(card) if card else "platform")
        if source_name.lower() == "unknown":
            source_name = "platform"

        signal_text = _v522_strip_placeholders(str(current.get("signal_text", current.get("title", ""))))
        title = _v522_strip_placeholders(str(current.get("title", signal_text)))
        if not signal_text and card is not None:
            signal_text = _v522_strip_placeholders(str(getattr(card, "title_plain", "") or ""))
        signal_text = signal_text or title or str(current.get("label", "Signal"))
        title = title or signal_text

        platform_count = max(int(current.get("platform_count", current.get("source_count", 1)) or 1), 1)
        heat_score = max(int(current.get("heat_score", 30) or 30), 30)
        heat_word = str(current.get("heat", "cool")).lower()

        snippet = _v522_strip_placeholders(str(current.get("example_snippet", "")))
        ok_snip, _reason_snip, snip_breakdown = evaluate_text_density(snippet, "signal")
        evidence_tokens = _v523_extract_evidence_tokens(
            f"{signal_text} {snippet}",
            source_name=source_name,
            heat_score=heat_score,
        )
        if (not ok_snip) or (snip_breakdown.entity_hits < 1 and snip_breakdown.numeric_hits < 1) or len(evidence_tokens) < 2:
            if card is not None:
                snippet = _smart_truncate(
                    _v522_strip_placeholders(
                        f"{getattr(card, 'title_plain', '')} | {getattr(card, 'what_happened', '')} | "
                        f"來源={_v523_card_source(card)} | score={float(getattr(card, 'final_score', 0.0) or 0.0):.1f}"
                    ),
                    120,
                )
            elif platform_stats:
                plat = platform_stats[idx % len(platform_stats)]
                snippet = (
                    f"{plat['source_name']}: heat_sum={plat['heat_sum']}, items={plat['items_seen']}, "
                    f"gate_pass={plat['gate_pass']}."
                )
            else:
                snippet = (
                    f"Coverage stats: total_in={signal_stats.total_in}, passed={signal_stats.passed}, "
                    f"tier_a={len(tier_a_cards)}, tier_b={len(tier_b_cards)}."
                )

        snippet = _smart_truncate(_v522_strip_placeholders(snippet), 120)
        if len(snippet) < 30:
            snippet = _smart_truncate(
                f"{source_name}: platform_count={platform_count}, heat_score={heat_score}, "
                f"coverage_items={len(signal_pool) or len(valid_cards)}.",
                120,
            )

        evidence_tokens = _v523_extract_evidence_tokens(
            f"{signal_text} {snippet}",
            source_name=source_name,
            heat_score=heat_score,
        )
        if len(evidence_tokens) < 2:
            evidence_tokens = list(dict.fromkeys(evidence_tokens + [source_name, str(platform_count), str(heat_score)]))[:4]

        current["signal_text"] = signal_text
        current["title"] = title
        current["platform_count"] = platform_count
        current["source_count"] = platform_count
        current["heat_score"] = heat_score
        current["heat"] = heat_word if heat_word in {"hot", "warm", "cool"} else ("warm" if heat_score >= 50 else "cool")
        current["source_name"] = source_name
        current["example_snippet"] = snippet
        current["evidence_tokens"] = evidence_tokens
        current["signals_insufficient"] = len(signal_pool) < 3
        current["passed_total_count"] = len(signal_pool)
        current["density_tier"] = str(getattr(card, "density_tier", "B")) if card is not None else "B"
        rows.append(current)

    while len(rows) < 3 and signal_pool:
        card = signal_pool[len(rows) % len(signal_pool)]
        source_name = _v523_card_source(card)
        score = float(getattr(card, "final_score", 0.0) or 0.0)
        heat_score = max(30, min(100, int(round(score * 12))))
        snippet = _smart_truncate(
            _v522_strip_placeholders(
                f"{getattr(card, 'what_happened', '')} | 來源={source_name} | score={score:.1f}"
            ),
            120,
        )
        evidence_tokens = _v523_extract_evidence_tokens(
            f"{getattr(card, 'title_plain', '')} {snippet}",
            source_name=source_name,
            heat_score=heat_score,
        )
        if len(evidence_tokens) < 2:
            evidence_tokens = list(dict.fromkeys(evidence_tokens + [source_name, str(heat_score), "1"]))[:4]
        rows.append(
            {
                "signal_name": "WORKFLOW_CHANGE",
                "signal_type": "WORKFLOW_CHANGE",
                "label": _SIGNAL_LABELS.get("WORKFLOW_CHANGE", "Workflow Change"),
                "title": _smart_truncate(_v522_strip_placeholders(str(getattr(card, "title_plain", "") or "")) or "Workflow change", 40),
                "source_count": 1,
                "heat": "warm" if heat_score >= 50 else "cool",
                "signal_text": _smart_truncate(_v522_strip_placeholders(str(getattr(card, "title_plain", "") or "")) or "Workflow change", 40),
                "platform_count": 1,
                "heat_score": heat_score,
                "example_snippet": snippet,
                "source_name": source_name,
                "evidence_tokens": evidence_tokens,
                "signals_insufficient": len(signal_pool) < 3,
                "passed_total_count": len(signal_pool),
                "density_tier": str(getattr(card, "density_tier", "B")),
            }
        )

    while len(rows) < 3 and platform_stats:
        plat = platform_stats[len(rows) % len(platform_stats)]
        items_seen = max(int(plat.get("items_seen", 0)), 1)
        heat_sum = int(plat.get("heat_sum", 0))
        heat_score = max(30, min(100, int(round(heat_sum / items_seen))))
        source_name = str(plat.get("source_name", "platform")) or "platform"
        if source_name.lower() == "unknown":
            source_name = "platform"
        snippet = _smart_truncate(
            f"{source_name}: heat_sum={heat_sum}, items={items_seen}, gate_pass={int(plat.get('gate_pass', 0))}.",
            120,
        )
        evidence_tokens = _v523_extract_evidence_tokens(snippet, source_name=source_name, heat_score=heat_score)
        if len(evidence_tokens) < 2:
            evidence_tokens = list(dict.fromkeys(evidence_tokens + [source_name, str(items_seen), str(heat_score)]))[:4]
        rows.append(
            {
                "signal_name": f"PLATFORM_HEAT_{source_name.upper()}",
                "signal_type": "PLATFORM_HEAT",
                "label": "Platform Heat",
                "title": f"{source_name} heat trend",
                "source_count": items_seen,
                "heat": "warm" if heat_score >= 50 else "cool",
                "signal_text": f"{source_name} heat trend",
                "platform_count": items_seen,
                "heat_score": heat_score,
                "example_snippet": snippet,
                "source_name": source_name,
                "evidence_tokens": evidence_tokens,
                "signals_insufficient": len(signal_pool) < 3,
                "passed_total_count": len(signal_pool),
                "density_tier": "B",
            }
        )

    fallback_names = ["TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE", "COST_PRESSURE", "COMPETITION_SIGNAL"]
    idx = 0
    while len(rows) < 3:
        name = fallback_names[idx % len(fallback_names)]
        idx += 1
        snippet = (
            f"Coverage stats: total_in={signal_stats.total_in}, tier_a={len(tier_a_cards)}, "
            f"tier_b={len(tier_b_cards)}, platform_count=1, heat_score=30."
        )
        evidence_tokens = _v523_extract_evidence_tokens(snippet, source_name="platform", heat_score=30)
        if len(evidence_tokens) < 2:
            evidence_tokens = ["platform", "30", str(signal_stats.total_in or 0)]
        rows.append(
            {
                "signal_name": name,
                "signal_type": name,
                "label": _SIGNAL_LABELS.get(name, name),
                "title": _SIGNAL_LABELS.get(name, name),
                "source_count": 1,
                "heat": "cool",
                "signal_text": _SIGNAL_LABELS.get(name, name),
                "platform_count": 1,
                "heat_score": 30,
                "example_snippet": _smart_truncate(snippet, 120),
                "source_name": "platform",
                "evidence_tokens": evidence_tokens[:4],
                "signals_insufficient": len(signal_pool) < 3,
                "passed_total_count": len(signal_pool),
                "density_tier": "B",
            }
        )

    final_rows: list[dict] = []
    for row in rows[:3]:
        current = dict(row)
        source_name = str(current.get("source_name", "")).strip() or "platform"
        if source_name.lower() == "unknown":
            source_name = "platform"

        signal_text = _v522_strip_placeholders(str(current.get("signal_text", current.get("title", "")))) or "Signal"
        title = _v522_strip_placeholders(str(current.get("title", signal_text))) or signal_text
        snippet = _v522_strip_placeholders(str(current.get("example_snippet", "")))
        snippet = snippet.replace("source=unknown", "來源未標示")
        if len(snippet) < 30:
            snippet = _smart_truncate(
                f"{source_name}: platform_count={int(current.get('platform_count', 1) or 1)}, "
                f"heat_score={int(current.get('heat_score', 30) or 30)}.",
                120,
            )

        heat_score = max(int(current.get("heat_score", 30) or 30), 30)
        platform_count = max(int(current.get("platform_count", current.get("source_count", 1)) or 1), 1)
        evidence_tokens = current.get("evidence_tokens")
        if not isinstance(evidence_tokens, list):
            evidence_tokens = []
        evidence_tokens = [str(t).strip() for t in evidence_tokens if str(t).strip()]
        if len(evidence_tokens) < 2:
            evidence_tokens = _v523_extract_evidence_tokens(
                f"{signal_text} {snippet}",
                source_name=source_name,
                heat_score=heat_score,
            )
        if len(evidence_tokens) < 2:
            evidence_tokens = list(dict.fromkeys(evidence_tokens + [source_name, str(platform_count), str(heat_score)]))[:4]

        current["source_name"] = source_name
        current["signal_text"] = signal_text
        current["title"] = title
        current["example_snippet"] = _smart_truncate(snippet, 120)
        current["platform_count"] = platform_count
        current["source_count"] = platform_count
        current["heat_score"] = heat_score
        current["evidence_tokens"] = evidence_tokens[:6]
        current["signals_insufficient"] = bool(current.get("signals_insufficient", len(signal_pool) < 3))
        current["passed_total_count"] = int(current.get("passed_total_count", len(signal_pool)))
        final_rows.append(current)

    return final_rows


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Final corp-watch summary with info-density gate and numeric-first fallback."""
    from core.info_density import apply_density_gate

    result = dict(_v523_prev_build_corp_watch_summary(cards, metrics=metrics) or {})
    valid_cards = [c for c in cards if bool(getattr(c, "is_valid_news", False))]
    corp_passed, _rejected, corp_stats, _ = apply_density_gate(valid_cards, "corp")
    passed_ids = {str(getattr(c, "item_id", "")) for c in corp_passed}
    top_sources = _v523_platform_stats(valid_cards, passed_ids)[:3]
    if not top_sources:
        top_sources = [{"source_name": "none", "items_seen": 0, "heat_sum": 0, "gate_pass": 0}]

    top_fail_reasons = result.get("top_fail_reasons") or []
    if not top_fail_reasons:
        top_fail_reasons = [{"reason": "none", "count": 0}]

    success_sources = [r for r in top_sources if int(r.get("gate_pass", 0)) > 0]
    last_success_source = str(success_sources[0].get("source_name", "none")) if success_sources else "none"

    sources_total = int(result.get("sources_total", len(top_sources)))
    success_count = int(result.get("success_count", len(corp_passed)))
    fail_count = int(result.get("fail_count", max(sources_total - success_count, 0)))

    mentions_count = int(result.get("mentions_count", result.get("total_mentions", 0) or 0))
    if mentions_count == 0:
        reason_bits = ", ".join(
            f"{str(item.get('reason', 'none'))}:{int(item.get('count', 0))}"
            for item in top_fail_reasons[:3]
        ) or "none"
        result["status_message"] = (
            f"掃描統計：sources_total={sources_total}、success_count={success_count}、"
            f"fail_count={fail_count}、last_success_source={last_success_source}、"
            f"top_fail_reasons={reason_bits}。"
        )

    result["sources_total"] = sources_total
    result["success_count"] = success_count
    result["fail_count"] = fail_count
    result["top_fail_reasons"] = top_fail_reasons
    result["top_sources"] = [
        {
            "source_name": str(r.get("source_name", "none")),
            "items_seen": int(r.get("items_seen", 0)),
            "gate_pass": int(r.get("gate_pass", 0)),
            "gate_soft_pass": int(r.get("gate_soft_pass", 0)) if "gate_soft_pass" in r else 0,
        }
        for r in top_sources
    ]
    result["last_success_source"] = last_success_source
    result["density_avg_score"] = corp_stats.avg_score
    return result


def build_structured_executive_summary(
    news_cards: list[EduNewsCard],
    tone: str = "neutral",
    metrics: dict | None = None,
) -> dict[str, list[str]]:
    """Final structured summary wrapper with density-aware no-event branch."""
    from core.info_density import apply_density_gate

    event_candidates = [c for c in news_cards if c.is_valid_news and not is_non_event_or_index(c)]
    event_passed, _event_rejected, event_stats, _ = apply_density_gate(event_candidates, "event")
    if event_passed:
        return _v523_prev_build_structured_summary(news_cards, tone, metrics=metrics)

    fetched_total = int((metrics or {}).get("fetched_total", len(news_cards)))
    gate_pass_total = int((metrics or {}).get("gate_pass_total", sum(1 for c in news_cards if c.is_valid_news)))
    sources_total = int((metrics or {}).get("sources_total", len({_v523_card_source(c) for c in news_cards})))
    rejected_bits = ", ".join(f"{k}:{v}" for k, v in event_stats.rejected_reason_top) or "none"

    return {
        "ai_trends": [
            (
                f"Scan coverage: fetched_total={fetched_total}, gate_pass_total={gate_pass_total}, "
                f"density_passed={event_stats.passed}, density_avg_score={event_stats.avg_score}."
            )
        ],
        "tech_landing": [
            (
                f"Source coverage: sources_total={sources_total}, density_rejected={event_stats.rejected_total}, "
                f"rejected_reason_top={rejected_bits}."
            )
        ],
        "market_competition": [
            "No event met the event-density gate today; platform-level signals remain available for monitoring."
        ],
        "opportunities_risks": [
            "Risk posture: prioritize sources with repeated low-density rejects and improve enrichment quality."
        ],
        "recommended_actions": [
            "Use signal/corp fallback numbers to guide WATCH/TEST decisions until higher-density events appear."
        ],
    }


def build_ceo_brief_blocks(card: EduNewsCard) -> dict:
    """Final slide-level density guard for WHY IT MATTERS content."""
    from core.info_density import candidate_text_from_card, evaluate_text_density

    brief = dict(_v523_prev_build_ceo_brief_blocks(card))
    q3 = list(brief.get("q3_actions", []))
    why_block = " ".join([str(brief.get("q1_meaning", "")), str(brief.get("q2_impact", ""))] + [str(v) for v in q3])
    ok, _reason, breakdown = evaluate_text_density(why_block, "event")

    if ok and breakdown.score >= 50 and not (breakdown.numeric_hits == 0 and breakdown.entity_hits < 2):
        return brief

    raw = candidate_text_from_card(card)
    terms = list(dict.fromkeys(re.findall(r"\b[A-Za-z0-9][A-Za-z0-9\-_]{2,}\b", raw)))
    numbers = list(dict.fromkeys(re.findall(r"\b\d+(?:\.\d+)?(?:%|ms|gb|mb|k|m|b|x)?\b", raw, flags=re.IGNORECASE)))

    key_terms = terms[:5] or ["coverage"]
    number_bits = numbers[:5] or [f"{float(getattr(card, 'final_score', 0.0) or 0.0):.1f}"]
    source_count = len(brief.get("sources", []))
    if source_count == 0 and _v523_card_source(card) != "unknown":
        source_count = 1

    # v5.2.3b — clean Chinese fallback (no WATCH/TEST/MOVE internal tags)
    title_snippet = str(getattr(card, "title_plain", "") or "").strip()[:30]
    what_snippet = str(getattr(card, "what_happened", "") or "").strip()
    source_name = _v523_card_source(card)

    brief["q1_meaning"] = (
        what_snippet if len(what_snippet) >= 20
        else (
            f"「{title_snippet}」：{source_name} 報導，"
            f"資訊密度較低，建議查閱原始來源確認細節。"
        )
    )
    brief["q2_impact"] = (
        f"來源：{source_name}；"
        f"核心實體：{'、'.join(key_terms[:3]) if key_terms else '待確認'}；"
        f"可核對數字：{'、'.join(number_bits[:3]) if number_bits else '尚無'}。"
    )
    brief["q3_actions"] = [
        f"核實「{title_snippet[:20]}」相關數據，確認來源可信度（T+3）。",
        "評估此事件對現有工作流程的實際影響範圍。",
        "密切追蹤後續進展，視下週指標決定行動時間點（T+7）。",
    ]
    return brief


# ---------------------------------------------------------------------------
# v5.2.4 terminal wrappers (ZH fallback + fixed non-empty event pool)
# ---------------------------------------------------------------------------

_v524_prev_build_signal_summary = build_signal_summary
_v524_prev_build_corp_watch_summary = build_corp_watch_summary
_v524_prev_build_structured_summary = build_structured_executive_summary

_V524_SIGNAL_LABEL_ZH = {
    "TOOL_ADOPTION": "工具採用",
    "USER_PAIN": "使用者痛點",
    "WORKFLOW_CHANGE": "流程變動",
    "COST_PRESSURE": "成本壓力",
    "COMPETITION_SIGNAL": "競爭訊號",
}


def _v524_ascii_alpha_ratio(text: str) -> float:
    payload = str(text or "")
    alpha = [c for c in payload if c.isalpha()]
    if not alpha:
        return 0.0
    ascii_alpha = sum(1 for c in alpha if ord(c) < 128)
    return ascii_alpha / len(alpha)


def _v524_has_cjk(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", str(text or "")))


def _v524_safe_source_name(source_name: str) -> str:
    name = str(source_name or "").strip()
    if not name or name.lower() in {"unknown", "none"}:
        return "來源平台"
    return name


def _v524_to_zh_signal_text(label: str, source_name: str, heat_score: int, platform_count: int) -> str:
    return (
        f"{label}：來源 {source_name}，熱度 {heat_score}，平台數 {platform_count}。"
    )


def _v524_to_zh_snippet(
    snippet: str,
    *,
    source_name: str,
    heat_score: int,
    platform_count: int,
    evidence_tokens: list[str],
) -> str:
    cleaned = _v522_strip_placeholders(str(snippet or ""))
    token_text = "、".join(evidence_tokens[:3]) if evidence_tokens else "來源統計"
    if _v524_ascii_alpha_ratio(cleaned) > 0.7 or not _v524_has_cjk(cleaned):
        cleaned = (
            f"中文轉述：來源 {source_name} 顯示熱度 {heat_score}、平台數 {platform_count}；"
            f"可核對資訊：{token_text}。"
        )
    if len(cleaned) < 30:
        cleaned = (
            f"來源 {source_name} 的觀測訊號已建立，熱度 {heat_score}、平台數 {platform_count}，"
            f"可核對資訊：{token_text}。"
        )
    return _smart_truncate(cleaned, 120)


def _v525_has_anchor_url(card: EduNewsCard) -> bool:
    url = str(getattr(card, "source_url", "") or "").strip()
    if url.startswith("http://") or url.startswith("https://"):
        return True
    source_name = str(getattr(card, "source_name", "") or "").strip().lower()
    return source_name not in {"", "unknown", "none", "platform", "source=unknown", "source=platform"}


def _v525_count_entities_numbers(text: str) -> tuple[int, int]:
    payload = str(text or "")
    entities = set(re.findall(r"\b[A-Z][A-Za-z0-9\-_]{2,}\b", payload))
    entities.update(re.findall(r"[\u4e00-\u9fff]{2,}", payload))
    numbers = re.findall(r"\b\d+(?:\.\d+)?(?:%|x|ms|gb|mb|k|m|b)?\b", payload, flags=re.IGNORECASE)
    return len(entities), len(numbers)


def _v525_is_placeholder_text(text: str) -> bool:
    lowered = str(text or "").lower()
    if not lowered.strip():
        return True
    banned = (
        "fallback monitoring signal",
        "desktop smoke signal",
        "signals_insufficient=true",
        "last july was",
        "weekly roundup",
        "top links",
        "subscribe",
        "sign in",
        "低信心事件候選",
        "source=platform",
        "本次無有效新聞；本次掃描統計",
    )
    return any(token in lowered for token in banned)


def _v525_extract_signal_snippet(card: EduNewsCard) -> str:
    raw = " ".join(
        [
            str(getattr(card, "what_happened", "") or ""),
            str(getattr(card, "why_important", "") or ""),
            str(getattr(card, "title_plain", "") or ""),
        ]
    )
    cleaned = _v522_strip_placeholders(raw).replace("source=platform", "來源平台")
    cleaned = cleaned.replace("source=unknown", "來源未標示")
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    if _v525_is_placeholder_text(cleaned) or len(cleaned) < 30:
        src_name = _v524_safe_source_name(str(getattr(card, "source_name", "") or ""))
        src_url = str(getattr(card, "source_url", "") or "")
        score = float(getattr(card, "final_score", 0.0) or 0.0)
        cleaned = (
            f"來源 {src_name}，網址 {src_url if src_url.startswith('http') else 'N/A'}，"
            f"分數 {score:.1f}。請以原文核對。"
        )
    return _smart_truncate(cleaned, 120)


def get_event_cards_for_deck(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
    *,
    min_events: int = 0,
) -> list[EduNewsCard]:
    """Return verifiable event cards, with conservative fallback when strict pool is empty.

    Applies relevance gate (building/non-AI hard reject) and quota-based
    topic selection (product/tech/business/dev) before returning the final pool.
    Writes audit meta to outputs/exec_selection.meta.json as a non-breaking side effect.
    """
    from utils.topic_router import is_relevant_ai as _is_relevant_ai

    event_cards: list[EduNewsCard] = []
    fallback_candidates: list[tuple[float, EduNewsCard, int, int]] = []
    for card in cards:
        if not bool(getattr(card, "is_valid_news", False)):
            continue
        if not _v525_has_anchor_url(card):
            continue

        payload = " ".join(
            [
                str(getattr(card, "title_plain", "") or ""),
                str(getattr(card, "what_happened", "") or ""),
                str(getattr(card, "why_important", "") or ""),
            ]
        )
        entities_count, numbers_count = _v525_count_entities_numbers(payload)
        if _v525_is_placeholder_text(payload):
            continue

        # Relevance gate (B): hard-reject building/non-AI noise
        src_url = str(getattr(card, "source_url", "") or "")
        _relevant, _rej_reasons = _is_relevant_ai(payload, src_url)
        if not _relevant:
            continue

        score = float(getattr(card, "final_score", 0.0) or 0.0)
        strict_ok = (
            not is_non_event_or_index(card)
            and bool(getattr(card, "event_gate_pass", True))
            and entities_count >= 2
            and (numbers_count >= 1 or score >= 8.5)
        )
        if strict_ok:
            try:
                setattr(card, "low_confidence", False)
            except Exception:
                pass
        else:
            # Conservative fallback from signal-tier cards: only when evidence exists.
            soft_ok = (
                bool(getattr(card, "signal_gate_pass", True))
                and entities_count >= 2
                and (numbers_count >= 1 or score >= 8.0)
            )
            if soft_ok:
                rank = float(score) + float(numbers_count) * 0.4 + float(entities_count) * 0.15
                fallback_candidates.append((rank, card, entities_count, numbers_count))
            continue

        try:
            setattr(card, "entities_count", entities_count)
            setattr(card, "numbers_count", numbers_count)
            setattr(card, "has_anchor_url", True)
            setattr(card, "event_gate_pass", True)
        except Exception:
            pass
        event_cards.append(card)

    min_required = max(0, int(min_events))
    if not event_cards and fallback_candidates:
        min_required = max(1, min_required)
    if len(event_cards) < min_required and fallback_candidates:
        existing_ids = {str(getattr(c, "item_id", "") or "") for c in event_cards}
        fallback_candidates.sort(key=lambda row: row[0], reverse=True)
        for _rank, card, entities_count, numbers_count in fallback_candidates:
            item_id = str(getattr(card, "item_id", "") or "")
            if item_id and item_id in existing_ids:
                continue
            try:
                setattr(card, "entities_count", entities_count)
                setattr(card, "numbers_count", numbers_count)
                setattr(card, "has_anchor_url", True)
                setattr(card, "event_gate_pass", True)
                setattr(card, "low_confidence", True)
            except Exception:
                pass
            event_cards.append(card)
            if item_id:
                existing_ids.add(item_id)
            if len(event_cards) >= min_required:
                break

    # Quota-based topic selection (C) + audit meta write (E)
    # Pass fallback_candidates as extra_pool so channel backfill can draw from them
    _extra_pool = [card for _rank, card, _ec, _nc in fallback_candidates]
    try:
        selected, sel_meta = select_executive_items(event_cards, extra_pool=_extra_pool or None)
        write_exec_selection_meta(sel_meta)
        write_exec_kpi_meta(sel_meta)
        write_exec_quality_meta(selected, sel_meta)
        return selected
    except RuntimeError:
        raise  # propagate G2/G3 fail-fast gates
    except Exception:
        # If quota selection fails for any reason, return unfiltered pool
        return event_cards


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Top 3 signal rows built from verifiable signal pool cards."""
    signal_cards = [
        c for c in cards
        if bool(getattr(c, "is_valid_news", False))
        and bool(getattr(c, "signal_gate_pass", True))
    ]
    if not signal_cards:
        signal_cards = [c for c in cards if bool(getattr(c, "is_valid_news", False))]

    if not signal_cards:
        base = list(_v524_prev_build_signal_summary(cards) or [])
        sanitized: list[dict] = []
        for row in base[:3]:
            source_name = _v524_safe_source_name(str(row.get("source_name", "") or ""))
            platform_count = max(int(row.get("platform_count", row.get("source_count", 1)) or 1), 1)
            heat_score = min(100, max(int(row.get("heat_score", 40) or 40), 30))
            signal_name = str(row.get("signal_name", row.get("signal_type", "TOOL_ADOPTION")) or "TOOL_ADOPTION").upper()
            label = _V524_SIGNAL_LABEL_ZH.get(signal_name, str(row.get("label", signal_name)))
            signal_text = _v524_to_zh_signal_text(label, source_name, heat_score, platform_count)
            sanitized.append(
                {
                    "signal_name": signal_name,
                    "signal_type": signal_name,
                    "label": label,
                    "title": str(row.get("title", signal_text) or signal_text)[:80],
                    "source_url": str(row.get("source_url", "") or ""),
                    "heat": "cool" if heat_score < 50 else ("warm" if heat_score < 75 else "hot"),
                    "signal_text": signal_text,
                    "platform_count": platform_count,
                    "source_count": platform_count,
                    "heat_score": heat_score,
                    "example_snippet": _smart_truncate(str(row.get("example_snippet", signal_text) or signal_text), 120),
                    "source_name": source_name,
                    "evidence_tokens": [source_name, str(platform_count), str(heat_score)],
                    "signals_insufficient": True,
                    "passed_total_count": 0,
                }
            )
        return sanitized[:3]

    source_counts: dict[str, int] = {}
    for card in signal_cards:
        source_name = _v524_safe_source_name(str(getattr(card, "source_name", "") or ""))
        source_counts[source_name] = source_counts.get(source_name, 0) + 1

    sorted_cards = sorted(
        signal_cards,
        key=lambda c: float(getattr(c, "final_score", 0.0) or 0.0),
        reverse=True,
    )
    with_url = [c for c in sorted_cards if _v525_has_anchor_url(c)]
    if with_url:
        sorted_cards = with_url + [c for c in sorted_cards if c not in with_url]

    names = ["TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE", "COST_PRESSURE", "COMPETITION_SIGNAL"]
    rows: list[dict] = []
    idx = 0
    while len(rows) < 3 and sorted_cards:
        card = sorted_cards[idx % len(sorted_cards)]
        idx += 1

        signal_name = names[(idx - 1) % len(names)]
        label = _V524_SIGNAL_LABEL_ZH.get(signal_name, signal_name)
        source_name = _v524_safe_source_name(str(getattr(card, "source_name", "") or ""))
        source_url = str(getattr(card, "source_url", "") or "").strip()
        title = _v522_strip_placeholders(str(getattr(card, "title_plain", "") or "")).strip() or "來源訊號"
        platform_count = max(source_counts.get(source_name, 1), 1)
        score = float(getattr(card, "final_score", 0.0) or 0.0)
        heat_score = max(30, min(100, int(round(score * 12))))
        heat = "hot" if heat_score >= 75 else ("warm" if heat_score >= 50 else "cool")
        signal_text = _v524_to_zh_signal_text(label, source_name, heat_score, platform_count)
        snippet = _v525_extract_signal_snippet(card)
        if _v525_is_placeholder_text(snippet):
            continue
        entities_count, numbers_count = _v525_count_entities_numbers(f"{title} {snippet}")
        if entities_count < 1 and numbers_count < 1:
            continue
        evidence_tokens = _v523_extract_evidence_tokens(
            f"{title} {snippet}",
            source_name=source_name,
            heat_score=heat_score,
        )
        if len(evidence_tokens) < 2:
            evidence_tokens = [source_name, str(platform_count), str(heat_score)]
        rows.append(
            {
                "signal_name": signal_name,
                "signal_type": signal_name,
                "label": label,
                "title": _smart_truncate(title, 80),
                "source_url": source_url if source_url.startswith("http") else "",
                "heat": heat,
                "signal_text": signal_text,
                "platform_count": platform_count,
                "source_count": platform_count,
                "heat_score": heat_score,
                "example_snippet": _smart_truncate(snippet, 120),
                "source_name": source_name,
                "evidence_tokens": evidence_tokens[:6],
                "signals_insufficient": len(sorted_cards) < 3,
                "passed_total_count": len(sorted_cards),
            }
        )

    while len(rows) < 3 and sorted_cards:
        card = sorted_cards[len(rows) % len(sorted_cards)]
        source_name = _v524_safe_source_name(str(getattr(card, "source_name", "") or ""))
        source_url = str(getattr(card, "source_url", "") or "").strip()
        score = float(getattr(card, "final_score", 0.0) or 0.0)
        heat_score = max(30, min(100, int(round(score * 12))))
        platform_count = max(source_counts.get(source_name, 1), 1)
        label = _V524_SIGNAL_LABEL_ZH.get("WORKFLOW_CHANGE", "流程變動")
        title = _v522_strip_placeholders(str(getattr(card, "title_plain", "") or "")).strip() or "來源訊號"
        snippet = _v525_extract_signal_snippet(card)
        rows.append(
            {
                "signal_name": "WORKFLOW_CHANGE",
                "signal_type": "WORKFLOW_CHANGE",
                "label": label,
                "title": _smart_truncate(title, 80),
                "source_url": source_url if source_url.startswith("http") else "",
                "heat": "warm" if heat_score >= 50 else "cool",
                "signal_text": _v524_to_zh_signal_text(label, source_name, heat_score, platform_count),
                "platform_count": platform_count,
                "source_count": platform_count,
                "heat_score": heat_score,
                "example_snippet": _smart_truncate(snippet, 120),
                "source_name": source_name,
                "evidence_tokens": [source_name, str(platform_count), str(heat_score)],
                "signals_insufficient": len(sorted_cards) < 3,
                "passed_total_count": len(sorted_cards),
            }
        )

    return rows[:3]


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    result = dict(_v524_prev_build_corp_watch_summary(cards, metrics=metrics) or {})
    if int(result.get("updates", result.get("total_mentions", 0) or 0)) == 0:
        sources_total = int(result.get("sources_total", (metrics or {}).get("sources_total", 0)))
        success_count = int(result.get("success_count", max(sources_total, 0)))
        fail_count = int(result.get("fail_count", max(sources_total - success_count, 0)))
        top_fail = result.get("top_fail_reasons") or [{"reason": "none", "count": 0}]
        top_fail_text = "、".join(
            f"{str(x.get('reason', 'none'))}:{int(x.get('count', 0))}" for x in top_fail[:3]
        )
        result["status_message"] = (
            f"掃描統計：sources_total={sources_total}、success_count={success_count}、"
            f"fail_count={fail_count}、top_fail_reasons={top_fail_text}。"
        )
    return result


def build_structured_executive_summary(
    news_cards: list[EduNewsCard],
    tone: str = "neutral",
    metrics: dict | None = None,
) -> dict[str, list[str]]:
    summary = _v524_prev_build_structured_summary(news_cards, tone, metrics=metrics)

    fetched_total = int((metrics or {}).get("fetched_total", len(news_cards)))
    gate_pass_total = int((metrics or {}).get("gate_pass_total", sum(1 for c in news_cards if c.is_valid_news)))
    hard_pass_total = int((metrics or {}).get("hard_pass_total", 0))
    soft_pass_total = int((metrics or {}).get("soft_pass_total", 0))
    rejected_total = int((metrics or {}).get("rejected_total", 0))
    sources_total = int((metrics or {}).get("sources_total", len({_v524_safe_source_name(_v523_card_source(c)) for c in news_cards})))
    density_top5 = list((metrics or {}).get("density_score_top5", []))
    density_preview = "、".join(
        f"{str(row[0])[:12]}:{int(row[2])}" for row in density_top5[:3] if isinstance(row, (list, tuple)) and len(row) >= 3
    ) or "none"

    defaults = {
        "ai_trends": [
            (
                f"本日掃描共取得 fetched_total={fetched_total} 筆資料，gate_pass_total={gate_pass_total}，"
                f"hard_pass_total={hard_pass_total}，soft_pass_total={soft_pass_total}。"
            )
        ],
        "tech_landing": [
            f"來源覆蓋 sources_total={sources_total}，rejected_total={rejected_total}，密度候選前列為 {density_preview}。"
        ],
        "market_competition": [
            "今日未形成可核對事件，已改以訊號池與來源統計提供決策參考。"
        ],
        "opportunities_risks": [
            "建議優先追蹤連續出現的來源與可核對數字，降低短摘要造成的判讀誤差。"
        ],
        "recommended_actions": [
            "先以 WATCH/TEST 驗證訊號池項目，待下一輪資料補齊後再決定是否 MOVE。"
        ],
    }
    translated: dict[str, list[str]] = {}
    for key, fallback_lines in defaults.items():
        source_lines = list(summary.get(key, [])) or list(fallback_lines)
        output_lines: list[str] = []
        for idx, line in enumerate(source_lines[:3]):
            text = _v522_strip_placeholders(str(line or "")).strip()
            if not text:
                text = fallback_lines[min(idx, len(fallback_lines) - 1)]
            elif _v524_ascii_alpha_ratio(text) > 0.7 and not _v524_has_cjk(text):
                text = fallback_lines[min(idx, len(fallback_lines) - 1)]
            output_lines.append(text)
        translated[key] = output_lines or list(fallback_lines)
    return translated


_v525_prev_build_ceo_actions = build_ceo_actions


def build_ceo_actions(cards: list[EduNewsCard]) -> list[dict]:
    """Fallback actions should be signal/corp based when event cards are unavailable."""
    actions = list(_v525_prev_build_ceo_actions(cards) or [])
    if actions:
        return actions
    if not cards:
        return actions

    signals = build_signal_summary(cards)
    if not signals:
        return actions

    fallback_actions: list[dict] = []
    action_types = ["WATCH", "TEST", "WATCH"]
    for idx, sig in enumerate(signals[:3]):
        action_type = action_types[idx % len(action_types)]
        title = _smart_truncate(str(sig.get("title", "") or str(sig.get("signal_text", "")) or "來源訊號"), 36)
        heat_score = int(sig.get("heat_score", 30) or 30)
        platform_count = int(sig.get("platform_count", 1) or 1)
        source_name = _v524_safe_source_name(str(sig.get("source_name", "") or ""))
        source_url = str(sig.get("source_url", "") or "").strip()
        detail = (
            f"追蹤 {title}；heat_score={heat_score}、platform_count={platform_count}，"
            f"來源={source_name}，網址={source_url if source_url.startswith('http') else 'N/A'}。"
        )
        fallback_actions.append(
            {
                "action_type": action_type,
                "title": title,
                "detail": detail,
                "owner": "策略長/PM",
            }
        )
    return fallback_actions


# ---------------------------------------------------------------------------
# v5.3 — evidence enrichment for signal rows + corp watch update_type_counts
# ---------------------------------------------------------------------------

_v53_prev_build_signal_summary = build_signal_summary


def _extract_evidence_terms(text: str) -> list[str]:
    """Extract verifiable proper nouns (companies/models/products/tech names)."""
    known = {
        "openai", "google", "microsoft", "amazon", "aws", "meta", "apple", "nvidia",
        "alibaba", "tencent", "bytedance", "baidu", "huawei",
        "chatgpt", "gpt", "claude", "gemini", "llama", "deepseek", "qwen", "mistral",
        "copilot", "langchain", "cursor", "github", "azure", "vertex", "bedrock",
        "anthropic", "tensorflow", "pytorch", "huggingface",
    }
    lowered = text.lower()
    found = [kw for kw in known if kw in lowered]
    # Also extract capitalized proper nouns not in stop words
    caps = set(re.findall(r"\b[A-Z][A-Za-z0-9\-]{2,}\b", text))
    for c in caps:
        cl = c.lower()
        if cl not in _STOP_WORDS and cl not in found:
            found.append(c)
    return found[:8]


def _extract_evidence_numbers(text: str) -> list[str]:
    """Extract verifiable numbers (versions/prices/dates/benchmarks/params)."""
    patterns = [
        r"\b20\d{2}\b",
        r"\bv?\d+\.\d+(?:\.\d+)?\b",
        r"\b\d+(?:\.\d+)?\s*(?:%|x|ms|gb|mb|tb|k|m|b|tokens?)\b",
        r"[$€¥]\s*\d+(?:\.\d+)?(?:\s*(?:M|B|K|million|billion))?\b",
        r"\b\d+(?:\.\d+)?\s*(?:usd|dollars?|元|萬|亿|億)\b",
    ]
    combined = "|".join(patterns)
    matches = re.findall(combined, text, re.IGNORECASE)
    return list(dict.fromkeys(matches))[:6]


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Enriched signal summary with evidence_terms and evidence_numbers."""
    rows = list(_v53_prev_build_signal_summary(cards) or [])
    for row in rows:
        combined_text = f"{row.get('title', '')} {row.get('example_snippet', '')} {row.get('signal_text', '')}"
        terms = _extract_evidence_terms(combined_text)
        nums = _extract_evidence_numbers(combined_text)
        # Ensure minimum evidence_terms >= 2
        source_name = str(row.get("source_name", ""))
        if source_name and source_name not in terms:
            terms.insert(0, source_name)
        if len(terms) < 2:
            terms.append(str(row.get("signal_name", "SIGNAL")))
        row["evidence_terms"] = terms[:8]
        row["evidence_numbers"] = nums[:6]
        if not nums:
            row["missing_reason"] = "no_verifiable_numbers_in_source"
    return rows


_CORP_UPDATE_TYPES = ("model", "product", "cloud_infra", "ecosystem", "risk_policy")

_v53_prev_build_corp_watch = build_corp_watch_summary


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Corp watch with update_type_counts and enriched top_fail_reasons."""
    result = dict(_v53_prev_build_corp_watch(cards, metrics=metrics) or {})

    # Always include update_type_counts (even if all zeros)
    if "update_type_counts" not in result:
        result["update_type_counts"] = {k: 0 for k in _CORP_UPDATE_TYPES}

    # Ensure top_fail_reasons has at least 1 entry
    if not result.get("top_fail_reasons"):
        result["top_fail_reasons"] = [{"reason": "no_event_candidates", "count": 1}]

    return result


# ---------------------------------------------------------------------------
# v5.3.2 terminal wrappers (non-air density guard + fragment-safe fallback)
# ---------------------------------------------------------------------------

_v532_prev_build_signal_summary = build_signal_summary
_v532_prev_build_corp_watch_summary = build_corp_watch_summary
_v532_prev_build_structured_summary = build_structured_executive_summary

_V532_FRAGMENT_END_RE = re.compile(r"\b(?:was|is|are|the|and|to|of)\s*$", re.IGNORECASE)
_V532_PLACEHOLDER_TOKENS = (
    "last july was",
    "desktop smoke signal",
    "fallback monitoring signal",
    "signals_insufficient=true",
    "source=platform",
)
_V532_SIGNAL_NAMES = ["TOOL_ADOPTION", "USER_PAIN", "WORKFLOW_CHANGE", "COST_PRESSURE", "COMPETITION_SIGNAL"]
_V532_SIGNAL_LABELS = {
    "TOOL_ADOPTION": "工具採用",
    "USER_PAIN": "使用者痛點",
    "WORKFLOW_CHANGE": "工作流變化",
    "COST_PRESSURE": "成本壓力",
    "COMPETITION_SIGNAL": "競爭訊號",
}


def _v532_domain_from_url(url: str) -> str:
    try:
        return urlparse(url).netloc.lower().strip()
    except Exception:
        return ""


def _v532_is_placeholder_source(source_name: str) -> bool:
    raw = str(source_name or "").strip()
    lowered = raw.lower()
    if lowered in {
        "",
        "platform",
        "source=platform",
        "unknown",
        "source=unknown",
        "scan",
    }:
        return True
    if ("來源" in raw and "平台" in raw) or ("來源" in raw and "未標示" in raw):
        return True
    return False


def _v532_is_fragment(text: str) -> bool:
    payload = str(text or "").strip()
    if not payload:
        return True
    lowered = payload.lower()
    if any(token in lowered for token in _V532_PLACEHOLDER_TOKENS):
        return True
    # Evaluate only last sentence fragment to avoid over-filtering numeric lines.
    tail = re.split(r"[.!?。？！]\s*", payload)[-1].strip() or payload
    if len(tail) < 8:
        return True
    return _V532_FRAGMENT_END_RE.search(tail) is not None


def _v532_clean_line(text: str, fallback: str) -> str:
    cleaned = _v522_strip_placeholders(str(text or "")).strip()
    cleaned = re.sub(r"\s+", " ", cleaned)
    if _v532_is_fragment(cleaned):
        cleaned = fallback
    return _smart_truncate(cleaned, 120)


def _v532_safe_source(row: dict, card: EduNewsCard | None) -> tuple[str, str]:
    source_name = _v524_safe_source_name(str(row.get("source_name", "") or ""))
    source_url = str(row.get("source_url", "") or "").strip()
    if card is not None:
        card_url = str(getattr(card, "source_url", "") or "").strip()
        if not source_url and card_url.startswith(("http://", "https://")):
            source_url = card_url
        card_source = _v524_safe_source_name(str(getattr(card, "source_name", "") or ""))
        if _v532_is_placeholder_source(source_name) and not _v532_is_placeholder_source(card_source):
            source_name = card_source
    if _v532_is_placeholder_source(source_name):
        domain = _v532_domain_from_url(source_url)
        source_name = domain or "scan"
    return source_name, source_url if source_url.startswith(("http://", "https://")) else ""




def _v532_signal_card_pool(cards: list[EduNewsCard]) -> list[EduNewsCard]:
    pool = [c for c in cards if bool(getattr(c, "is_valid_news", False))]
    pool.sort(key=lambda c: float(getattr(c, "final_score", 0.0) or 0.0), reverse=True)
    return pool


def _v532_signal_snippet(
    *,
    row: dict,
    card: EduNewsCard | None,
    source_name: str,
    heat_score: int,
    platform_count: int,
    evidence_terms: list[str],
    evidence_numbers: list[str],
) -> str:
    raw = " ".join(
        [
            str(row.get("example_snippet", "") or ""),
            str(row.get("signal_text", "") or ""),
            str(row.get("title", "") or ""),
            str(getattr(card, "what_happened", "") if card else ""),
            str(getattr(card, "why_important", "") if card else ""),
            str(getattr(card, "title_plain", "") if card else ""),
        ]
    ).strip()
    # Only use raw snippet when it's sufficiently rich and not fragment-like.
    if len(raw) >= 200 and not _v532_is_fragment(raw):
        return _smart_truncate(_v522_strip_placeholders(raw), 120)

    term_a = evidence_terms[0] if evidence_terms else source_name
    term_b = evidence_terms[1] if len(evidence_terms) > 1 else "AI signal"
    num_a = evidence_numbers[0] if evidence_numbers else str(heat_score)
    num_b = evidence_numbers[1] if len(evidence_numbers) > 1 else str(platform_count)
    fallback = (
        f"來源 {source_name} 觀測到 {term_a} 與 {term_b}；"
        f"heat_score={heat_score}、platform_count={platform_count}、evidence={num_a}/{num_b}。"
    )
    return _smart_truncate(fallback, 120)


def build_signal_summary(cards: list[EduNewsCard]) -> list[dict]:
    """Final non-empty signal summary with verifiable terms/numbers and fragment guard."""
    base_rows = list(_v532_prev_build_signal_summary(cards) or [])
    pool = _v532_signal_card_pool(cards)
    source_counts: dict[str, int] = {}
    for card in pool:
        name = _v524_safe_source_name(str(getattr(card, "source_name", "") or "scan"))
        source_counts[name] = source_counts.get(name, 0) + 1

    rows: list[dict] = []
    idx = 0
    while len(rows) < 3:
        # Avoid cloning the same prebuilt row when signal pool is very small.
        row = dict(base_rows[idx]) if (idx < len(base_rows) and idx < len(pool)) else {}
        card = pool[idx % len(pool)] if pool else None
        use_card_title = idx < len(pool)
        signal_name = str(row.get("signal_name", _V532_SIGNAL_NAMES[idx % len(_V532_SIGNAL_NAMES)])).upper()
        label = str(row.get("label", _V532_SIGNAL_LABELS.get(signal_name, signal_name)))

        source_name, source_url = _v532_safe_source(row, card)
        platform_count = max(int(row.get("platform_count", row.get("source_count", source_counts.get(source_name, 1) or 1)) or 1), 1)
        heat_score = min(100, max(int(row.get("heat_score", max(30, int(round(float(getattr(card, "final_score", 3.0) or 3.0) * 12)))) or 30), 30))
        heat = str(row.get("heat", "hot" if heat_score >= 75 else ("warm" if heat_score >= 50 else "cool")))

        title = str(row.get("title", "") or "").strip()
        if not title and use_card_title and card is not None:
            title = str(getattr(card, "title_plain", "") or "").strip()
        if not title:
            title = _V532_SIGNAL_LABELS.get(signal_name, signal_name)
        title = _smart_truncate(_v522_strip_placeholders(title), 80)
        # Guard: reject placeholder/fragment titles before they reach PPT/DOCX
        try:
            from utils.semantic_quality import is_placeholder_or_fragment as _v532_ipf  # noqa: PLC0415
            if not title or _v532_ipf(title):
                title = signal_name  # ASCII-safe enum name, e.g. "TOOL_ADOPTION"
        except ImportError:
            if not title:
                title = signal_name

        combined = " ".join(
            [
                title,
                str(row.get("example_snippet", "") or ""),
                str(row.get("signal_text", "") or ""),
                str(getattr(card, "what_happened", "") if card else ""),
                str(getattr(card, "why_important", "") if card else ""),
                source_name,
                source_url,
                str(heat_score),
                str(platform_count),
            ]
        )
        evidence_terms = list(_extract_evidence_terms(combined))
        if source_name and source_name not in evidence_terms:
            evidence_terms.insert(0, source_name)
        while len(evidence_terms) < 2:
            evidence_terms.append(_V532_SIGNAL_LABELS.get(signal_name, signal_name))

        evidence_numbers = list(_extract_evidence_numbers(combined))
        if not evidence_numbers:
            evidence_numbers = [str(heat_score), str(platform_count)]

        snippet = _v532_signal_snippet(
            row=row,
            card=card if use_card_title else None,
            source_name=source_name,
            heat_score=heat_score,
            platform_count=platform_count,
            evidence_terms=evidence_terms,
            evidence_numbers=evidence_numbers,
        )
        snippet = _v532_clean_line(
            snippet,
            f"來源 {source_name} 訊號觀測：heat_score={heat_score}、platform_count={platform_count}。",
        )
        signal_text = _v532_clean_line(
            str(row.get("signal_text", "") or ""),
            f"{label}：來源 {source_name}，heat_score={heat_score}，platform_count={platform_count}。",
        )

        # Guard: signal_text must NOT be a signal_type enum or its Chinese label.
        _signal_type_tokens = set(_V532_SIGNAL_NAMES) | set(_V532_SIGNAL_LABELS.values())
        if signal_text.strip() in _signal_type_tokens or signal_name == signal_text.strip().upper().replace(" ", "_"):
            signal_text = title if title else f"{label}：{source_name} heat_score={heat_score}。"

        rows.append(
            {
                "signal_name": signal_name,
                "signal_type": signal_name,
                "label": label,
                "title": title,
                "source_url": source_url,
                "heat": heat,
                "signal_text": signal_text,
                "platform_count": platform_count,
                "source_count": platform_count,
                "heat_score": heat_score,
                "example_snippet": snippet,
                "source_name": source_name,
                "evidence_terms": evidence_terms[:8],
                "evidence_numbers": evidence_numbers[:6],
                "evidence_tokens": (evidence_terms[:2] + evidence_numbers[:2])[:6],
                "signals_insufficient": len(pool) < 3,
                "passed_total_count": len(pool),
            }
        )
        idx += 1

    return rows[:3]


def _v532_top_sources(cards: list[EduNewsCard]) -> list[dict]:
    agg: dict[str, dict[str, int | str]] = {}
    for card in cards:
        source_name = _v524_safe_source_name(str(getattr(card, "source_name", "") or "scan"))
        row = agg.setdefault(
            source_name,
            {"source_name": source_name, "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0},
        )
        row["items_seen"] = int(row["items_seen"]) + 1
        if bool(getattr(card, "event_gate_pass", False)):
            row["gate_pass"] = int(row["gate_pass"]) + 1
        elif bool(getattr(card, "signal_gate_pass", True)):
            row["gate_soft_pass"] = int(row["gate_soft_pass"]) + 1
    rows = sorted(agg.values(), key=lambda r: int(r["items_seen"]), reverse=True)
    return [dict(r) for r in rows[:3]]


def build_corp_watch_summary(
    cards: list[EduNewsCard],
    metrics: dict | None = None,
) -> dict:
    """Corp watch fallback with numeric scan stats and top source observability."""
    result = dict(_v532_prev_build_corp_watch_summary(cards, metrics=metrics) or {})
    result.setdefault("update_type_counts", {k: 0 for k in _CORP_UPDATE_TYPES})

    top_sources = result.get("top_sources") or _v532_top_sources(cards)
    if not top_sources:
        top_sources = [{"source_name": "none", "items_seen": 0, "gate_pass": 0, "gate_soft_pass": 0}]

    m = metrics or {}
    sources_total = int(result.get("sources_total", m.get("sources_total", len(top_sources))))
    success_count = int(result.get("success_count", m.get("sources_success", max(sources_total, 0))))
    fail_count = int(result.get("fail_count", m.get("sources_failed", max(sources_total - success_count, 0))))
    top_fail_reasons = result.get("top_fail_reasons") or [{"reason": "none", "count": 0}]
    fail_text = "、".join(f"{str(x.get('reason', 'none'))}:{int(x.get('count', 0))}" for x in top_fail_reasons[:3]) or "none:0"

    result["sources_total"] = sources_total
    result["success_count"] = success_count
    result["fail_count"] = fail_count
    result["top_fail_reasons"] = top_fail_reasons
    result["top_sources"] = top_sources

    updates = int(result.get("updates", result.get("total_mentions", 0) or 0))
    if updates == 0:
        result["status_message"] = (
            f"掃描統計：sources_total={sources_total}、success_count={success_count}、"
            f"fail_count={fail_count}、top_fail_reasons={fail_text}。"
        )
        result["trend_direction"] = str(result.get("trend_direction", "STABLE") or "STABLE")

    return result


def build_structured_executive_summary(
    news_cards: list[EduNewsCard],
    tone: str = "neutral",
    metrics: dict | None = None,
) -> dict[str, list[str]]:
    """Structured summary with numeric-first fallback to prevent 'air' content."""
    summary = dict(_v532_prev_build_structured_summary(news_cards, tone, metrics=metrics) or {})
    m = metrics or {}
    fetched_total = int(m.get("fetched_total", len(news_cards)))
    gate_pass_total = int(m.get("gate_pass_total", m.get("event_gate_pass_total", 0)))
    signal_gate_pass_total = int(m.get("signal_gate_pass_total", m.get("soft_pass_total", 0)))
    hard_pass_total = int(m.get("hard_pass_total", 0))
    soft_pass_total = int(m.get("soft_pass_total", 0))
    gate_reject_total = int(m.get("gate_reject_total", m.get("rejected_total", 0)))
    sources_total = int(m.get("sources_total", 0))
    success_count = int(m.get("sources_success", 0))
    fail_count = int(m.get("sources_failed", 0))
    reason_top = list(m.get("rejected_reason_top", []))
    reason_line = "、".join(f"{str(r)}:{int(c)}" for r, c in reason_top[:3]) or "none:0"

    defaults: dict[str, list[str]] = {
        "ai_trends": [
            (
                f"掃描統計：fetched_total={fetched_total}、gate_pass_total={gate_pass_total}、"
                f"signal_gate_pass_total={signal_gate_pass_total}。"
            ),
            f"門檻分流：hard_pass_total={hard_pass_total}、soft_pass_total={soft_pass_total}、gate_reject_total={gate_reject_total}。",
        ],
        "tech_landing": [
            f"來源覆蓋：sources_total={sources_total}、success_count={success_count}、fail_count={fail_count}。",
            f"拒絕原因 Top：{reason_line}。",
        ],
        "market_competition": [
            "若今日事件密度不足，改以 Signal/Cop Watch 的熱度與來源統計支撐決策。",
            "建議先以 WATCH/TEST 驗證，再等待下一輪可核對數據後決定是否 MOVE。",
        ],
        "opportunities_risks": [
            "機會：追蹤連續出現的公司/模型名與數字訊號，避免單篇摘要誤導。",
            "風險：若來源失敗率升高，需先處理抓取品質，再解讀事件重要性。",
        ],
        "recommended_actions": [
            "動作 1：先看 heat_score 與 platform_count 前三名，確認市場溫度是否持續。",
            "動作 2：交叉檢查 top_fail_reasons 與來源覆蓋，避免因資料缺口造成誤判。",
        ],
    }

    cleaned: dict[str, list[str]] = {}
    for key, fallback_lines in defaults.items():
        lines: list[str] = []
        for raw in list(summary.get(key, [])) + list(fallback_lines):
            line = _v532_clean_line(str(raw or ""), "")
            if not line:
                continue
            if line in lines:
                continue
            lines.append(line)
            if len(lines) >= 3:
                break
        while len(lines) < 2:
            lines.append(fallback_lines[len(lines)])
        cleaned[key] = lines

    return cleaned


# ---------------------------------------------------------------------------
# ReportQualityGuard — per-block density checking (v5.5)
# ---------------------------------------------------------------------------

from utils.text_quality import (
    count_evidence_terms,
    count_evidence_numbers,
    count_sentences,
    is_fragment as _is_fragment,
    trim_trailing_fragment as _trim_frag,
)

import logging as _logging_qg
_logger_qg = _logging_qg.getLogger("quality_guard")


def _block_density(text: str) -> dict:
    """Return density metrics for a single text block."""
    return {
        "evidence_terms": count_evidence_terms(text),
        "evidence_numbers": count_evidence_numbers(text),
        "sentences": count_sentences(text),
    }


def quality_guard_block(
    text: str,
    *,
    card: EduNewsCard | None = None,
    min_terms: int = 2,
    min_numbers: int = 1,
    min_sentences: int = 2,
) -> tuple[str, dict]:
    """Check a text block's density; backfill from card if too thin.

    Returns (improved_text, metrics_dict).
    """
    from config.settings import (
        PER_BLOCK_MIN_TERMS,
        PER_BLOCK_MIN_NUMBERS,
        PER_BLOCK_MIN_SENTENCES,
    )
    min_terms = min_terms or PER_BLOCK_MIN_TERMS
    min_numbers = min_numbers or PER_BLOCK_MIN_NUMBERS
    min_sentences = min_sentences or PER_BLOCK_MIN_SENTENCES

    # Trim fragments first.
    text = _trim_frag(sanitize(text)) if text else ""

    density_before = _block_density(text)
    filled_fields: list[str] = []
    trimmed = 0

    # Backfill from card data if density is insufficient.
    if card is not None:
        parts = [text] if text else []

        if density_before["evidence_terms"] < min_terms:
            title = getattr(card, "title_plain", "") or ""
            what = getattr(card, "what_happened", "") or ""
            for supplement in [title, what]:
                supplement = sanitize(supplement)
                if supplement and supplement not in text:
                    parts.append(supplement)
                    filled_fields.append("title/what")
                    break

        if density_before["evidence_numbers"] < min_numbers:
            why = getattr(card, "why_important", "") or ""
            why = sanitize(why)
            if why and why not in text:
                parts.append(why)
                filled_fields.append("why_important")

        if density_before["sentences"] < min_sentences:
            what = getattr(card, "what_happened", "") or ""
            what = sanitize(what)
            if what and what not in text:
                parts.append(what)
                filled_fields.append("what_happened")

        text = " ".join(p for p in parts if p)

    # Final fragment trim.
    text = _trim_frag(text)
    if _is_fragment(text):
        trimmed += 1
        text = ""

    density_after = _block_density(text)

    metrics = {
        "density_before": density_before,
        "density_after": density_after,
        "filled_fields": filled_fields,
        "trimmed_fragments": trimmed,
    }
    return text, metrics


def apply_quality_guard(
    cards: list[EduNewsCard],
    blocks: list[dict],
) -> tuple[list[dict], dict]:
    """Apply quality guard across all blocks for a deck.

    Each block dict is expected to have string values that can be density-checked.
    Returns (improved_blocks, aggregate_stats).
    """
    total_filled = 0
    total_trimmed = 0
    total_blocks = 0
    passed_blocks = 0

    card_lookup = {c.item_id: c for c in cards}

    improved: list[dict] = []
    for block in blocks:
        card_id = block.get("item_id", "")
        card = card_lookup.get(card_id)
        new_block = dict(block)
        block_passed = True

        for key in ["what_happened", "why_important", "body", "text"]:
            if key not in new_block:
                continue
            val = new_block[key]
            if not isinstance(val, str):
                continue
            guarded, metrics = quality_guard_block(val, card=card)
            new_block[key] = guarded
            total_filled += len(metrics["filled_fields"])
            total_trimmed += metrics["trimmed_fragments"]
            if not guarded:
                block_passed = False

        total_blocks += 1
        if block_passed:
            passed_blocks += 1
        improved.append(new_block)

    stats = {
        "total_blocks": total_blocks,
        "passed_blocks": passed_blocks,
        "total_filled": total_filled,
        "total_trimmed": total_trimmed,
    }
    _logger_qg.info(
        "QUALITY_GUARD filled_fields=%d trimmed_fragments=%d "
        "density_before=%d/%d density_after=%d/%d",
        total_filled, total_trimmed,
        total_blocks - passed_blocks, total_blocks,
        passed_blocks, total_blocks,
    )
    return improved, stats


# ---------------------------------------------------------------------------
# Semantic guard — last-mile hollow-content prevention
# ---------------------------------------------------------------------------


def semantic_guard_text(
    text: str,
    card: "EduNewsCard",
    *,
    context: str = "",
    density_threshold: int = 80,
) -> str:
    """Return semantically sound text, backfilling from card if text is hollow.

    If *text* is a placeholder/fragment, too short (< 15 non-space chars), or
    has semantic density below *density_threshold*:
      1. Step 1 — candidates from card (why/what/title) that pass density gate
      2. Step 2 — candidates from card that pass basic validity (not fragment, len OK)
      3. Guaranteed fallback: structured sentence with sanitized title, source,
         and impact score (always includes a number for density scoring).

    Never returns empty string or a fragment.
    Does NOT modify the card schema.
    Applies EN-ZH Hybrid Glossing v1 to every returned string.
    """
    from utils.semantic_quality import is_placeholder_or_fragment, semantic_density_score
    from utils.hybrid_glossing import normalize_exec_text as _norm_gloss, load_glossary as _load_gl
    _glossary = _load_gl()

    def _is_good(t: str, require_density: bool = True) -> bool:
        t = t.strip()
        if not t or is_placeholder_or_fragment(t):
            return False
        if len(t.replace(" ", "")) < 15:
            return False
        if require_density and semantic_density_score(t) < density_threshold:
            return False
        return True

    s = (text or "").strip()
    # Fast-pass: text meets all quality gates
    if _is_good(s, require_density=True):
        return _norm_gloss(s, _glossary)

    # Build backfill candidates from card (all run through sanitize)
    what = sanitize(card.what_happened or "")
    why = sanitize(card.why_important or "")
    title = sanitize(card.title_plain or "")
    source = (getattr(card, "source_name", None) or "來源").strip()

    candidates = [why, what, title] if context == "action" else [what, why, title]

    # Step 1: candidates that also pass semantic density gate
    for cand in candidates:
        if _is_good(cand, require_density=True):
            return _norm_gloss(cand.strip()[:120], _glossary)

    # Step 2: candidates that pass basic validity (not fragment, len OK) but not density
    for cand in candidates:
        if _is_good(cand, require_density=False):
            return _norm_gloss(cand.strip()[:120], _glossary)

    # Guaranteed structured fallback — sanitize title, use category if title is hollow,
    # always include a numeric score so density scoring can find a number.
    title_raw = sanitize(card.title_plain or "").strip()
    if not title_raw or is_placeholder_or_fragment(title_raw):
        title_safe = f"{card.category or 'AI'}領域事件"
    else:
        title_safe = title_raw[:40]
    score_val = float(getattr(card, "final_score", 5.0) or 5.0)
    return _norm_gloss(
        f"{title_safe}（{source}，影響評分 {score_val:.1f}/10）。"
        f"建議相關負責人於 T+7 日內評估確認。",
        _glossary,
    )


# ---------------------------------------------------------------------------
# Executive selection: quota-based topic routing (B + C)
# ---------------------------------------------------------------------------

_EXEC_QUOTA: dict[str, int] = {"product": 2, "tech": 2, "business": 2, "dev": 1}
_EXEC_MAX_TOTAL: int = 12
_EXEC_SPARSE_THRESHOLD: int = 6


def select_executive_items(
    candidates: list[EduNewsCard],
    extra_pool: "list[EduNewsCard] | None" = None,
) -> tuple[list[EduNewsCard], dict]:
    """Quota-based selection with relevance gate for executive events.

    Returns (selected_items, selection_meta_dict).

    Flow:
      1. Classify each candidate via topic_router (channel scores + relevance gate)
      2. Hard-reject building/non-AI noise
      3. Bucket candidates by best channel
      4. Fill quota from each bucket (product>=2, tech>=2, business>=2, dev>=1)
      5. Backfill with remaining relevant items up to _EXEC_MAX_TOTAL
      6. Channel Backfill: for any channel still below quota, scan extra_pool
         and unused bucket items for high channel-score candidates
      7. If total < _EXEC_SPARSE_THRESHOLD → mark sparse_day=True (WARN-OK)
    """
    from utils.topic_router import classify_channels, is_relevant_ai

    quota = dict(_EXEC_QUOTA)
    buckets: dict[str, list[tuple[float, EduNewsCard]]] = {k: [] for k in quota}
    rejected_irrelevant: list[EduNewsCard] = []
    rejected_top_reasons: list[str] = []

    for card in candidates:
        text = " ".join(filter(None, [
            str(getattr(card, "title_plain", "") or ""),
            str(getattr(card, "what_happened", "") or ""),
            str(getattr(card, "why_important", "") or ""),
        ]))
        url = str(getattr(card, "source_url", "") or "")

        relevant, reasons = is_relevant_ai(text, url)
        if not relevant:
            rejected_irrelevant.append(card)
            if reasons:
                rejected_top_reasons.extend(reasons[:1])
            continue

        ch = classify_channels(text, url)
        best = ch["best_channel"]
        # Combine channel score (primary) + final_score (secondary) for ranking
        ch_score = max(
            ch["product_score"], ch["tech_score"],
            ch["business_score"], ch["dev_score"],
        )
        final_s = float(getattr(card, "final_score", 0.0) or 0.0)
        rank = ch_score * 0.7 + final_s * 3.0

        if best in buckets:
            buckets[best].append((rank, card))
        else:
            buckets["dev"].append((rank, card))

    # Sort each bucket by rank descending
    for b in buckets:
        buckets[b].sort(key=lambda x: x[0], reverse=True)

    selected: list[EduNewsCard] = []
    used_ids: set[str] = set()

    def _add(card: EduNewsCard) -> bool:
        iid = str(getattr(card, "item_id", "") or id(card))
        if iid in used_ids:
            return False
        selected.append(card)
        used_ids.add(iid)
        return True

    # First pass: quota fill per bucket
    for bucket, target in quota.items():
        filled = 0
        for _rank, card in buckets[bucket]:
            if filled >= target:
                break
            if _add(card):
                filled += 1

    # Second pass: backfill with remaining ranked candidates
    all_remaining: list[tuple[float, EduNewsCard]] = []
    for b in buckets:
        for rank, card in buckets[b]:
            iid = str(getattr(card, "item_id", "") or id(card))
            if iid not in used_ids:
                all_remaining.append((rank, card))
    all_remaining.sort(key=lambda x: x[0], reverse=True)
    for _rank, card in all_remaining:
        if len(selected) >= _EXEC_MAX_TOTAL:
            break
        _add(card)

    # Forced channel attribution: cards added via backfill are credited to the
    # channel they were selected for, overriding topic-router recount.
    # key = item_id (or id(card) fallback), value = channel name
    _forced_channel: dict[str, str] = {}

    # Compute per-bucket count on selected items
    def _recount_by_bucket() -> dict[str, int]:
        _bb: dict[str, int] = {k: 0 for k in quota}
        for _card in selected:
            _iid = str(getattr(_card, "item_id", "") or id(_card))
            if _iid in _forced_channel:
                # Backfill-attributed: use the channel that caused selection
                _b = _forced_channel[_iid]
            else:
                _text = " ".join(filter(None, [
                    str(getattr(_card, "title_plain", "") or ""),
                    str(getattr(_card, "what_happened", "") or ""),
                ]))
                _url = str(getattr(_card, "source_url", "") or "")
                _ch = classify_channels(_text, _url)
                _b = _ch["best_channel"]
            if _b in _bb:
                _bb[_b] += 1
            else:
                _bb["dev"] += 1
        return _bb

    by_bucket = _recount_by_bucket()
    _pre_backfill_by_bucket = dict(by_bucket)  # snapshot before channel backfill runs

    quota_pass = all(by_bucket.get(b, 0) >= q for b, q in quota.items() if b != "dev")
    sparse_day = len(selected) < _EXEC_SPARSE_THRESHOLD

    # ---------------------------------------------------------------------------
    # Channel Backfill Track — for any quota channel still below target,
    # scan unused bucket items AND extra_pool for high channel-score candidates.
    #
    # Settings (env-overridable, per channel):
    #   Z0_EXEC_MIN_CHANNEL_BIZ/PROD/TECH  — min channel_score  (defaults: 55/55/50)
    #   Z0_EXEC_MIN_FRONTIER_BIZ/PROD/TECH — min frontier_score  (defaults: 40/35/35)
    #   EXEC_MIN_BUSINESS/PRODUCT/TECH      — quota targets       (default: 2/2/2)
    # ---------------------------------------------------------------------------
    import os as _os

    _ch_score_defaults   = {"business": 55, "product": 55, "tech": 45}  # TECH: 50→45
    _ch_frontier_defaults = {"business": 40, "product": 35, "tech": 30}  # TECH: 35→30
    _ch_env_suffixes     = {"business": "BIZ", "product": "PROD", "tech": "TECH"}
    _ch_min_target_envs  = {
        "business": "EXEC_MIN_BUSINESS",
        "product":  "EXEC_MIN_PRODUCT",
        "tech":     "EXEC_MIN_TECH",
    }

    _channel_backfills: dict[str, dict] = {}
    _any_backfill_fired = False

    def _is_72h_recent(_card: EduNewsCard) -> bool:
        _pub = str(getattr(_card, "published_at", "") or "")
        if not _pub:
            return True  # no date → assume recent
        try:
            from datetime import datetime as _dt, timezone as _tz, timedelta as _td
            _pub_dt = _dt.fromisoformat(_pub.replace("Z", "+00:00"))
            return _dt.now(_tz.utc) - _pub_dt <= _td(hours=72)
        except Exception:
            return True

    def _ch_pool_from_source(
        source_items: "list[tuple[float, EduNewsCard]] | list[EduNewsCard]",
        ch_name: str,
        min_score: int,
        min_frontier: int,
        *,
        tuples: bool = True,
    ) -> "list[tuple[int, EduNewsCard]]":
        """Collect backfill candidates with sufficient channel score from a source list."""
        _result: list[tuple[int, EduNewsCard]] = []
        _score_key = f"{ch_name}_score"
        for _entry in source_items:
            _card = _entry[1] if tuples else _entry  # type: ignore[index]
            _iid = str(getattr(_card, "item_id", "") or id(_card))
            if _iid in used_ids:
                continue
            _text = " ".join(filter(None, [
                str(getattr(_card, "title_plain", "") or ""),
                str(getattr(_card, "what_happened", "") or ""),
            ]))
            _url = str(getattr(_card, "source_url", "") or "")
            _ch_s = classify_channels(_text, _url)
            if _ch_s[_score_key] < min_score:
                continue
            _frontier = int(getattr(_card, "z0_frontier_score", 0) or 0)
            if _frontier > 0 and _frontier < min_frontier:
                continue
            if not _is_72h_recent(_card):
                continue
            _result.append((_ch_s[_score_key], _card))
        return _result

    for _ch_name in ["business", "product", "tech"]:
        _ch_suffix  = _ch_env_suffixes[_ch_name]
        _min_target = int(_os.environ.get(_ch_min_target_envs[_ch_name], "2") or "2")
        _min_score  = int(
            _os.environ.get(f"Z0_EXEC_MIN_CHANNEL_{_ch_suffix}",
                            str(_ch_score_defaults[_ch_name]))
            or str(_ch_score_defaults[_ch_name])
        )
        _min_frontier = int(
            _os.environ.get(f"Z0_EXEC_MIN_FRONTIER_{_ch_suffix}",
                            str(_ch_frontier_defaults[_ch_name]))
            or str(_ch_frontier_defaults[_ch_name])
        )

        _ch_bf: dict = {
            "candidates_total": 0,
            "selected_total": 0,
            "selected_ids": [],
            "triggered": False,
            "note": "quota_met_by_primary_pool",
            "extra_pool_selected": 0,
        }
        _channel_backfills[_ch_name] = _ch_bf

        if by_bucket.get(_ch_name, 0) >= _min_target:
            continue  # already meets quota — triggered stays False

        _ch_bf["triggered"] = True

        # Gather candidates: unused bucket items first, then extra_pool (tracked separately)
        _bucket_items: list[tuple[float, EduNewsCard]] = []
        for _b in buckets:
            _bucket_items.extend(buckets[_b])

        _ch_pool_bucket = _ch_pool_from_source(
            _bucket_items, _ch_name, _min_score, _min_frontier, tuples=True
        )
        _ch_pool_extra = (
            _ch_pool_from_source(extra_pool, _ch_name, _min_score, _min_frontier, tuples=False)
            if extra_pool else []
        )
        # Track IDs that came from extra_pool for origin_counts accounting
        _extra_pool_ids_ch: set[str] = {
            str(getattr(_c, "item_id", "") or id(_c)) for _, _c in _ch_pool_extra
        }
        _ch_pool = _ch_pool_bucket + _ch_pool_extra

        _ch_bf["candidates_total"] = len(_ch_pool)
        _ch_pool.sort(key=lambda x: x[0], reverse=True)

        _needed = _min_target - by_bucket.get(_ch_name, 0)
        _extra_selected_count = 0
        for _bscore, _card in _ch_pool:
            if _needed <= 0:
                break
            if _add(_card):
                _iid = str(getattr(_card, "item_id", "") or id(_card))
                _ch_bf["selected_ids"].append(_iid)
                # Force-attribute this card to the channel it was selected for
                _forced_channel[_iid] = _ch_name
                if _iid in _extra_pool_ids_ch:
                    _extra_selected_count += 1
                _needed -= 1

        _ch_bf["selected_total"] = len(_ch_bf["selected_ids"])
        _ch_bf["selected_ids"] = _ch_bf["selected_ids"][:5]
        _ch_bf["extra_pool_selected"] = _extra_selected_count
        _ch_bf["note"] = (
            "quota_filled_by_backfill" if _ch_bf["selected_total"] > 0
            else "quota_unmet_and_no_candidates"
        )

        if _ch_bf["selected_total"] > 0:
            _any_backfill_fired = True

    if _any_backfill_fired:
        by_bucket = _recount_by_bucket()
        quota_pass = all(by_bucket.get(b, 0) >= q for b, q in quota.items() if b != "dev")

    _empty_ch_bf: dict = {
        "candidates_total": 0, "selected_total": 0, "selected_ids": [],
        "triggered": False, "note": "quota_met_by_primary_pool", "extra_pool_selected": 0,
    }
    selection_meta: dict = {
        "events_total": len(selected),
        "events_by_bucket": dict(by_bucket),
        "pre_backfill_by_bucket": _pre_backfill_by_bucket,
        "rejected_irrelevant_count": len(rejected_irrelevant),
        "rejected_top_reasons": list(dict.fromkeys(rejected_top_reasons))[:5],
        "quota_target": dict(quota),
        "quota_pass": quota_pass,
        "sparse_day": sparse_day,
        "business_backfill": _channel_backfills.get("business", _empty_ch_bf),
        "product_backfill":  _channel_backfills.get("product",  _empty_ch_bf),
        "tech_backfill":     _channel_backfills.get("tech",     _empty_ch_bf),
    }
    return selected, selection_meta


def write_exec_selection_meta(meta: dict, project_root: "Path | None" = None) -> None:
    """Write exec_selection.meta.json to outputs/ — non-breaking side effect."""
    import json
    from pathlib import Path

    try:
        root = project_root or Path(__file__).resolve().parent.parent
        out_dir = root / "outputs"
        out_dir.mkdir(parents=True, exist_ok=True)
        (out_dir / "exec_selection.meta.json").write_text(
            json.dumps(meta, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except Exception:
        pass  # audit write must never break the pipeline


def write_exec_kpi_meta(sel_meta: dict, project_root: "Path | None" = None) -> None:
    """Write exec_kpi.meta.json to outputs/ with KPI targets/actuals and backfill audit."""
    import json
    import os
    from pathlib import Path

    try:
        root = project_root or Path(__file__).resolve().parent.parent
        out_dir = root / "outputs"
        out_dir.mkdir(parents=True, exist_ok=True)

        min_ev = int(os.environ.get("EXEC_MIN_EVENTS",   "6") or "6")
        min_pr = int(os.environ.get("EXEC_MIN_PRODUCT",  "2") or "2")
        min_te = int(os.environ.get("EXEC_MIN_TECH",     "2") or "2")
        min_bu = int(os.environ.get("EXEC_MIN_BUSINESS", "2") or "2")

        ev_by_bucket = sel_meta.get("events_by_bucket", {})
        _pre_bf_bb   = sel_meta.get("pre_backfill_by_bucket", {})
        _empty_bf: dict = {
            "candidates_total": 0, "selected_total": 0, "selected_ids": [],
            "triggered": False, "note": "quota_met_by_primary_pool", "extra_pool_selected": 0,
        }

        def _bf(key: str) -> dict:
            return sel_meta.get(key, _empty_bf)

        def _origin_counts(ch_key: str, bf_key: str) -> dict:
            _b = _bf(bf_key)
            return {
                "primary_pool": _pre_bf_bb.get(ch_key, 0),
                "extra_pool":   _b.get("extra_pool_selected", 0),
                "backfill":     _b.get("selected_total", 0),
            }

        kpi_meta: dict = {
            "kpi_targets": {
                "events":   min_ev,
                "product":  min_pr,
                "tech":     min_te,
                "business": min_bu,
            },
            "kpi_actuals": {
                "events":   sel_meta.get("events_total", 0),
                "product":  ev_by_bucket.get("product", 0),
                "tech":     ev_by_bucket.get("tech", 0),
                "business": ev_by_bucket.get("business", 0),
            },
            "business_backfill":      _bf("business_backfill"),
            "product_backfill":       _bf("product_backfill"),
            "tech_backfill":          _bf("tech_backfill"),
            "business_origin_counts": _origin_counts("business", "business_backfill"),
            "product_origin_counts":  _origin_counts("product",  "product_backfill"),
            "tech_origin_counts":     _origin_counts("tech",     "tech_backfill"),
        }
        (out_dir / "exec_kpi.meta.json").write_text(
            json.dumps(kpi_meta, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
    except Exception:
        pass  # audit write must never break the pipeline


# ---------------------------------------------------------------------------
# G1/G2/G3 quality gate helper functions (pure, testable)
# ---------------------------------------------------------------------------

def _compute_source_diversity(selected_cards: list, max_share: float = 0.45) -> dict:
    """Compute source distribution and gate result from a list of EduNewsCard."""
    events_by_source: dict = {}
    for card in selected_cards:
        src = _v522_source_name(card) if hasattr(card, "__class__") else "unknown"
        events_by_source[src] = events_by_source.get(src, 0) + 1
    total = max(len(selected_cards), 1)
    if events_by_source:
        max_src = max(events_by_source, key=lambda k: events_by_source[k])
        max_src_count = events_by_source[max_src]
    else:
        max_src, max_src_count = "none", 0
    max_src_share = round(max_src_count / total, 4)
    gate = "PASS" if max_src_share <= max_share else "FAIL"
    return {
        "events_by_source": events_by_source,
        "max_source": max_src,
        "max_source_share": max_src_share,
        "source_diversity_gate": gate,
    }


def _compute_proof_coverage(selected_cards: list, min_coverage: float = 0.85) -> dict:
    """Compute proof token coverage (G3) over selected event cards.

    Uses the strict whitelist (count_proof_evidence_tokens) so that bare
    percentages, generic year numbers, and K/GB/TB unit counts do NOT count
    as hard evidence.  Also computes PROOF_EMPTY_GATE: cards with zero
    whitelisted proof tokens are listed separately.
    """
    from utils.narrative_compact import count_proof_evidence_tokens

    proof_missing_ids: list = []
    proof_empty_ids: list = []
    proof_covered = 0
    for card in selected_cards:
        cid = str(getattr(card, "id", None) or getattr(card, "item_id", None) or "?")
        text = " ".join(filter(None, [
            str(getattr(card, "title_plain", "") or ""),
            str(getattr(card, "what_happened", "") or ""),
            str(getattr(card, "why_important", "") or ""),
            str(getattr(card, "technical_interpretation", "") or ""),
        ]))
        token_count = count_proof_evidence_tokens(text)
        if token_count > 0:
            proof_covered += 1
        else:
            proof_missing_ids.append(cid)
            proof_empty_ids.append(cid)

    if not selected_cards:
        return {
            "proof_coverage_ratio": 1.0,
            "proof_missing_event_ids": [],
            "proof_coverage_gate": "PASS",
            "proof_empty_event_count": 0,
            "proof_empty_event_ids": [],
            "proof_empty_gate": "PASS",
        }

    ratio = round(proof_covered / len(selected_cards), 4)
    gate = "PASS" if ratio >= min_coverage else "FAIL"
    proof_empty_count = len(proof_empty_ids)
    return {
        "proof_coverage_ratio": ratio,
        "proof_missing_event_ids": proof_missing_ids[:10],
        "proof_coverage_gate": gate,
        "proof_empty_event_count": proof_empty_count,
        "proof_empty_event_ids": proof_empty_ids[:10],
        "proof_empty_gate": "PASS" if proof_empty_count == 0 else "FAIL",
    }


def write_exec_quality_meta(
    selected_cards: list,
    selection_meta: dict,
    project_root: "Path | None" = None,
) -> None:
    """Write outputs/exec_quality.meta.json (G1/G2/G3 gates).

    G4 (fragment_leak) is updated later by ppt_generator._v1_write_exec_layout_meta.
    Never raises — audit writes must not break the pipeline.
    However, if G2 or G3 FAIL and sparse_day is False, a RuntimeError is raised
    so the pipeline exits before generating a low-quality report.
    """
    import json
    import os

    try:
        root = project_root or Path(__file__).resolve().parent.parent
        out_dir = root / "outputs"
        out_dir.mkdir(parents=True, exist_ok=True)

        sparse_day: bool = bool(selection_meta.get("sparse_day", False))

        # G1: AI relevance rejection counts (already computed by select_executive_items)
        non_ai_rejected_count = int(selection_meta.get("rejected_irrelevant_count", 0))
        non_ai_rejected_top_reasons: list = list(
            selection_meta.get("rejected_top_reasons", [])
        )[:5]

        # G2: Source diversity
        max_share_threshold = float(os.environ.get("MAX_SOURCE_SHARE", "0.45"))
        g2 = _compute_source_diversity(selected_cards, max_share=max_share_threshold)

        # G3: Proof coverage
        min_proof = float(os.environ.get("MIN_PROOF_COVERAGE", "0.85"))
        g3 = _compute_proof_coverage(selected_cards, min_coverage=min_proof)

        meta: dict = {
            "non_ai_rejected_count": non_ai_rejected_count,
            "non_ai_rejected_top_reasons": non_ai_rejected_top_reasons,
            **g2,
            **g3,
            # G4 placeholder — updated by ppt_generator after generation
            "fragments_detected": 0,
            "fragments_fixed": 0,
            "fragments_leaked": 0,
            "fragment_leak_gate": "PASS",
            # Actions normalization placeholders — updated by ppt_generator
            "actions_normalized_count": 0,
            "actions_fragment_leak_count": 0,
            # ZH skeletonization placeholder — updated by ppt_generator
            "english_heavy_skeletonized_count": 0,
        }

        (out_dir / "exec_quality.meta.json").write_text(
            json.dumps(meta, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

        # Fail-fast for G2, G3, PROOF_EMPTY (skip on sparse day)
        if not sparse_day:
            if g2["source_diversity_gate"] == "FAIL":
                raise RuntimeError(
                    f"SOURCE_DIVERSITY_GATE FAIL: max_source='{g2['max_source']}' "
                    f"share={g2['max_source_share']:.1%} > {max_share_threshold:.1%}"
                )
            if g3["proof_coverage_gate"] == "FAIL":
                raise RuntimeError(
                    f"PROOF_COVERAGE_GATE FAIL: coverage={g3['proof_coverage_ratio']:.1%} "
                    f"< {min_proof:.1%}  missing_ids={g3['proof_missing_event_ids'][:5]}"
                )
            if g3.get("proof_empty_gate") == "FAIL":
                raise RuntimeError(
                    f"PROOF_EMPTY_GATE FAIL: {g3['proof_empty_event_count']} events "
                    f"have no whitelisted proof tokens  ids={g3['proof_empty_event_ids'][:5]}"
                )
    except RuntimeError:
        raise  # propagate fail-fast gates
    except Exception:
        pass  # other errors must not break pipeline


# ---------------------------------------------------------------------------
# v5.2.5 — Longform Narrative enrichment layer for build_ceo_brief_blocks
# ---------------------------------------------------------------------------

_v525_prev_build_ceo_brief_blocks = build_ceo_brief_blocks


def build_ceo_brief_blocks(card: EduNewsCard) -> dict:  # type: ignore[misc]
    """V5.2.5: wrap previous build_ceo_brief_blocks with BBC longform enrichment."""
    brief = dict(_v525_prev_build_ceo_brief_blocks(card))
    try:
        from utils.longform_narrative import render_bbc_longform
        lf = render_bbc_longform(card)
        if lf.get("eligible") and not lf.get("watchlist"):
            # Slide A enrichment
            if lf.get("bg"):
                brief["event_liner"] = lf["bg"]
            if lf.get("what_is"):
                brief["ai_trend_liner"] = lf["what_is"]
            # Slide B enrichment
            if lf.get("why"):
                brief["q1_meaning"] = lf["why"]
            if lf.get("risks"):
                brief["q2_impact"] = lf["risks"]
            if lf.get("next"):
                next_val = lf["next"]
                if isinstance(brief.get("q3_actions"), list):
                    # Prepend longform next as first action item
                    brief["q3_actions"] = [next_val] + list(brief["q3_actions"])[:2]
                else:
                    brief["q3_actions"] = [next_val]
    except Exception:
        pass  # longform enrichment is non-fatal
    return brief


# ---------------------------------------------------------------------------
# v5.2.6 — Final-mile sanitizer layer for build_ceo_brief_blocks
# ---------------------------------------------------------------------------

_v526_prev_build_ceo_brief_blocks = build_ceo_brief_blocks


def build_ceo_brief_blocks(card: EduNewsCard) -> dict:  # type: ignore[misc]
    """V5.2.6: Apply exec_sanitizer to all text fields after longform enrichment."""
    brief = dict(_v526_prev_build_ceo_brief_blocks(card))
    try:
        from utils.exec_sanitizer import sanitize_exec_text

        _STR_FIELDS = (
            "event_liner", "ai_trend_liner",
            "q1_meaning", "q2_impact",
        )
        for field in _STR_FIELDS:
            if field in brief and isinstance(brief[field], str):
                brief[field] = sanitize_exec_text(brief[field])

        if "q3_actions" in brief and isinstance(brief["q3_actions"], list):
            brief["q3_actions"] = [
                sanitize_exec_text(str(a)) for a in brief["q3_actions"]
            ]
    except Exception:
        pass  # sanitizer is non-fatal
    return brief

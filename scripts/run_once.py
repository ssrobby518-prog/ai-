"""Run the full pipeline once: Ingest -> Process -> Store -> Deliver."""

import sys
import time
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from config import settings
from core.ai_core import process_batch
from core.content_strategy import (
    build_corp_watch_summary,
    build_signal_summary,
    is_non_event_or_index,
)
from core.deep_analyzer import analyze_batch
from core.deep_delivery import write_deep_analysis
from core.delivery import print_console_summary, push_to_feishu, push_to_notion, write_digest
from core.education_renderer import (
    generate_binary_reports,
    generate_executive_reports,
    render_education_report,
    render_error_report,
    write_education_reports,
)
from core.ingestion import batch_items, dedup_items, fetch_all_feeds, filter_items
from core.info_density import apply_density_gate
from core.notifications import send_all_notifications
from core.storage import get_existing_item_ids, init_db, save_items, save_results
from schemas.education_models import EduNewsCard
from utils.entity_cleaner import clean_entities
from utils.logger import setup_logger
from utils.metrics import get_collector, reset_collector


def _apply_entity_cleaning(all_results: list) -> None:
    """Clean entities on all results and record metrics."""
    collector = get_collector()
    for r in all_results:
        a = r.schema_a
        before = len(a.entities)
        result = clean_entities(
            entities=a.entities,
            category=a.category,
            key_points=a.key_points,
            title=a.title_zh,
            body=a.summary_zh,
        )
        a.entities = result.cleaned
        collector.record_entity_cleaning(before, len(result.cleaned))


def _build_quality_cards(all_results: list) -> list[EduNewsCard]:
    """Build lightweight EduNewsCard objects for v5.2 metrics aggregation."""
    cards: list[EduNewsCard] = []
    for r in all_results:
        a = r.schema_a
        b = r.schema_b
        source_url = str(a.source_id or "")
        cards.append(
            EduNewsCard(
                item_id=str(r.item_id),
                is_valid_news=bool(getattr(r, "passed_gate", False)),
                invalid_reason="" if bool(getattr(r, "passed_gate", False)) else "failed_gate",
                title_plain=str(a.title_zh or ""),
                what_happened=str(a.summary_zh or ""),
                why_important=str(a.summary_zh or ""),
                source_name=str(a.source_id or ""),
                source_url=source_url if source_url.startswith("http") else "",
                category=str(a.category or ""),
                final_score=float(getattr(b, "final_score", 0.0) or 0.0),
            )
        )
    return cards


def _build_soft_quality_cards_from_filtered(filtered_items: list) -> list[EduNewsCard]:
    """Build low-confidence cards from post-gate RawItems when AI results are empty."""
    cards: list[EduNewsCard] = []
    for item in filtered_items:
        title = str(getattr(item, "title", "") or "").strip() or "來源訊號"
        body = str(getattr(item, "body", "") or "").strip()
        summary = body[:260] if body else "來源內容有限，先保留為低信心觀測訊號。"
        source_name = str(getattr(item, "source_name", "") or "").strip() or "platform"
        source_url = str(getattr(item, "url", "") or "").strip()
        density = float(getattr(item, "density_score", 0) or 0)
        score = max(3.0, min(10.0, round(density / 10.0, 2)))
        card = EduNewsCard(
            item_id=str(getattr(item, "item_id", "") or ""),
            is_valid_news=True,
            title_plain=title,
            what_happened=summary,
            why_important=f"來源：{source_name}。此卡片為低信心事件候選，用於避免報告空白。",
            source_name=source_name,
            source_url=source_url if source_url.startswith("http") else "",
            category=str(getattr(item, "source_category", "") or "tech"),
            final_score=score,
        )
        try:
            setattr(card, "low_confidence", True)
            setattr(card, "confidence", "low")
            setattr(card, "density_score", int(density))
            setattr(card, "density_tier", "B")
        except Exception:
            pass
        cards.append(card)
    return cards


def run_pipeline() -> None:
    """Execute the full pipeline once."""
    log = setup_logger(settings.LOG_PATH)
    log.info("=" * 60)
    log.info("PIPELINE START")
    log.info("=" * 60)
    t_start = time.time()
    from datetime import UTC, datetime

    t_start_iso = datetime.now(UTC).isoformat()

    # Initialize metrics collector
    collector = reset_collector()
    collector.start()

    # Ensure DB exists
    init_db(settings.DB_PATH)

    # Z1: Ingestion & Preprocessing
    log.info("--- Z1: Ingestion & Preprocessing ---")
    raw_items = fetch_all_feeds()
    log.info("Fetched %d total raw items", len(raw_items))
    collector.fetched_total = len(raw_items)
    # fetch_all_feeds() already returns normalized + enrichment-applied items.
    collector.normalized_total = len(raw_items)
    collector.enriched_total = len(raw_items)
    log.info(
        "INGEST_COUNTS fetched_total=%d normalized_total=%d enriched_total=%d",
        collector.fetched_total,
        collector.normalized_total,
        collector.enriched_total,
    )

    if not raw_items:
        log.warning("No items fetched from any feed. Exiting.")
        collector.stop()
        collector.write_json()
        send_all_notifications(t_start_iso, 0, True, "")
        return

    # Dedup against DB + within batch
    existing_ids = get_existing_item_ids(settings.DB_PATH)
    log.info("Existing items in DB: %d", len(existing_ids))
    deduped = dedup_items(raw_items, existing_ids)
    collector.deduped_total = len(deduped)
    log.info("INGEST_COUNTS deduped_total=%d", collector.deduped_total)

    # Filter
    filtered, filter_summary = filter_items(deduped)
    gate_stats = dict(filter_summary.gate_stats or {})
    collector.gate_pass_total = int(gate_stats.get("gate_pass_total", filter_summary.kept_count))
    collector.hard_pass_total = int(gate_stats.get("hard_pass_total", gate_stats.get("passed_strict", 0)))
    collector.soft_pass_total = int(gate_stats.get("soft_pass_total", gate_stats.get("passed_relaxed", 0)))
    collector.gate_reject_total = int(gate_stats.get("gate_reject_total", 0))
    collector.rejected_total = int(gate_stats.get("rejected_total", collector.gate_reject_total))
    collector.after_filter_total = len(filtered)
    collector.rejected_reason_top = list(gate_stats.get("rejected_reason_top", []))
    collector.density_score_top5 = list(gate_stats.get("density_score_top5", []))
    log.info(
        "INGEST_COUNTS hard_pass_total=%d soft_pass_total=%d gate_pass_total=%d gate_reject_total=%d after_filter_total=%d rejected_reason_top=%s density_score_top5=%s",
        collector.hard_pass_total,
        collector.soft_pass_total,
        collector.gate_pass_total,
        collector.gate_reject_total,
        collector.after_filter_total,
        collector.rejected_reason_top,
        collector.density_score_top5,
    )

    # Build filter_summary dict for Z5
    filter_summary_dict: dict = {
        "input_count": filter_summary.input_count,
        "kept_count": filter_summary.kept_count,
        "dropped_by_reason": dict(filter_summary.dropped_by_reason),
    }

    all_results: list = []
    digest_path = None

    if filtered:
        # Save raw items to DB
        save_items(settings.DB_PATH, filtered)

        # Z2: AI Core (batch processing)
        log.info("--- Z2: AI Core ---")
        for batch_num, batch in enumerate(batch_items(filtered), 1):
            log.info("Processing batch %d (%d items)", batch_num, len(batch))
            results = process_batch(batch)
            all_results.extend(results)

        # Entity cleaning (between extraction and deep analysis)
        _apply_entity_cleaning(all_results)

        # Update metrics
        collector.total_items = len(all_results)
        collector.passed_gate = sum(1 for r in all_results if r.passed_gate)

        # Z3: Storage & Delivery
        log.info("--- Z3: Storage & Delivery ---")
        save_results(settings.DB_PATH, all_results)

        # Local sink
        digest_path = write_digest(all_results)
        print_console_summary(all_results)

        # Optional sinks
        push_to_notion(all_results)
        push_to_feishu(all_results)
    else:
        log.warning("No items passed filters — skipping Z2/Z3, proceeding to Z4/Z5.")
        digest_path = write_digest([])
        print_console_summary([])

    # Z4: Deep Analysis (non-blocking)
    z4_report = None  # 供 Z5 使用
    if settings.DEEP_ANALYSIS_ENABLED:
        passed_results = [r for r in all_results if r.passed_gate]
        if passed_results:
            try:
                log.info("--- Z4: Deep Analysis ---")
                z4_report = analyze_batch(passed_results)
                deep_path = write_deep_analysis(z4_report, metrics_md=collector.as_markdown())
                log.info("Deep analysis: %s", deep_path)
            except Exception as exc:
                log.error("Z4 Deep Analysis failed (non-blocking): %s", exc)
        else:
            log.info("Z4: No passed items, skipping deep analysis")
    else:
        log.info("Z4: Deep analysis disabled")

    quality_cards = _build_quality_cards(all_results)
    if not quality_cards and filtered:
        quality_cards = _build_soft_quality_cards_from_filtered(filtered)
    density_candidates = [c for c in quality_cards if c.is_valid_news]
    event_candidates = [c for c in density_candidates if not is_non_event_or_index(c)]

    event_density_cards, _event_rejected, event_density_stats, _ = apply_density_gate(
        event_candidates, "event"
    )
    _signal_density_cards, _signal_rejected, signal_density_stats, _ = apply_density_gate(
        density_candidates, "signal"
    )
    _corp_density_cards, _corp_rejected, corp_density_stats, _ = apply_density_gate(
        density_candidates, "corp"
    )

    collector.density_total_in = event_density_stats.total_in
    collector.density_passed = event_density_stats.passed
    collector.density_rejected = event_density_stats.rejected_total
    collector.density_avg_score = event_density_stats.avg_score
    collector.density_rejected_reason_top = list(event_density_stats.rejected_reason_top)

    log.info(
        "INFO_DENSITY[event] total_in=%d passed=%d rejected=%d avg_score=%.2f reasons_top=%s",
        event_density_stats.total_in,
        event_density_stats.passed,
        event_density_stats.rejected_total,
        event_density_stats.avg_score,
        event_density_stats.rejected_reason_top,
    )
    log.info(
        "INFO_DENSITY[signal] total_in=%d passed=%d rejected=%d avg_score=%.2f reasons_top=%s",
        signal_density_stats.total_in,
        signal_density_stats.passed,
        signal_density_stats.rejected_total,
        signal_density_stats.avg_score,
        signal_density_stats.rejected_reason_top,
    )
    log.info(
        "INFO_DENSITY[corp] total_in=%d passed=%d rejected=%d avg_score=%.2f reasons_top=%s",
        corp_density_stats.total_in,
        corp_density_stats.passed,
        corp_density_stats.rejected_total,
        corp_density_stats.avg_score,
        corp_density_stats.rejected_reason_top,
    )

    collector.events_detected = len(event_density_cards)

    signal_summary = build_signal_summary(quality_cards)
    collector.signals_detected = len(signal_summary)

    corp_summary = build_corp_watch_summary(quality_cards, metrics=collector.to_dict())
    collector.corp_updates_detected = int(corp_summary.get("updates", 0))
    log.info(
        "STRATEGY_COUNTS event_candidates_total=%d signals_total=%d corp_mentions_total=%d",
        collector.events_detected,
        collector.signals_detected,
        int(corp_summary.get("mentions_count", corp_summary.get("total_mentions", 0))),
    )

    # Finalize metrics
    collector.stop()
    metrics_path = collector.write_json()

    # Z5: Education Renderer (non-blocking, always runs)
    if settings.EDU_REPORT_ENABLED:
        try:
            log.info("--- Z5: Education Renderer ---")
            metrics_dict = collector.to_dict()
            # 模式 A（優先）：結構化輸入
            z5_results = all_results if all_results else None
            z5_report = z4_report
            # 模式 B fallback：讀取文本
            z5_text = None
            if z5_report is None and all_results:
                da_path = Path(settings.DEEP_ANALYSIS_OUTPUT_PATH)
                if da_path.exists():
                    z5_text = da_path.read_text(encoding="utf-8")

            notion_md, ppt_md, xmind_md = render_education_report(
                results=z5_results,
                report=z5_report,
                metrics=metrics_dict,
                deep_analysis_text=z5_text,
                max_items=settings.EDU_REPORT_MAX_ITEMS,
                filter_summary=filter_summary_dict,
            )
            edu_paths = write_education_reports(notion_md, ppt_md, xmind_md)
            log.info("Z5: 教育版報告已生成 → %s", [str(p) for p in edu_paths])

            # Generate executive output files (PPTX + DOCX + Notion + XMind)
            try:
                pptx_path, docx_path, notion_path, xmind_path = generate_executive_reports(
                    results=z5_results,
                    report=z5_report,
                    metrics=metrics_dict,
                    deep_analysis_text=z5_text,
                    max_items=settings.EDU_REPORT_MAX_ITEMS,
                )
                log.info("Executive PPTX generated: %s", pptx_path)
                log.info("Executive DOCX generated: %s", docx_path)
                log.info("Notion page generated: %s", notion_path)
                log.info("XMind mindmap generated: %s", xmind_path)
            except Exception as exc_bin:
                log.error("Executive report generation failed (non-blocking): %s", exc_bin)
        except Exception as exc:
            log.error("Z5 Education Renderer failed (non-blocking): %s", exc)
            try:
                err_md = render_error_report(exc)
                err_path = Path(settings.PROJECT_ROOT) / "outputs" / "deep_analysis_education.md"
                err_path.parent.mkdir(parents=True, exist_ok=True)
                err_path.write_text(err_md, encoding="utf-8")
                log.info("Z5: 錯誤說明已寫入 %s", err_path)
            except Exception:
                log.error("Z5: 連錯誤報告都寫不出來")
    else:
        log.info("Z5: Education report disabled")

    elapsed = time.time() - t_start
    passed = sum(1 for r in all_results if r.passed_gate)
    log.info("PIPELINE COMPLETE | %d processed | %d passed | %.2fs total", len(all_results), passed, elapsed)
    log.info("Digest: %s", digest_path)
    log.info("Metrics: %s", metrics_path)

    # Notifications
    send_all_notifications(t_start_iso, len(all_results), True, str(digest_path))


if __name__ == "__main__":
    run_pipeline()
